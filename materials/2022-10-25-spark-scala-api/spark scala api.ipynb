{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"200\" height=\"200\" src=\"https://static.tildacdn.com/tild6236-6337-4339-b337-313363643735/new_logo.png\">\n",
    "\n",
    "# Spark Dataframes I\n",
    "**Сергей Гришаев**  \n",
    "serg.grishaev@gmail.com  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На этом занятии\n",
    "+ Сравнение RDD API и DataFrame API \n",
    "+ Базовые функции\n",
    "+ Очистка данных\n",
    "+ Агрегаты\n",
    "+ Кеширование \n",
    "+ Репартиционирование\n",
    "+ Встроенные функции\n",
    "+ Пользовательские функции\n",
    "+ Соединения\n",
    "+ Оконные функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравнение RDD API и DataFrame API \n",
    "\n",
    "### Типы данных\n",
    "**RDD**: низкоуревная распределенная коллекция данных любого типа  \n",
    "**DF**: таблица со схемой, состоящей из колонок разных типов, описанных в `org.apache.spark.sql.types`  \n",
    "\n",
    "### Обработка данных\n",
    "**RDD**: сериализуемые функции  \n",
    "**DF**: кодогенерация SQL > Java код  \n",
    "\n",
    "### Функции и алгоритмы\n",
    "**RDD**: нет ограничений  \n",
    "**DF**: ограничен SQL операторами, функциями `org.apache.spark.sql.functions` и пользовательскими функциями  \n",
    "\n",
    "### Источники данных\n",
    "**RDD**: каждый источник имеет свое API  \n",
    "**DF**: единое API для всех источников \n",
    "\n",
    "### Производительность\n",
    "**RDD**: напрямую зависит от качества кода\n",
    "**DF**: встроенные механизмы оптимизации SQL запроса\n",
    "\n",
    "\n",
    "### Потоковая обработка данных\n",
    "**RDD**: устаревший DStreams  \n",
    "**DF**: активно развивающийся Structured Streaming\n",
    "\n",
    "\n",
    "### Выводы:\n",
    "+ На текущий момент RDD является низкоуровневым API, которое постепенно уходит \"под капот\" Apache Spark\n",
    "+ DF API представляет собой библиотеку для обработки данных с использованием SQL примитивов\n",
    "\n",
    "## Базовые функции\n",
    "\n",
    "Создать dataframe можно на основе:\n",
    "+ локальной коллекции\n",
    "+ файлов\n",
    "+ базы данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cityList = Vector(Moscow, Paris, Madrid, London, New York)\n",
       "df = [value: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: string]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "val cityList: Vector[String] = Vector(\"Moscow\", \"Paris\", \"Madrid\", \"London\", \"New York\")\n",
    "\n",
    "// метод toDF изначально отсутствует у Vector[T], он добавляется через import spark.implicits._\n",
    "val df: DataFrame = cityList.toDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У любого DF есть схема:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотреть содержимое DF можно с помощью метода `show()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|   value|\n",
      "+--------+\n",
      "|  Moscow|\n",
      "|   Paris|\n",
      "|  Madrid|\n",
      "|  London|\n",
      "|New York|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также можно вывести содержимое в вертикальной ориентации - это удобно при большое количестве столбцов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------\n",
      " value | Moscow   \n",
      "-RECORD 1---------\n",
      " value | Paris    \n",
      "-RECORD 2---------\n",
      " value | Madrid   \n",
      "-RECORD 3---------\n",
      " value | London   \n",
      "-RECORD 4---------\n",
      " value | New York \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(numRows = 20, truncate = 100, vertical=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсчет количества элементов в DF с помощью `count()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отфильтровать данные можно с помощью метода `filter`. В отличие от RDD, он принимает SQL выражение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "warning: there were two deprecation warnings; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|   value|\n",
      "+--------+\n",
      "|   Paris|\n",
      "|  Madrid|\n",
      "|  London|\n",
      "|New York|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Требует наличия import spark.implicits._\n",
    "\n",
    "df.filter(('value !== \"Moscow\").and( ('value !== \"Moscow\"))).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|   value|\n",
      "+--------+\n",
      "|   Paris|\n",
      "|  Madrid|\n",
      "|  London|\n",
      "|New York|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter('value.!==(\"Moscow\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "value"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "$\"value\" toString ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| value|\n",
      "+------+\n",
      "|Moscow|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Требует наличия import spark.implicits._\n",
    "\n",
    "df.filter($\"value\" === \"Moscow\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| value|\n",
      "+------+\n",
      "|Moscow|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// sugar free & type safe\n",
    "// Три знака равно здесь используются, тк на самом деле это метод,\n",
    "// применяемый к колонке org.apache.spark.sql.Column\n",
    "\n",
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "df.filter(col(\"value\") === \"Moscow\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "qE = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "== Parsed Logical Plan ==\n",
       "Project [value#1, foo#68, bar AS bar#71]\n",
       "+- Project [value#1, foo AS foo#68]\n",
       "   +- LogicalRDD [value#1], false\n",
       "== Analyzed Logical Plan ==\n",
       "value: string, foo: string, bar: string\n",
       "Project [value#1, foo#68, bar AS bar#71]\n",
       "+- Project [value#1, foo AS foo#68]\n",
       "   +- LogicalRDD [value#1], false\n",
       "== Optimized Logical Plan ==\n",
       "Project [value#1, foo AS foo#68, bar AS bar#71]\n",
       "+- LogicalRDD [value#1], false\n",
       "== Physical Plan ==\n",
       "*(1) Project [value#1, foo AS foo#68, bar AS bar#71]\n",
       "+- Scan ExistingRDD[value#1]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// легко ошибиться и получить ошибку в рантайме\n",
    "import org.apache.spark.sql.functions.lit\n",
    "\n",
    "val qE = \n",
    "    df.filter(\"value = 'Moscow'\")\n",
    "        .localCheckpoint\n",
    "        .withColumn(\"foo\", lit(\"foo\"))\n",
    "        .withColumn(\"bar\", lit(\"bar\"))\n",
    "        .queryExecution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Project [value#1, foo#68, bar AS bar#71]\n",
       "+- Project [value#1, foo AS foo#68]\n",
       "   +- LogicalRDD [value#1], false\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qE.logical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqlParser = org.apache.spark.sql.execution.SparkSqlParser@5bd91b0f\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n",
       "'Project ['FOO]\n",
       "+- 'UnresolvedRelation `BAR`\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val sqlParser = spark.sessionState.sqlParser\n",
    "sqlParser.parsePlan(\"\"\"SELECT FOO FROM BAR\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Project [value#1, foo#68, bar AS bar#71]\n",
       "+- Project [value#1, foo AS foo#68]\n",
       "   +- LogicalRDD [value#1], false\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qE.analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Project [value#1, foo AS foo#68, bar AS bar#71]\n",
       "+- LogicalRDD [value#1], false\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qE.optimizedPlan // CollapseProject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "*(1) Project [value#1, foo AS foo#68, bar AS bar#71]\n",
       "+- Scan ExistingRDD[value#1]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qE.executedPlan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd = SQLExecutionRDD[24] at toRdd at <console>:34\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SQLExecutionRDD[24] at toRdd at <console>:34"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.catalyst.InternalRow\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "val rdd: RDD[InternalRow] = qE.toRdd\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| value|\n",
      "+------+\n",
      "|Moscow|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// промежуточный вариант между col и обычной строкой\n",
    "// expr также может использоваться для вызова SQL builtin функций, \n",
    "// отсутствующих в org.apache.spark.sql.functions\n",
    "\n",
    "import org.apache.spark.sql.functions.expr\n",
    "\n",
    "df.filter(expr(\"value = 'Moscow'\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавить новую колонку можно с помощью метода `withColumn`. Необходимо помнить, что данный метод, как и другие, является трансформацией и не изменяет оригинальный DF, а создает новый."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|   value|upperCity|\n",
      "+--------+---------+\n",
      "|  Moscow|   MOSCOW|\n",
      "|   Paris|    PARIS|\n",
      "|  Madrid|   MADRID|\n",
      "|  London|   LONDON|\n",
      "|New York| NEW YORK|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.upper\n",
    "df.withColumn(\"upperCity\", upper('value)).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогичный результат получить, используя метод `select`. Данный метод может быть использован не только для выбора определенных колонок, но и для создания новых."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|   value|upperCity|\n",
      "+--------+---------+\n",
      "|  Moscow|   MOSCOW|\n",
      "|   Paris|    PARIS|\n",
      "|  Madrid|   MADRID|\n",
      "|  London|   LONDON|\n",
      "|New York| NEW YORK|\n",
      "+--------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "withUpper = [value: string, upperCity: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: string, upperCity: string]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val withUpper = df.select('value, upper('value).alias(\"upperCity\"))\n",
    "withUpper.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если передать `col(\"*\")` в `select`, то вы получите DF со всеми колонками. Это полезно, когда вы не знаете список всех колонок (например вы получили его через API), но вам нужно их все выбрать и добавить новую колонку. Это можно сделать следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+------+---+\n",
      "|   value|upperCity|lowerCity|length|bar|\n",
      "+--------+---------+---------+------+---+\n",
      "|  Moscow|   MOSCOW|   moscow|     7|foo|\n",
      "|   Paris|    PARIS|    paris|     6|foo|\n",
      "|  Madrid|   MADRID|   madrid|     7|foo|\n",
      "|  London|   LONDON|   london|     7|foo|\n",
      "|New York| NEW YORK| new york|     9|foo|\n",
      "+--------+---------+---------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// методы name, as и alias часто являются взаимозаменяемыми\n",
    "\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "withUpper.select(\n",
    "    col(\"*\"), \n",
    "    lower($\"value\").name(\"lowerCity\"), \n",
    "    (length('value) + 1).as(\"length\"),\n",
    "    lit(\"foo\").alias(\"bar\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При необходимости в `select` можно передать список колонок, используя обычные строки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|   value|upperCity|\n",
      "+--------+---------+\n",
      "|  Moscow|   MOSCOW|\n",
      "|   Paris|    PARIS|\n",
      "|  Madrid|   MADRID|\n",
      "|  London|   LONDON|\n",
      "|New York| NEW YORK|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "withUpper.select(\"value\", \"upperCity\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалить колонку из DF можно с помощью метода `drop`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|   value|\n",
      "+--------+\n",
      "|  Moscow|\n",
      "|   Paris|\n",
      "|  Madrid|\n",
      "|  London|\n",
      "|New York|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// drop не будет выдавать ошибку, если будет указана несуществующая колонка\n",
    "\n",
    "withUpper.drop(\"upperCity\", \"abraKadabra\", \"2323\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Project [value#1]\n",
      "+- LogicalRDD [value#1, upperCity#126], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "value: string\n",
      "Project [value#1]\n",
      "+- LogicalRDD [value#1, upperCity#126], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [value#1]\n",
      "+- LogicalRDD [value#1, upperCity#126], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [value#1]\n",
      "+- Scan ExistingRDD[value#1,upperCity#126]\n"
     ]
    }
   ],
   "source": [
    "// Жертвуем отказоустойчивостью для повышения производительности\n",
    "withUpper.localCheckpoint.drop(\"upperCity\", \"abraKadabra\").explain(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(abc = cde) AS `123`"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(col(\"abc\") === col(\"cde\")) alias \"123\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "+ методы `filter` и `select` принимают в качестве аргументов колонки [org.apache.spark.sql.Column](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column). Это может быть либо ссылка на существующую колонку, либо функцию из [org.apache.spark.sql.functions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$)\n",
    "+ любые трансформации возвращают новый DF, не меняя существующий\n",
    "+ тип [org.apache.spark.sql.Column](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column) играет важную роль в DF API - на его основе создаются ссылки на существующие колонки, а также функции, принимающие [org.apache.spark.sql.Column](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column) и возвращающие [org.apache.spark.sql.Column](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column). По этой причине обычное сравнение `==` не будет работать в DF API, тк `filter` принимает [org.apache.spark.sql.Column](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column), а не `Boolean`\n",
    "+ Класс DataFrame в последних версиях Spark представляет собой `org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]`, поэтому его описание следует искать в [org.apache.spark.sql.Dataset](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)\n",
    "\n",
    "## Очистка данных\n",
    "\n",
    "Одной из задач обработки данных является их очистка. DF API содержит класс функций \"not available\", описанный в пакете [org.apache.spark.sql.DataFrameNaFunctions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameNaFunctions). В данном пакете есть три функции:\n",
    "+ `na.drop`\n",
    "+ `na.fill`\n",
    "+ `na.replace`\n",
    "\n",
    "Для демонстрации работы данных функций создадим новый датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- population: long (nullable = true)\n",
      "\n",
      "+--------------------+---------+-------+---------+----------+\n",
      "|     _corrupt_record|continent|country|     name|population|\n",
      "+--------------------+---------+-------+---------+----------+\n",
      "|                null|   Europe|Rossiya|   Moscow|  12380664|\n",
      "|                null|     null|  Spain|   Madrid|      null|\n",
      "|                null|   Europe| France|    Paris|   2196936|\n",
      "|                null|   Europe|Germany|   Berlin|   3490105|\n",
      "|                null|   Europe|  Spain|Barselona|      null|\n",
      "|                null|   Africa|  Egypt|    Cairo|  11922948|\n",
      "|                null|   Africa|  Egypt|    Cairo|  11922948|\n",
      "|{ \"name\":\"New Yor...|     null|   null|     null|      null|\n",
      "+--------------------+---------+-------+---------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "testData = \n",
       "raw = [value: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{ \"name\":\"Moscow\", \"country\":\"Rossiya\", \"continent\": \"Europe\", \"population\": 12380664}\n",
       "{ \"name\":\"Madrid\", \"country\":\"Spain\" }\n",
       "{ \"name\":\"Paris\", \"country\":\"France\", \"continent\": \"Europe\", \"population\" : 2196936}\n",
       "{ \"name\":\"Berlin\", \"country\":\"Germany\", \"continent\": \"Europe\", \"population\": 3490105}\n",
       "{ \"name\":\"Barselona\", \"country\":\"Spain\", \"continent\": \"Europe\" }\n",
       "{ \"name\":\"Cairo\", \"country\":\"Egypt\", \"continent\": \"Africa\", \"population\": 11922948 }\n",
       "{ \"name\":\"Cairo\", \"country\":\"Egypt\", \"continent\": \"Africa\", \"population\": 11922948 }\n",
       "{ \"name\":\"New York, \"country\":\"USA\",\n",
       "jsonStrings:...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: string]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Column\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.Dataset\n",
    "\n",
    "val testData =\n",
    "\"\"\"{ \"name\":\"Moscow\", \"country\":\"Rossiya\", \"continent\": \"Europe\", \"population\": 12380664}\n",
    "{ \"name\":\"Madrid\", \"country\":\"Spain\" }\n",
    "{ \"name\":\"Paris\", \"country\":\"France\", \"continent\": \"Europe\", \"population\" : 2196936}\n",
    "{ \"name\":\"Berlin\", \"country\":\"Germany\", \"continent\": \"Europe\", \"population\": 3490105}\n",
    "{ \"name\":\"Barselona\", \"country\":\"Spain\", \"continent\": \"Europe\" }\n",
    "{ \"name\":\"Cairo\", \"country\":\"Egypt\", \"continent\": \"Africa\", \"population\": 11922948 }\n",
    "{ \"name\":\"Cairo\", \"country\":\"Egypt\", \"continent\": \"Africa\", \"population\": 11922948 }\n",
    "{ \"name\":\"New York, \"country\":\"USA\",\"\"\"\n",
    "\n",
    "// Создаем DF из одной строки и добавляем данные в виде новой колонки\n",
    "val raw = spark.range(0,1).select(lit(testData).alias(\"value\"))\n",
    "\n",
    "// Создаем новую колонку, разибая наши данные по \\n\n",
    "val jsonStrings: Column = split(col(\"value\"), \"\\n\").alias(\"value\")\n",
    "\n",
    "// Используем функцию explode для того, чтобы развернуть массив мехом наружу и используем темную магию \n",
    "// для превращения DataFrame в Dataset[String]\n",
    "val splited: Dataset[String] = raw.select(explode(jsonStrings)).as[String]\n",
    "\n",
    "// splited.show(numRows = 10, truncate = false)\n",
    "\n",
    "\n",
    "// Создаем новый датафре... датасет, в котором наши JSON строки будут распарсены\n",
    "val df: Dataset[Row] = spark.read.json(splited)\n",
    "df.printSchema\n",
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для очистки датасета:\n",
    "+ удалим строку с навалидным JSON, сохраним ее в отдельное место\n",
    "+ удалим дубликаты\n",
    "+ заполним `null`ы в колонках\n",
    "+ исправим `Rossiya` на `Russia`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "corruptData = Array([{ \"name\":\"New York, \"country\":\"USA\",])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array([{ \"name\":\"New York, \"country\":\"USA\",])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val corruptData = df.select(col(\"_corrupt_record\")).na.drop(\"all\").collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+----------+\n",
      "|continent|country|     name|population|\n",
      "+---------+-------+---------+----------+\n",
      "|   Europe| France|    Paris|   2196936|\n",
      "|   Europe|Germany|   Berlin|   3490105|\n",
      "|Undefined|  Spain|   Madrid|         0|\n",
      "|   Africa|  Egypt|    Cairo|  11922948|\n",
      "|   Europe|  Spain|Barselona|         0|\n",
      "|   Europe| Russia|   Moscow|  12380664|\n",
      "+---------+-------+---------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fillData = Map(continent -> Undefined, population -> 0)\n",
       "replaceData = Map(Rossiya -> Russia)\n",
       "cleanData = [continent: string, country: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[continent: string, country: string ... 2 more fields]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fillData: Map[String, Any] = Map(\"continent\" -> \"Undefined\", \"population\" -> 0)\n",
    "val replaceData: Map[Any, Any] = Map(\"Rossiya\" -> \"Russia\")\n",
    "\n",
    "val cleanData = \n",
    "    df\n",
    "    .drop(col(\"_corrupt_record\"))\n",
    "    .na.drop(\"all\")\n",
    "    .na.fill(fillData)\n",
    "    .na.replace(\"country\", replaceData)\n",
    "    .dropDuplicates\n",
    "    .localCheckpoint\n",
    "\n",
    "\n",
    "cleanData.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "+ DF API обладает удобным API для очистки данных, позволяющим разработчику сконцентрироваться разработчику на бизнес логике, а не на написании функций для обработки всех возможных исключительных ситуаций\n",
    "+ метод `spark.read.json` позволяет читать не только файлы, но и `Dataset[String]`, содержащие JSON строки.\n",
    "\n",
    "## Агрегаты\n",
    "Посчитаем суммарное население и количество городов с разбивкой по континентам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|continent|count|\n",
      "+---------+-----+\n",
      "|   Europe|    4|\n",
      "|   Africa|    1|\n",
      "|Undefined|    1|\n",
      "+---------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "aggCount = [continent: string, count: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[continent: string, count: bigint]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val aggCount = cleanData.groupBy('continent).count\n",
    "aggCount.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+\n",
      "|continent|sum(population)|\n",
      "+---------+---------------+\n",
      "|   Europe|       18067705|\n",
      "|   Africa|       11922948|\n",
      "|Undefined|              0|\n",
      "+---------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "aggSum = [continent: string, sum(population): bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[continent: string, sum(population): bigint]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val aggSum = cleanData.groupBy('continent).sum(\"population\")\n",
    "aggSum.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы совместить несколько агрегатов в одном DF, мы можем использовать метод `agg()`. Данный метод позволяет использовать любые `Aggregate functions` из пакета [org.apache.spark.sql.functions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+--------+\n",
      "|continent|count|  sumPop|\n",
      "+---------+-----+--------+\n",
      "|   Europe|    4|18067705|\n",
      "|   Africa|    1|11922948|\n",
      "|Undefined|    1|       0|\n",
      "+---------+-----+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "agg = [continent: string, count: bigint ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[continent: string, count: bigint ... 1 more field]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val agg = cleanData.groupBy('continent).agg(count(\"*\").alias(\"count\"), sum(\"population\").alias(\"sumPop\"))\n",
    "agg.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью агрегатов мы можем выполнять такие действия, как, например, `collect_list` и `collect_set`. Стоит отметить, что колонки в Spark могут иметь не только скалярные типы, но и структуры, словари и массивы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- continent: string (nullable = false)\n",
      " |-- countries: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "-RECORD 0-------------------------------------\n",
      " continent | Europe                           \n",
      " countries | [France, Germany, Spain, Russia] \n",
      "-RECORD 1-------------------------------------\n",
      " continent | Africa                           \n",
      " countries | [Egypt]                          \n",
      "-RECORD 2-------------------------------------\n",
      " continent | Undefined                        \n",
      " countries | [Spain]                          \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "aggList = [continent: string, countries: array<string>]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[continent: string, countries: array<string>]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val aggList = cleanData.groupBy('continent).agg(collect_list(\"country\").alias(\"countries\"))\n",
    "aggList.printSchema\n",
    "aggList.show(numRows = 10, truncate = 100, vertical = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя методы `struct` и `to_json`, мы можем превратить произвольный набор колонок в JSON строку. Этот методы часто используется перед отправкой данных в Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- s: struct (nullable = false)\n",
      " |    |-- continent: string (nullable = false)\n",
      " |    |-- countries: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      "\n",
      "+------------------------------------------+\n",
      "|s                                         |\n",
      "+------------------------------------------+\n",
      "|[Europe, [France, Germany, Spain, Russia]]|\n",
      "|[Africa, [Egypt]]                         |\n",
      "|[Undefined, [Spain]]                      |\n",
      "+------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "withStruct = [s: struct<continent: string, countries: array<string>>]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[continent: string]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val withStruct = aggList.select(struct('continent, 'countries).alias(\"s\"))\n",
    "withStruct.printSchema\n",
    "\n",
    "withStruct.show(10, false)\n",
    "withStruct.select(col(\"`s`.`continent`\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------+\n",
      "|s                                                                       |\n",
      "+------------------------------------------------------------------------+\n",
      "|{\"continent\":\"Europe\",\"countries\":[\"France\",\"Germany\",\"Spain\",\"Russia\"]}|\n",
      "|{\"continent\":\"Africa\",\"countries\":[\"Egypt\"]}                            |\n",
      "|{\"continent\":\"Undefined\",\"countries\":[\"Spain\"]}                         |\n",
      "+------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "withStruct.withColumn(\"s\", to_json('s)).show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если необходимо превратить все колонки DF в JSON String, можно воспользоваться функций `toJSON`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------+\n",
      "|value                                                                   |\n",
      "+------------------------------------------------------------------------+\n",
      "|{\"continent\":\"Europe\",\"countries\":[\"France\",\"Germany\",\"Spain\",\"Russia\"]}|\n",
      "|{\"continent\":\"Africa\",\"countries\":[\"Egypt\"]}                            |\n",
      "|{\"continent\":\"Undefined\",\"countries\":[\"Spain\"]}                         |\n",
      "+------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "jString = [value: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: string]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val jString: Dataset[String] = aggList.toJSON\n",
    "jString.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если нам необходимо создать колонки из значений текущих колонок, мы можем воспользоваться функцией `pivot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+----------+\n",
      "|continent|country|     name|population|\n",
      "+---------+-------+---------+----------+\n",
      "|   Europe| France|    Paris|   2196936|\n",
      "|   Europe|Germany|   Berlin|   3490105|\n",
      "|Undefined|  Spain|   Madrid|         0|\n",
      "|   Africa|  Egypt|    Cairo|  11922948|\n",
      "|   Europe|  Spain|Barselona|         0|\n",
      "|   Europe| Russia|   Moscow|  12380664|\n",
      "+---------+-------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanData.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------+---------+\n",
      "|country|  Africa|  Europe|Undefined|\n",
      "+-------+--------+--------+---------+\n",
      "| Russia|    null|12380664|     null|\n",
      "|Germany|    null| 3490105|     null|\n",
      "| France|    null| 2196936|     null|\n",
      "|  Spain|    null|       0|        0|\n",
      "|  Egypt|11922948|    null|     null|\n",
      "+-------+--------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanData.groupBy(col(\"country\")).pivot(\"continent\").agg(sum(\"population\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:57: error: value show is not a member of org.apache.spark.sql.RelationalGroupedDataset\n",
       "       cleanData.groupBy(col(\"country\")).pivot(\"continent\").show()\n",
       "                                                            ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanData.groupBy(col(\"country\")).pivot(\"continent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "+ DF API позволяет строить большое количество агрегатов. При этом необходимо помнить, что операции `groupBy`, `cube`, `rollup` возвращают [org.apache.spark.sql.RelationalGroupedDataset](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset), к которому затем необходимо применить одну из функций агрегации - `count`, `sum`, `agg` и т. п.\n",
    "+ При вычислении агрегатов необходимо помнить, что эта операция требует перемешивания данных между воркерами, что, в случае перекошенных данных, может привести к OOM на воркере.\n",
    "\n",
    "## Кеширование\n",
    "По умолчанию при применении каждого действия Spark пересчитывает весь граф, что может негативно сказать на производительности приложения. Для демонстрации возьмем датасет [Airport Codes](https://datahub.io/core/airport-codes)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n",
      "-RECORD 0------------------------------------------\n",
      " ident        | 00A                                \n",
      " type         | heliport                           \n",
      " name         | Total Rf Heliport                  \n",
      " elevation_ft | 11                                 \n",
      " continent    | NA                                 \n",
      " iso_country  | US                                 \n",
      " iso_region   | US-PA                              \n",
      " municipality | Bensalem                           \n",
      " gps_code     | 00A                                \n",
      " iata_code    | null                               \n",
      " local_code   | 00A                                \n",
      " coordinates  | -74.93360137939453, 40.07080078125 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "csvOptions = Map(header -> true, inferSchema -> true)\n",
       "airports = [ident: string, type: string ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, type: string ... 10 more fields]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val csvOptions = Map(\"header\" -> \"true\", \"inferSchema\" -> \"true\")\n",
    "val airports = spark.read.options(csvOptions).csv(\"/tmp/airport-codes_csv.csv\")\n",
    "airports.printSchema\n",
    "airports.show(numRows = 1, truncate = 100, vertical = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airports.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|     US-KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|     US-AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "| 00AL|small_airport|        Epps Airpark|         820|       NA|         US|     US-AL|     Harvest|    00AL|     null|      00AL|-86.7703018188476...|\n",
      "| 00AR|       closed|Newport Hospital ...|         237|       NA|         US|     US-AR|     Newport|    null|     null|      null| -91.254898, 35.6087|\n",
      "| 00AS|small_airport|      Fulton Airport|        1100|       NA|         US|     US-OK|        Alex|    00AS|     null|      00AS|-97.8180194, 34.9...|\n",
      "| 00AZ|small_airport|      Cordes Airport|        3810|       NA|         US|     US-AZ|      Cordes|    00AZ|     null|      00AZ|-112.165000915527...|\n",
      "| 00CA|small_airport|Goldstone /Gts/ A...|        3038|       NA|         US|     US-CA|     Barstow|    00CA|     null|      00CA|-116.888000488, 3...|\n",
      "| 00CL|small_airport| Williams Ag Airport|          87|       NA|         US|     US-CA|       Biggs|    00CL|     null|      00CL|-121.763427, 39.4...|\n",
      "| 00CN|     heliport|Kitchen Creek Hel...|        3350|       NA|         US|     US-CA| Pine Valley|    00CN|     null|      00CN|-116.4597417, 32....|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем несколько агрегатов. Несмотря на то, что `onlyRu` является общим для всех действий, он пересчитывается при вызове каждого действия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " ident        | RU-0006               \n",
      " type         | closed                \n",
      " name         | Arabatuk Air Base     \n",
      " elevation_ft | 2280                  \n",
      " continent    | EU                    \n",
      " iso_country  | RU                    \n",
      " iso_region   | RU-CHI                \n",
      " municipality | Daurija               \n",
      " gps_code     | null                  \n",
      " iata_code    | null                  \n",
      " local_code   | ZA2N                  \n",
      " coordinates  | 117.098999, 50.223801 \n",
      "only showing top 1 row\n",
      "\n",
      "== Physical Plan ==\n",
      "*(3) Sort [count#914L DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(count#914L DESC NULLS LAST, 200)\n",
      "   +- *(2) Filter AtLeastNNulls(n, municipality#581,count#914L)\n",
      "      +- *(2) HashAggregate(keys=[municipality#581], functions=[count(1)])\n",
      "         +- Exchange hashpartitioning(municipality#581, 200)\n",
      "            +- *(1) HashAggregate(keys=[municipality#581], functions=[partial_count(1)])\n",
      "               +- *(1) Project [municipality#581]\n",
      "                  +- *(1) Filter (((isnotnull(iso_country#579) && isnotnull(elevation_ft#577)) && (iso_country#579 = RU)) && (elevation_ft#577 > 1000))\n",
      "                     +- *(1) FileScan csv [elevation_ft#577,iso_country#579,municipality#581] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://spark-master-1.newprolab.com:8020/tmp/airport-codes_csv.csv], PartitionFilters: [], PushedFilters: [IsNotNull(iso_country), IsNotNull(elevation_ft), EqualTo(iso_country,RU), GreaterThan(elevation_..., ReadSchema: struct<elevation_ft:int,iso_country:string,municipality:string>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "onlyRuAndHigh = [ident: string, type: string ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, type: string ... 10 more fields]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val onlyRuAndHigh = airports.filter('iso_country === \"RU\" and 'elevation_ft > 1000)\n",
    "onlyRuAndHigh.show(numRows = 1, truncate = 100, vertical = true)\n",
    "onlyRuAndHigh.count\n",
    "onlyRuAndHigh.collect\n",
    "onlyRuAndHigh.groupBy('municipality).count.orderBy('count.desc).na.drop(\"any\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для решения этой проблемы следует использовать методы `cache`, либо `persist`. Данные методы сохраняют состояние графа после первого действия, и следующие обращаются к нему. Разница между методами заключается в том, что `persist` позволяет выбрать, куда сохранить данные, а `cache` использует значение по умолчанию. В текущей версии Spark это [StorageLevel.MEMORY_ONLY](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence). Важно помнить, что данный кеш не предназначен для обмена данными между разными Spark приложения - он является внутренним для приложения. После того, как работа с данными окончена, необходимо выполнить `unpersist` для очистки памяти"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " ident        | RU-0006               \n",
      " type         | closed                \n",
      " name         | Arabatuk Air Base     \n",
      " elevation_ft | 2280                  \n",
      " continent    | EU                    \n",
      " iso_country  | RU                    \n",
      " iso_region   | RU-CHI                \n",
      " municipality | Daurija               \n",
      " gps_code     | null                  \n",
      " iata_code    | null                  \n",
      " local_code   | ZA2N                  \n",
      " coordinates  | 117.098999, 50.223801 \n",
      "only showing top 1 row\n",
      "\n",
      "== Physical Plan ==\n",
      "*(3) Sort [count#1514L DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(count#1514L DESC NULLS LAST, 200)\n",
      "   +- *(2) Filter AtLeastNNulls(n, municipality#581,count#1514L)\n",
      "      +- *(2) HashAggregate(keys=[municipality#581], functions=[count(1)])\n",
      "         +- Exchange hashpartitioning(municipality#581, 200)\n",
      "            +- *(1) HashAggregate(keys=[municipality#581], functions=[partial_count(1)])\n",
      "               +- InMemoryTableScan [municipality#581]\n",
      "                     +- InMemoryRelation [ident#574, type#575, name#576, elevation_ft#577, continent#578, iso_country#579, iso_region#580, municipality#581, gps_code#582, iata_code#583, local_code#584, coordinates#585], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                           +- *(1) Project [ident#574, type#575, name#576, elevation_ft#577, continent#578, iso_country#579, iso_region#580, municipality#581, gps_code#582, iata_code#583, local_code#584, coordinates#585]\n",
      "                              +- *(1) Filter (((isnotnull(iso_country#579) && isnotnull(elevation_ft#577)) && (iso_country#579 = RU)) && (elevation_ft#577 > 1000))\n",
      "                                 +- *(1) FileScan csv [ident#574,type#575,name#576,elevation_ft#577,continent#578,iso_country#579,iso_region#580,municipality#581,gps_code#582,iata_code#583,local_code#584,coordinates#585] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://spark-master-1.newprolab.com:8020/tmp/airport-codes_csv.csv], PartitionFilters: [], PushedFilters: [IsNotNull(iso_country), IsNotNull(elevation_ft), EqualTo(iso_country,RU), GreaterThan(elevation_..., ReadSchema: struct<ident:string,type:string,name:string,elevation_ft:int,continent:string,iso_country:string,...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, type: string ... 10 more fields]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onlyRuAndHigh.cache\n",
    "onlyRuAndHigh.count\n",
    "// при вычислении count данные будут помещены в cache\n",
    "onlyRuAndHigh.show(numRows = 1, truncate = 100, vertical = true)\n",
    "onlyRuAndHigh.collect\n",
    "onlyRuAndHigh.groupBy('municipality).count.orderBy('count.desc).na.drop(\"any\").explain()\n",
    "\n",
    "onlyRuAndHigh.unpersist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "+ Использование `cache` и `persist` позволяет существенно сократить время обработки данных, однако следует помнить и об увеличении потребляемой памяти на воркерах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Репартиционирование\n",
    "RDD и DF являются представляют собой классы, описывающие распределенные коллекции данных. Они (коллекции) разбиты на крупные блоки, которые называются партициями. В графе вычисления, который называется в Spark DAG (Direct Acyclic Graph), есть три основных компонента - `job`, `stage`, `task`.\n",
    "\n",
    "`job` представляет собой весь граф целиком, от момента создания DF, до применения `action` к нему. Состоит из одной или более `stage`. Когда возникает необходимость сделать `shuffle` данных, Spark создает новый `stage`. Каждый `stage` состоит из большого количества `task`. `task` это базовая операция над данными. Одновременно Spark выполняет N `task`, которые обрабатывают N партиций, где N - это суммарное число доступных потоков на всех воркерах.\n",
    "\n",
    "Исходя из этого, важно обеспечивать:\n",
    "+ достаточное количество партиций для распределения нагрузки по всем воркерам\n",
    "+ равномерное распределение данных между партициями\n",
    "\n",
    "Создадим датасет с перекосом данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|itemPerPartition|\n",
      "+----------------+\n",
      "|0               |\n",
      "|900             |\n",
      "|0               |\n",
      "|100             |\n",
      "|0               |\n",
      "|0               |\n",
      "|0               |\n",
      "|0               |\n",
      "|0               |\n",
      "|0               |\n",
      "+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "skewColumn = CASE WHEN (id < 900) THEN 0 ELSE 1 END\n",
       "skewDf = [id: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "printItemPerPartition: [T](ds: org.apache.spark.sql.Dataset[T])Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: bigint]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val skewColumn = when(col(\"id\") < 900, lit(0)).otherwise(lit(1))\n",
    "\n",
    "val skewDf = spark.range(0,1000).repartition(10, skewColumn)\n",
    "\n",
    "def printItemPerPartition[T](ds: Dataset[T]): Unit = {\n",
    "    ds.mapPartitions { x => Iterator(x.length) }\n",
    "    .withColumnRenamed(\"value\", \"itemPerPartition\")\n",
    "    .show(50, false)\n",
    "}\n",
    "\n",
    "printItemPerPartition[java.lang.Long](skewDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Любые операции с таким датасетом будут работать медленно, т.к.\n",
    "+ если суммарное количество потоков на всех воркерах больше 10, то в один момент времени работать будут максимум 10, остальные будут простаивать\n",
    "+ из 10 партицийи только в 2 есть данные и это означает, что только 2 потока будут обрабатывать данные, при этом из-за перекоса данных между ними (900 vs 100) первый станет bottleneck'ом\n",
    "\n",
    "Обычно перекошенные датасеты возникают после вычисления агрегатов, оконных функций и соединений, но также могут возникать и при чтении источников.\n",
    "\n",
    "Для устранения проблемы перекоса данных, следует использовать метод `repartition`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Exchange RoundRobinPartitioning(20)\n",
      "+- *(1) Range (0, 1000, step=1, splits=2)\n",
      "+----------------+\n",
      "|itemPerPartition|\n",
      "+----------------+\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "repartitionedDf = [id: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: bigint]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// здесь мы передаем только новое количество партиций и Spark выполнит RoundRobinPartitioning\n",
    "val repartitionedDf = skewDf.repartition(20)\n",
    "repartitionedDf.explain()\n",
    "printItemPerPartition[java.lang.Long](repartitionedDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Exchange hashpartitioning(id#1643L, 20)\n",
      "+- *(1) Range (0, 1000, step=1, splits=2)\n",
      "+----------------+\n",
      "|itemPerPartition|\n",
      "+----------------+\n",
      "|37              |\n",
      "|61              |\n",
      "|48              |\n",
      "|59              |\n",
      "|47              |\n",
      "|54              |\n",
      "|45              |\n",
      "|58              |\n",
      "|55              |\n",
      "|55              |\n",
      "|56              |\n",
      "|46              |\n",
      "|45              |\n",
      "|46              |\n",
      "|49              |\n",
      "|64              |\n",
      "|44              |\n",
      "|39              |\n",
      "|40              |\n",
      "|52              |\n",
      "+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "repartitionedDf = [id: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: bigint]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// здесь мы добавляем к числу партиций колонку, по которой необходимо сделать репартиционирование,\n",
    "// поэтому Spark выполнит HashPartitioning\n",
    "val repartitionedDf = skewDf.repartition(20, col(\"id\"))\n",
    "repartitionedDf.explain()\n",
    "printItemPerPartition[java.lang.Long](repartitionedDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"200\" height=\"200\" src=\"https://pngimage.net/wp-content/uploads/2018/06/соленья-png-4.png\">\n",
    "\n",
    "### Соленья\n",
    "Часто при вычислении агрегатов приходится работать с перекошенными данными:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n",
      "+--------------+-----+\n",
      "|          type|count|\n",
      "+--------------+-----+\n",
      "| small_airport|34808|\n",
      "|      heliport|12028|\n",
      "|medium_airport| 4537|\n",
      "|        closed| 4378|\n",
      "| seaplane_base| 1030|\n",
      "| large_airport|  616|\n",
      "|   balloonport|   24|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports.printSchema\n",
    "\n",
    "airports.groupBy('type).count.orderBy('count.desc).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку при вычислении агрегата происходит неявный `HashPartitioning` по ключу (ключам) агрегата, то при выполнении определенных условий происходит нехватка памяти на воркере, которую нельзя исправить, не изменив подход к построению агрегата.\n",
    "\n",
    "Один из вариантов устранение - соление ключей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------\n",
      " ident        | 00A                                \n",
      " type         | heliport                           \n",
      " name         | Total Rf Heliport                  \n",
      " elevation_ft | 11                                 \n",
      " continent    | NA                                 \n",
      " iso_country  | US                                 \n",
      " iso_region   | US-PA                              \n",
      " municipality | Bensalem                           \n",
      " gps_code     | 00A                                \n",
      " iata_code    | null                               \n",
      " local_code   | 00A                                \n",
      " coordinates  | -74.93360137939453, 40.07080078125 \n",
      " salt         | 3                                  \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "saltModTen = CAST(pmod(round((rand(-5330693255816678478) * 100), 0), 10) AS INT)\n",
       "salted = [ident: string, type: string ... 11 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, type: string ... 11 more fields]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val saltModTen = pmod(round((rand() * 100), 0), lit(10)).cast(\"int\")\n",
    "\n",
    "val salted = airports.withColumn(\"salt\", saltModTen)\n",
    "salted.show(numRows = 1, truncate = 200, vertical = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это позволяет нам существенно снизить объем данных в каждой партиции (30к vs 3к):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+-----+\n",
      "|type          |salt|count|\n",
      "+--------------+----+-----+\n",
      "|small_airport |3   |3616 |\n",
      "|small_airport |2   |3558 |\n",
      "|small_airport |5   |3518 |\n",
      "|small_airport |0   |3490 |\n",
      "|small_airport |6   |3479 |\n",
      "|small_airport |9   |3449 |\n",
      "|small_airport |8   |3433 |\n",
      "|small_airport |7   |3431 |\n",
      "|small_airport |1   |3427 |\n",
      "|small_airport |4   |3407 |\n",
      "|heliport      |4   |1255 |\n",
      "|heliport      |7   |1233 |\n",
      "|heliport      |6   |1225 |\n",
      "|heliport      |2   |1220 |\n",
      "|heliport      |3   |1206 |\n",
      "|heliport      |0   |1192 |\n",
      "|heliport      |1   |1192 |\n",
      "|heliport      |9   |1178 |\n",
      "|heliport      |5   |1164 |\n",
      "|heliport      |8   |1163 |\n",
      "|medium_airport|5   |491  |\n",
      "|medium_airport|0   |489  |\n",
      "|medium_airport|8   |468  |\n",
      "|closed        |9   |457  |\n",
      "|medium_airport|4   |456  |\n",
      "|closed        |6   |455  |\n",
      "|closed        |2   |454  |\n",
      "|medium_airport|6   |454  |\n",
      "|medium_airport|1   |447  |\n",
      "|closed        |1   |447  |\n",
      "|medium_airport|3   |446  |\n",
      "|closed        |3   |446  |\n",
      "|medium_airport|9   |444  |\n",
      "|closed        |5   |440  |\n",
      "|medium_airport|7   |435  |\n",
      "|closed        |4   |434  |\n",
      "|closed        |8   |431  |\n",
      "|closed        |7   |408  |\n",
      "|medium_airport|2   |407  |\n",
      "|closed        |0   |406  |\n",
      "|seaplane_base |9   |118  |\n",
      "|seaplane_base |5   |114  |\n",
      "|seaplane_base |6   |112  |\n",
      "|seaplane_base |1   |103  |\n",
      "|seaplane_base |4   |103  |\n",
      "|seaplane_base |7   |99   |\n",
      "|seaplane_base |3   |98   |\n",
      "|seaplane_base |8   |97   |\n",
      "|seaplane_base |0   |97   |\n",
      "|seaplane_base |2   |89   |\n",
      "|large_airport |5   |70   |\n",
      "|large_airport |1   |70   |\n",
      "|large_airport |2   |68   |\n",
      "|large_airport |4   |68   |\n",
      "|large_airport |9   |63   |\n",
      "|large_airport |7   |59   |\n",
      "|large_airport |3   |56   |\n",
      "|large_airport |6   |55   |\n",
      "|large_airport |8   |54   |\n",
      "|large_airport |0   |53   |\n",
      "|balloonport   |1   |7    |\n",
      "|balloonport   |4   |3    |\n",
      "|balloonport   |0   |3    |\n",
      "|balloonport   |9   |3    |\n",
      "|balloonport   |2   |3    |\n",
      "|balloonport   |8   |1    |\n",
      "|balloonport   |3   |1    |\n",
      "|balloonport   |6   |1    |\n",
      "|balloonport   |5   |1    |\n",
      "|balloonport   |7   |1    |\n",
      "+--------------+----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "firstStep = [type: string, salt: int ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[type: string, salt: int ... 1 more field]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val firstStep = salted.groupBy('type, 'salt).count()\n",
    "\n",
    "firstStep.orderBy('count.desc).show(200, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вторым шагом мы делаем еще один агрегат, суммируя предыдущие значения `count`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|type          |count|\n",
      "+--------------+-----+\n",
      "|small_airport |34808|\n",
      "|heliport      |12028|\n",
      "|medium_airport|4537 |\n",
      "|closed        |4378 |\n",
      "|seaplane_base |1030 |\n",
      "|large_airport |616  |\n",
      "|balloonport   |24   |\n",
      "+--------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "secondStep = [type: string, count: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[type: string, count: bigint]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val secondStep = firstStep.groupBy('type).agg(sum(\"count\").alias(\"count\"))\n",
    "\n",
    "secondStep.orderBy('count.desc).show(200, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Несмотря на то, что мы сделали две группировки вместо одной, распределение данных по воркерам было более равномерным, что позволило избежать OOM на воркерах.\n",
    "\n",
    "### Выводы:\n",
    "+ Партиционирование - важный аспект распределенных вычислений, от которого напрямую зависит стабильность и скорость вычислений\n",
    "+ В Spark всегда работает правило 1 TASK = 1 THREAD (= vCore = 1 virtual CPU core) = 1 PARTITION\n",
    "+ Репартиционирование и соление данных позволяет решить проблему перекоса данных и вычислений\n",
    "+ Важно помнить, что репартиционирование использует дисковую и сетевую подсистемы - обмен данными происходит **по сети**, а результат записывается **на диск**, что может стать узким местом при выполнении репартиционирования\n",
    "\n",
    "## Встроенные функции\n",
    "Помимо базовых SQL операторов, в Spark существует большой набор встроенных функций:\n",
    "+ API методы из [org.apache.spark.sql.functions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$)\n",
    "+ [SQL built-in functions](https://spark.apache.org/docs/latest/api/sql/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|pmod|\n",
      "+---+----+\n",
      "|  0|null|\n",
      "|  1|   0|\n",
      "|  2|   0|\n",
      "|  3|   0|\n",
      "|  4|   0|\n",
      "|  5|   0|\n",
      "|  6|   0|\n",
      "|  7|   0|\n",
      "|  8|   0|\n",
      "|  9|   0|\n",
      "+---+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [id: bigint]\n",
       "newCol = pmod(id, id)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pmod(id, id)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.range(0,10)\n",
    "\n",
    "// используем org.apache.spark.sql.functions\n",
    "val newCol: Column = pmod(col(\"id\"), lit(2))\n",
    "df.withColumn(\"pmod\", newCol).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|pmod|\n",
      "+---+----+\n",
      "|  0|   0|\n",
      "|  1|   1|\n",
      "|  2|   0|\n",
      "|  3|   1|\n",
      "|  4|   0|\n",
      "|  5|   1|\n",
      "|  6|   0|\n",
      "|  7|   1|\n",
      "|  8|   0|\n",
      "|  9|   1|\n",
      "+---+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "newCol = pmod(id, 2)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pmod(id, 2)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.expr\n",
    "\n",
    "// используем SQL built-in functions\n",
    "val newCol: Column = expr(\"\"\"pmod(id, 2)\"\"\")\n",
    "df.withColumn(\"pmod\", newCol).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы\n",
    "+ Spark обладает широким набором функций для работы с колонками разных типов, включая простые типы - строки, числа, и т. д., а также словари, массивы и структуры\n",
    "+ Встроенные функции принимают колонки `org.apache.spark.sql.Column` и возвращают `org.apache.spark.sql.Column` в большинстве случаев\n",
    "+ Встроенные функции доступны в двух местах - org.apache.spark.sql.functions и SQL built-in functions\n",
    "+ Встроенные функции можно (и нужно) использовать вместе - на вход во встроенные функции могут подаваться результаты встроенной функции, тк все они возвращают `sql.Column` \n",
    "\n",
    "### Пользовательские функции\n",
    "\n",
    "В том случае, если функционала встроенных функций не хватает, можно написать пользовательскую функцию - UDF. Пользовательская функция может принимать до 16 аргументов. Соответствие Spark и Scala типов описано [здесь](https://spark.apache.org/docs/latest/sql-reference.html#data-types)\n",
    "\n",
    "Необходимо помнить, что `null` в Spark превращается в `null` внутри UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|idPlusOne|\n",
      "+---+---------+\n",
      "|  0|       14|\n",
      "|  1|       16|\n",
      "|  2|       18|\n",
      "|  3|       20|\n",
      "|  4|       22|\n",
      "|  5|       24|\n",
      "|  6|       26|\n",
      "|  7|       28|\n",
      "|  8|       30|\n",
      "|  9|       32|\n",
      "+---+---------+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "'Project [id#1920L, UDF('id, 'id, 14) AS idPlusOne#1932]\n",
      "+- Range (0, 10, step=1, splits=Some(2))\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: bigint, idPlusOne: bigint\n",
      "Project [id#1920L, if (((isnull(id#1920L) || isnull(id#1920L)) || isnull(cast(14 as bigint)))) null else UDF(id#1920L, id#1920L, cast(14 as bigint)) AS idPlusOne#1932L]\n",
      "+- Range (0, 10, step=1, splits=Some(2))\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [id#1920L, UDF(id#1920L, id#1920L, 14) AS idPlusOne#1932L]\n",
      "+- Range (0, 10, step=1, splits=Some(2))\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [id#1920L, UDF(id#1920L, id#1920L, 14) AS idPlusOne#1932L]\n",
      "+- *(1) Range (0, 10, step=1, splits=2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [id: bigint]\n",
       "plusOne = UserDefinedFunction(<function3>,LongType,Some(List(LongType, LongType, LongType)))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UserDefinedFunction(<function3>,LongType,Some(List(LongType, LongType, LongType)))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{udf, col}\n",
    "\n",
    "val df = spark.range(0,10)\n",
    "\n",
    "val plusOne = udf { (value: Long, v2: Long, v3: Long) => value + v2 + v3 }\n",
    "\n",
    "df.withColumn(\"idPlusOne\", plusOne(col(\"id\"), col(\"id\"), lit(10+2+2))).show()\n",
    "df.withColumn(\"idPlusOne\", plusOne(col(\"id\"), col(\"id\"), lit(10+2+2))).explain(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пользовательская функция может возвращать:\n",
    "+ простой тип - `String`, `Long`, `Float`, `Boolean` и т.д.\n",
    "+ массив - любые коллекции, наследующие `Seq[T]` - `List[T]`, `Vector[T]` и т. д.\n",
    "+ словарь - `Map[A,B]`\n",
    "+ инстанс `case class`'а\n",
    "+ Option[T]\n",
    "\n",
    "Реализуем функцию, которая возвращает имя хоста, на котором работает воркер:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "|id |hostname      |\n",
      "+---+--------------+\n",
      "|0  |spark-master-3|\n",
      "|1  |spark-master-3|\n",
      "|2  |spark-master-3|\n",
      "|3  |spark-master-3|\n",
      "|4  |spark-master-3|\n",
      "|5  |spark-master-3|\n",
      "|6  |spark-master-3|\n",
      "|7  |spark-master-3|\n",
      "|8  |spark-master-3|\n",
      "|9  |spark-master-3|\n",
      "+---+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hostname = UserDefinedFunction(<function0>,StringType,Some(List()))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UserDefinedFunction(<function0>,StringType,Some(List()))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.net.InetAddress\n",
    "\n",
    "val hostname = udf { () => InetAddress.getLocalHost().getHostName() }\n",
    "\n",
    "df.withColumn(\"hostname\", hostname()).show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы также можем использовать монады `Try[T]` и `Option[T]` и для написания пользовательской функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- divideTwoBy: long (nullable = true)\n",
      "\n",
      "+---+-----------+\n",
      "|id |divideTwoBy|\n",
      "+---+-----------+\n",
      "|0  |null       |\n",
      "|1  |2          |\n",
      "|2  |1          |\n",
      "|3  |0          |\n",
      "|4  |0          |\n",
      "|5  |0          |\n",
      "|6  |0          |\n",
      "|7  |0          |\n",
      "|8  |0          |\n",
      "|9  |0          |\n",
      "+---+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [id: bigint]\n",
       "divideTwoBy = UserDefinedFunction(<function1>,LongType,Some(List(LongType)))\n",
       "result = [id: bigint, divideTwoBy: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: bigint, divideTwoBy: bigint]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.util.Try\n",
    "import org.apache.spark.sql.functions.{udf, col}\n",
    "\n",
    "val df = spark.range(0,10)\n",
    "\n",
    "val divideTwoBy = udf { (inputValue: Long) => Try(2L / inputValue).toOption }\n",
    "\n",
    "val result = df.withColumn(\"divideTwoBy\", divideTwoBy(col(\"id\")))\n",
    "result.printSchema\n",
    "result.show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы\n",
    "+ Пользовательские функции позволяют реализовать произвольный алгоритм и использовать его в DF API\n",
    "+ Пользовательские функции работают медленнее встроенных, поскольку при использовании встроенных функций Spark использует ряд оптимизаций, например векторизацию вычислений на уровне CPU\n",
    "\n",
    "## Соединения\n",
    "\n",
    "Join'ы позволяют соединять два DF в один по заданным условиям.\n",
    "\n",
    "По типу условия join'ы делятся на:\n",
    "+ equ-join - соединение по равенству одного или более ключей\n",
    "+ non-equ join - соединение по условию, отличному от равенства одного или более ключей\n",
    "\n",
    "#### Виды соединений\n",
    "+ **BroadcastHashJoin**\n",
    "  - equ join\n",
    "  - broadcast\n",
    "+ **SortMergeJoin**\n",
    "  - equ join\n",
    "  - sortable keys\n",
    "+ **BroadcastNestedLoopJoin**\n",
    "  - non-equ join\n",
    "  - using broadcast\n",
    "+ **CartesianProduct**\n",
    "  - non-equ join\n",
    "\n",
    "Добавим новую колонку к датасету `airports`, в которой будет процент заданного типа аэропорта ко всем типам аэропорта по каждой стране. Первым шагом посчитаем число аэропортов каждого типа по стране:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+----------------+\n",
      "|type          |iso_country|cnt_country_type|\n",
      "+--------------+-----------+----------------+\n",
      "|small_airport |MP         |1               |\n",
      "|large_airport |GB         |27              |\n",
      "|heliport      |CH         |19              |\n",
      "|closed        |LT         |4               |\n",
      "|medium_airport|SS         |3               |\n",
      "+--------------+-----------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "aggTypeCountry = [type: string, iso_country: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[type: string, iso_country: string ... 1 more field]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{count, round, lit}\n",
    "\n",
    "val aggTypeCountry = airports.groupBy('type, 'iso_country).agg(count(\"*\").alias(\"cnt_country_type\"))\n",
    "\n",
    "aggTypeCountry.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посчитаем количество аэропортов по каждой стране:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|iso_country|cnt_country|\n",
      "+-----------+-----------+\n",
      "|MM         |75         |\n",
      "|DZ         |61         |\n",
      "|LT         |59         |\n",
      "|CI         |26         |\n",
      "|TC         |8          |\n",
      "+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "aggCountry = [iso_country: string, cnt_country: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[iso_country: string, cnt_country: bigint]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val aggCountry = airports.groupBy('iso_country).agg(count(\"*\").alias(\"cnt_country\"))\n",
    "aggCountry.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соединим получившиеся датасеты и получим процентное распределение типов аэропорта по стране"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+-------+\n",
      "|iso_country|type          |percent|\n",
      "+-----------+--------------+-------+\n",
      "|MP         |small_airport |9.09   |\n",
      "|GB         |large_airport |2.24   |\n",
      "|CH         |heliport      |21.84  |\n",
      "|LT         |closed        |6.78   |\n",
      "|SS         |medium_airport|6.52   |\n",
      "+-----------+--------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "percent = [iso_country: string, type: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[iso_country: string, type: string ... 1 more field]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val percent = \n",
    "    aggTypeCountry\n",
    "        .join(aggCountry, Seq(\"iso_country\"), \"inner\")\n",
    "        .select('iso_country, 'type, (round(lit(100) * 'cnt_country_type / 'cnt_country, 2).alias(\"percent\")))\n",
    "percent.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соединим полученный датасет с изначальным:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------------+-------+\n",
      "|ident  |iso_country|type          |percent|\n",
      "+-------+-----------+--------------+-------+\n",
      "|EGAC   |GB         |large_airport |2.24   |\n",
      "|EGCN   |GB         |large_airport |2.24   |\n",
      "|EGHI   |GB         |large_airport |2.24   |\n",
      "|EGNX   |GB         |large_airport |2.24   |\n",
      "|EGSS   |GB         |large_airport |2.24   |\n",
      "|EGTE   |GB         |large_airport |2.24   |\n",
      "|EGUL   |GB         |large_airport |2.24   |\n",
      "|EGVN   |GB         |large_airport |2.24   |\n",
      "|TT01   |MP         |small_airport |9.09   |\n",
      "|LSER   |CH         |heliport      |21.84  |\n",
      "|LSHC   |CH         |heliport      |21.84  |\n",
      "|LSHG   |CH         |heliport      |21.84  |\n",
      "|LSXL   |CH         |heliport      |21.84  |\n",
      "|LSXR   |CH         |heliport      |21.84  |\n",
      "|LSXW   |CH         |heliport      |21.84  |\n",
      "|HSFA   |SS         |medium_airport|6.52   |\n",
      "|ZLG    |EH         |closed        |16.67  |\n",
      "|MHCT   |HN         |closed        |3.16   |\n",
      "|MD-0001|MD         |closed        |10.0   |\n",
      "|SPIM   |PE         |large_airport |1.14   |\n",
      "+-------+-----------+--------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "result = [iso_country: string, type: string ... 11 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[iso_country: string, type: string ... 11 more fields]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result = airports.join(percent, Seq(\"iso_country\", \"type\"), \"left\")\n",
    "result.select('ident, 'iso_country, 'type, 'percent).sample(0.2).show(20, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во всех наших джойнах присутствует массив `Seq[String]`. Это синтаксических сахар, позволяющий не переименовывать колонки датасетов, а просто указать, что соединение будет делаться по колонкам с именами, входящим в массив.\n",
    "\n",
    "В общем случае условие джойна должно быть выражено в виде колонки `sql.Column`, например:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joinCondition = ((iso_country = iso_country) AND (type = type))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((iso_country = iso_country) AND (type = type))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Column\n",
    "val joinCondition: Column = (airports(\"iso_country\") === percent(\"iso_country\")) and (airports(\"type\") === percent(\"type\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------------+-------+\n",
      "|ident|iso_country|type          |percent|\n",
      "+-----+-----------+--------------+-------+\n",
      "|EGGD |GB         |large_airport |2.24   |\n",
      "|EGGW |GB         |large_airport |2.24   |\n",
      "|EGHH |GB         |large_airport |2.24   |\n",
      "|EGKK |GB         |large_airport |2.24   |\n",
      "|EGNM |GB         |large_airport |2.24   |\n",
      "|TT01 |MP         |small_airport |9.09   |\n",
      "|LSHG |CH         |heliport      |21.84  |\n",
      "|LSXR |CH         |heliport      |21.84  |\n",
      "|LSXW |CH         |heliport      |21.84  |\n",
      "|LSXY |CH         |heliport      |21.84  |\n",
      "|SPZO |PE         |large_airport |1.14   |\n",
      "|LOWG |AT         |medium_airport|4.79   |\n",
      "|EBCV |BE         |medium_airport|5.48   |\n",
      "|EBFS |BE         |medium_airport|5.48   |\n",
      "|HEAL |EG         |medium_airport|37.14  |\n",
      "|HEAR |EG         |medium_airport|37.14  |\n",
      "|HEBA |EG         |medium_airport|37.14  |\n",
      "|HEGS |EG         |medium_airport|37.14  |\n",
      "|HEMA |EG         |medium_airport|37.14  |\n",
      "|HESC |EG         |medium_airport|37.14  |\n",
      "+-----+-----------+--------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "*(7) Sample 0.0, 0.2, false, 6279013698421751137\n",
      "+- *(7) Project [ident#574, iso_country#579, type#575, percent#2033]\n",
      "   +- SortMergeJoin [type#575, iso_country#579], [type#2411, iso_country#2415], LeftOuter\n",
      "      :- *(2) Sort [type#575 ASC NULLS FIRST, iso_country#579 ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(type#575, iso_country#579, 200)\n",
      "      :     +- *(1) FileScan csv [ident#574,type#575,iso_country#579] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://spark-master-1.newprolab.com:8020/tmp/airport-codes_csv.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ident:string,type:string,iso_country:string>\n",
      "      +- *(6) Sort [type#2411 ASC NULLS FIRST, iso_country#2415 ASC NULLS FIRST], false, 0\n",
      "         +- *(6) Project [iso_country#2415, type#2411, round((cast((100 * cnt_country_type#1972L) as double) / cast(cnt_country#2004L as double)), 2) AS percent#2033]\n",
      "            +- *(6) BroadcastHashJoin [iso_country#2415], [iso_country#2022], Inner, BuildRight\n",
      "               :- *(6) HashAggregate(keys=[type#2411, iso_country#2415], functions=[count(1)])\n",
      "               :  +- Exchange hashpartitioning(type#2411, iso_country#2415, 200)\n",
      "               :     +- *(3) HashAggregate(keys=[type#2411, iso_country#2415], functions=[partial_count(1)])\n",
      "               :        +- *(3) Project [type#2411, iso_country#2415]\n",
      "               :           +- *(3) Filter (isnotnull(iso_country#2415) && isnotnull(type#2411))\n",
      "               :              +- *(3) FileScan csv [type#2411,iso_country#2415] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://spark-master-1.newprolab.com:8020/tmp/airport-codes_csv.csv], PartitionFilters: [], PushedFilters: [IsNotNull(iso_country), IsNotNull(type)], ReadSchema: struct<type:string,iso_country:string>\n",
      "               +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n",
      "                  +- *(5) HashAggregate(keys=[iso_country#2022], functions=[count(1)])\n",
      "                     +- Exchange hashpartitioning(iso_country#2022, 200)\n",
      "                        +- *(4) HashAggregate(keys=[iso_country#2022], functions=[partial_count(1)])\n",
      "                           +- *(4) Project [iso_country#2022]\n",
      "                              +- *(4) Filter isnotnull(iso_country#2022)\n",
      "                                 +- *(4) FileScan csv [iso_country#2022] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://spark-master-1.newprolab.com:8020/tmp/airport-codes_csv.csv], PartitionFilters: [], PushedFilters: [IsNotNull(iso_country)], ReadSchema: struct<iso_country:string>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "result2 = [ident: string, type: string ... 13 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, type: string ... 13 more fields]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result2 = airports.join(percent, joinCondition, \"left\")\n",
    "result2.select(airports(\"ident\"), airports(\"iso_country\"), airports(\"type\"), percent(\"percent\")).sample(0.2).show(20, false)\n",
    "result2.select(airports(\"ident\"), airports(\"iso_country\"), airports(\"type\"), percent(\"percent\")).sample(0.2).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При этом в данном выражении допускается использование встроенных функций, пользовательских функций и операторов сравнения. Однако следует помнить, что мы выполняем джойн двух распределенных датасетов и если условие соединения будет плохо составлено, то Spark выполнит `cross join`, производительность которого будет \"крайне мала\" &copy;\n",
    "\n",
    "### Выводы:\n",
    "+ Spark поддерживает большое число типов соединений\n",
    "+ Условием соединения может быть `Seq[String]`, либо `sql.Column`\n",
    "+ При использовании сложных условий соединения следует избегать тех, которые приведут к `cross join`\n",
    "\n",
    "## Оконные функции\n",
    "Оконные функции позволяют делать функции над \"окнами\" (кто бы мог подумать) данных\n",
    "\n",
    "Окно создается из класса [org.apache.spark.sql.expressions.Window](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.expressions.Window) с указанием полей, определяющих границы окон и полей, определяющих порядок сортировки внутри окна:\n",
    "\n",
    "```val window = Window.partitionBy(\"a\", \"b\").orderBy(\"a\")```\n",
    "\n",
    "Применяя окна, можно использовать такие полезные функции из [org.apache.spark.sql.functions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$), как ```lag()``` и ```lead()```, а также эффективно работать с данными time-series данными.\n",
    "\n",
    "Выполним задачу с вычисление процента отношения типов аэропортов, используя оконные функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1 AS `a`"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit(1).as(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:88: error: overloaded method value count with alternatives:\n",
       "  (columnName: String)org.apache.spark.sql.TypedColumn[Any,Long] <and>\n",
       "  (e: org.apache.spark.sql.Column)org.apache.spark.sql.Column\n",
       " cannot be applied to (Int)\n",
       "                       .withColumn(\"cnt_all\", count(1).over(w))\n",
       "                                              ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "val w = Window.partitionBy()\n",
    "val windowCountry = Window.partitionBy(\"iso_country\")\n",
    "val windowTypeCountry = Window.partitionBy(\"type\", \"iso_country\")\n",
    "\n",
    "val result = airports\n",
    "                .withColumn(\"cnt_country\", count(\"*\").over(windowCountry))\n",
    "                .withColumn(\"cnt_country_type\", count(\"*\").over(windowTypeCountry))\n",
    "                .withColumn(\"cnt_all\", count(lit(1)).over(w))\n",
    "                .withColumn(\"percent\", round(lit(100) * 'cnt_country_type / 'cnt_country, 2))\n",
    "                            \n",
    "result.select('ident, 'iso_country, 'type, 'percent, 'cnt_all).sample(0.2).show(20, false)\n",
    "result.select('ident, 'iso_country, 'type, 'percent, 'cnt_all).sample(0.2).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "+ Оконные функции позволяют применять функции, применительно к окнам данных\n",
    "+ Окно определяется списком колонок и сортировкой\n",
    "+ Применение оконных функций приводит к `shuffle`\n",
    "\n",
    "После завершения работы не забывайте останавливать `SparkSession`, чтобы освободить ресурсы кластера!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
