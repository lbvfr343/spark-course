{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import Row\n",
    "import json\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .config(conf=conf)\n",
    "         .appName(\"test\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master-5.newprolab.com:4047\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe80bef2518>"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, asc, desc, lower, lit, array\n",
    "from pyspark.ml.linalg import DenseVector, SparseVector\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, Normalizer, StopWordsRemover\n",
    "import pyspark.sql.functions as psf\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка датасета и его репартицирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_films = [[23126, u'en', u'Compass - powerful SASS library that makes your life easier'], [21617, u'en', u'Preparing for the AP* Computer Science A Exam \\u2014 Part 2'], [16627, u'es', u'Aprende Excel: Nivel Intermedio by Alfonso Rinsche'], [11556, u'es', u'Aprendizaje Colaborativo by UNID Universidad Interamericana para el Desarrollo'], [16704, u'ru', u'\\u041f\\u0440\\u043e\\u0433\\u0440\\u0430\\u043c\\u043c\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u043d\\u0430 Lazarus'], [13702, u'ru', u'\\u041c\\u0430\\u0442\\u0435\\u043c\\u0430\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u0430\\u044f \\u044d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u043a\\u0430']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"/labs/slaba02/DO_record_per_line.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.repartition(6)\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оставим только нужные столбцы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_desc_lang = df.select([c for c in df.columns if c in ['id','desc', 'lang']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чистка текста с помощью pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_en = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "stop_words_ru = StopWordsRemover.loadDefaultStopWords(\"russian\")\n",
    "stop_words_es = StopWordsRemover.loadDefaultStopWords(\"spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(data, column):\n",
    "    data=data.withColumn(column, lower(col('desc')))\n",
    "    data = data.withColumn(column, F.regexp_replace('desc', '[!@\"“’«»#$%&\\'()*+,—/:;<=>?^_`{|}~\\[\\]]', ''))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_desc_lang_clean = clean_text(id_desc_lang, 'desc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разделим датасет по языкам фильмов, для которых нужно дать рекомендацию, то есть на en и ru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_data = id_desc_lang_clean.filter(id_desc_lang_clean.lang == 'ru').select('id', 'desc')\n",
    "en_data = id_desc_lang_clean.filter(id_desc_lang_clean.lang == 'en').select('id', 'desc')\n",
    "es_data = id_desc_lang_clean.filter(id_desc_lang_clean.lang == 'es').select('id', 'desc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1231"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24553"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1374"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расчет количества уникальных слов в каждом датасете(это пригодиться при выборе кол-ва фичей для TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ru = ru_data.select('desc').collect()\n",
    "list_en = en_data.select('desc').collect()\n",
    "list_es = es_data.select('desc').collect()\n",
    "lst_full_language = [list_ru, list_en, list_es]\n",
    "#mvv_array = [int(row.mvv) for row in mvv_list.collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20335\n",
      "347337\n",
      "57222\n"
     ]
    }
   ],
   "source": [
    "for lst in lst_full_language:\n",
    "    set_lang = set()\n",
    "    for i in lst:\n",
    "        for j in i[0].split():\n",
    "            set_lang.add(j)\n",
    "    print(len(set_lang))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En = 347337\n",
    "\n",
    "Ru = 20335\n",
    "\n",
    "Es = 57222"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построение Pipeline(выход предыдущего шага является входом для следующего)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline for english films"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"desc\", outputCol=\"words\")\n",
    "\n",
    "swr_en = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"words_filtered_en\", stopWords=stop_words_en)\n",
    "\n",
    "hasher = HashingTF(numFeatures=347337, binary=False, inputCol=swr_en.getOutputCol(), outputCol=\"word_vector\")\n",
    "\n",
    "idf = IDF(inputCol=\"word_vector\", outputCol=\"features\")\n",
    "\n",
    "pipeline_en = Pipeline(stages=[\n",
    "    tokenizer,\n",
    "    swr_en,\n",
    "    hasher,\n",
    "    idf\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model_en = pipeline_en.fit(en_data)\n",
    "tr_df_en = pipeline_model_en.transform(en_data).select('id', 'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline for russian films"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"desc\", outputCol=\"words\")\n",
    "\n",
    "swr_en = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"words_filtered_en\", stopWords=stop_words_ru)\n",
    "\n",
    "hasher = HashingTF(numFeatures=20335, binary=False, inputCol=swr_en.getOutputCol(), outputCol=\"word_vector\")\n",
    "\n",
    "idf = IDF(inputCol=\"word_vector\", outputCol=\"features\")\n",
    "\n",
    "pipeline_ru = Pipeline(stages=[\n",
    "    tokenizer,\n",
    "    swr_en,\n",
    "    hasher,\n",
    "    idf\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model_ru = pipeline_ru.fit(ru_data)\n",
    "tr_df_ru = pipeline_model_ru.transform(ru_data).select('id', 'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline for spanish films"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"desc\", outputCol=\"words\")\n",
    "\n",
    "swr_en = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"words_filtered_en\", stopWords=stop_words_es)\n",
    "\n",
    "hasher = HashingTF(numFeatures=57222, binary=False, inputCol=swr_en.getOutputCol(), outputCol=\"word_vector\")\n",
    "\n",
    "idf = IDF(inputCol=\"word_vector\", outputCol=\"features\")\n",
    "\n",
    "pipeline_es = Pipeline(stages=[\n",
    "    tokenizer,\n",
    "    swr_en,\n",
    "    hasher,\n",
    "    idf\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model_es = pipeline_es.fit(es_data)\n",
    "tr_df_es = pipeline_model_es.transform(es_data).select('id', 'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расчет косинусного расстояния"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(v,u):\n",
    "    return float(v.dot(u) / (v.norm(2) * u.norm(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обрабатывается рекомендаций: 1\n",
      "Обрабатывается рекомендаций: 2\n",
      "Обрабатывается рекомендаций: 3\n",
      "Обрабатывается рекомендаций: 4\n",
      "Обрабатывается рекомендаций: 5\n",
      "Обрабатывается рекомендаций: 6\n"
     ]
    }
   ],
   "source": [
    "film_recomendation = {}\n",
    "count = 0\n",
    "for i in my_films:\n",
    "    count += 1\n",
    "    print(\"Обрабатывается рекомендаций:\", count)\n",
    "    if i[1] == 'ru':\n",
    "        df_lang = tr_df_ru\n",
    "    elif i[1] == 'en':\n",
    "        df_lang = tr_df_en\n",
    "    elif i[1] == 'es':\n",
    "        df_lang = tr_df_es\n",
    "    lst_id = []\n",
    "    lst_cosine = []\n",
    "    f_vec = df_lang.filter(df_lang.id == i[0]).collect()[0]['features']\n",
    "    for itertator in df_lang.filter(df_lang.id != i[0]).collect():\n",
    "        lst_id.append(itertator['id'])\n",
    "        lst_cosine.append(cos_sim(itertator['features'], f_vec))\n",
    "    res = sqlContext.createDataFrame(zip(lst_id, lst_cosine), schema=['id', 'cos'])\n",
    "    res = res.repartition(6)\n",
    "    res = res.dropna()\n",
    "    res = res.sort(\"cos\", ascending=False).collect()[0:10]\n",
    "    lst = []\n",
    "    for j in res:\n",
    "        lst.append(j[0])\n",
    "    film_recomendation[i[0]] = lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[23126, 'en', 'Compass - powerful SASS library that makes your life easier'],\n",
       " [21617, 'en', 'Preparing for the AP* Computer Science A Exam — Part 2'],\n",
       " [16627, 'es', 'Aprende Excel: Nivel Intermedio by Alfonso Rinsche'],\n",
       " [11556,\n",
       "  'es',\n",
       "  'Aprendizaje Colaborativo by UNID Universidad Interamericana para el Desarrollo'],\n",
       " [16704, 'ru', 'Программирование на Lazarus'],\n",
       " [13702, 'ru', 'Математическая экономика']]"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_films"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{23126: [13665, 13782, 15909, 25782, 14760, 13348, 19270, 17499, 25071, 7153],\n",
       " 21617: [21609, 21608, 21616, 21492, 21703, 21675, 21506, 21624, 21623, 21630],\n",
       " 16627: [11431, 12247, 5687, 17964, 12660, 16694, 5558, 9563, 10738, 13529],\n",
       " 11556: [16488, 13461, 22710, 468, 10447, 23357, 11523, 19330, 12679, 9289],\n",
       " 16704: [1228, 1327, 20362, 1215, 13696, 1365, 26980, 1236, 8186, 875],\n",
       " 13702: [864, 21079, 1052, 8123, 1396, 1041, 1033, 13057, 1217, 8313]}"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "film_recomendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сохранение результата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lab02.json', 'w') as outfile:\n",
    "    json.dump(film_recomendation, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
