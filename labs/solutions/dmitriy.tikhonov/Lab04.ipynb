{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Лаба 4. Прогнозирование пола и возрастной категории — Spark Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T20:25:11.224110Z",
     "start_time": "2022-11-04T20:24:51.320143Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 --executor-memory 3g pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Данные и библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T20:25:11.229146Z",
     "start_time": "2022-11-04T20:25:11.225713Z"
    }
   },
   "outputs": [],
   "source": [
    "# Библиотеки \n",
    "from pyspark.sql.functions import json_tuple, from_json, get_json_object, col, explode, expr, \\\n",
    "collect_set, collect_list, regexp_replace, get_json_object, to_json, struct\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover, StringIndexer, IndexToString\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.linalg import Vector, DenseVector\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import json, pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T16:36:29.944913Z",
     "start_time": "2022-11-04T16:36:27.914777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "-rw-r--r--   3 hdfs hdfs  655090069 2022-01-06 18:46 /labs/slaba04/gender_age_dataset.txt\r\n"
     ]
    }
   ],
   "source": [
    "# папка с датасетом на HDFS\n",
    "!hdfs dfs -ls /labs/slaba04/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T16:36:32.041567Z",
     "start_time": "2022-11-04T16:36:29.947010Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender\tage\tuid\tuser_json\r\n",
      "F\t18-24\td50192e5-c44e-4ae8-ae7a-7cfe67c8b777\t{\"visits\": [{\"url\": \"http://zebra-zoya.ru/200028-chehol-organayzer-dlja-macbook-11-grid-it.html?utm_campaign=397720794&utm_content=397729344&utm_medium=cpc&utm_source=begun\", \"timestamp\": 1419688144068}, {\"url\": \"http://news.yandex.ru/yandsearch?cl4url=chezasite.com/htc/htc-one-m9-delay-86327.html&lr=213&rpt=story\", \"timestamp\": 1426666298001}, {\"url\": \"http://www.sotovik.ru/news/240283-htc-one-m9-zaderzhivaetsja.html\", \"timestamp\": 1426666298000}, {\"url\": \"http://news.yandex.ru/yandsearch?cl4url=chezasite.com/htc/htc-one-m9-delay-86327.html&lr=213&rpt=story\", \"timestamp\": 1426661722001}, {\"url\": \"http://www.sotovik.ru/news/240283-htc-one-m9-zaderzhivaetsja.html\", \"timestamp\": 1426661722000}]}\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "# датасет для обучения модели\n",
    "# Note: проверить таргет на наличие пустых значений\n",
    "!hdfs dfs -cat /labs/slaba04/gender_age_dataset.txt | head -n2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T16:36:32.453457Z",
     "start_time": "2022-11-04T16:36:32.043555Z"
    }
   },
   "outputs": [],
   "source": [
    "# формируем spark DF\n",
    "path = '/labs/slaba04/gender_age_dataset.txt'\n",
    "\n",
    "schema = t.StructType(fields=[\n",
    "    t.StructField('gender', t.StringType()),\n",
    "    t.StructField('age', t.StringType()),\n",
    "    t.StructField('uid', t.StringType()),\n",
    "    t.StructField('user_json', t.StringType()),\n",
    "])\n",
    "\n",
    "train_data = spark.read.csv(path, header=True, schema=schema, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T16:36:35.317128Z",
     "start_time": "2022-11-04T16:36:32.455309Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------------+--------------------+\n",
      "|gender|  age|                 uid|           user_json|\n",
      "+------+-----+--------------------+--------------------+\n",
      "|     F|18-24|d50192e5-c44e-4ae...|{\"visits\": [{\"url...|\n",
      "|     M|25-34|d502331d-621e-472...|{\"visits\": [{\"url...|\n",
      "+------+-----+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T16:36:35.322240Z",
     "start_time": "2022-11-04T16:36:35.318785Z"
    }
   },
   "outputs": [],
   "source": [
    "# схема для json с визитами\n",
    "visits_schema = t.StructType([\n",
    "    t.StructField('visits', t.ArrayType(\n",
    "        t.StructType([\n",
    "            t.StructField('url', t.StringType(), True),\n",
    "            t.StructField('timestamp', t.LongType(), True)\n",
    "        ])\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Формируем фичи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T16:51:55.030117Z",
     "start_time": "2022-11-04T16:51:38.767506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- uid: string (nullable = true)\n",
      " |-- hosts: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+------+---+--------------------+--------------------+\n",
      "|gender|age|                 uid|               hosts|\n",
      "+------+---+--------------------+--------------------+\n",
      "|     -|  -|13292e10-60bf-435...|[dateandtime.info...|\n",
      "|     -|  -|13f91463-8386-44c...|[go.mail.ru, pass...|\n",
      "|     -|  -|1dec593b-4dc5-41b...|[www.sport-expres...|\n",
      "+------+---+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# извлечем из визитов пользователя URL, чтобы применить HashingTF\n",
    "train_df_parsed = (\n",
    "    train_data\n",
    "    .withColumn('visits', from_json(col('user_json'), visits_schema))\n",
    "    .withColumn('visit', explode('visits.visits').alias('visit'))\n",
    "    .withColumn('host', expr('parse_url(visit.url, \"HOST\")').alias('host'))\n",
    "    .drop('visits', 'visit', 'user_json')\n",
    ")\n",
    "# используем explode, parse_url, collect_list\n",
    "train_df_final = (\n",
    "    train_df_parsed\n",
    "    .groupBy('gender', 'age', 'uid')\n",
    "    .agg(collect_list('host')\n",
    "    .alias('hosts'))\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "train_df_final.printSchema()\n",
    "train_df_final.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T20:25:16.788711Z",
     "start_time": "2022-11-04T20:25:16.720856Z"
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o7056.fit.\n: java.lang.IllegalStateException: SparkContext has been shutdown\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2053)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:370)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:370)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.PairRDDFunctions.countByKey(PairRDDFunctions.scala:369)\n\tat org.apache.spark.rdd.RDD$$anonfun$countByValue$1.apply(RDD.scala:1259)\n\tat org.apache.spark.rdd.RDD$$anonfun$countByValue$1.apply(RDD.scala:1259)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.countByValue(RDD.scala:1258)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:140)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-182-a2d3ed94bd87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                              \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ageIndex'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                             )\n\u001b[0;32m----> 8\u001b[0;31m                \u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m               )\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o7056.fit.\n: java.lang.IllegalStateException: SparkContext has been shutdown\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2053)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:370)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:370)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.PairRDDFunctions.countByKey(PairRDDFunctions.scala:369)\n\tat org.apache.spark.rdd.RDD$$anonfun$countByValue$1.apply(RDD.scala:1259)\n\tat org.apache.spark.rdd.RDD$$anonfun$countByValue$1.apply(RDD.scala:1259)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.countByValue(RDD.scala:1258)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:140)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "# import and clean data + parse JSON column + extract URL from visits + Hashing TF + RandomForest \n",
    "train_df, test_df = train_df_final.randomSplit([0.8, 0.2], seed=42)\n",
    "hashing_TF = HashingTF(inputCol='hosts', outputCol='rawFeatures', numFeatures=10000, binary=False)\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "indexer_age = (StringIndexer(inputCol='age', \n",
    "                             outputCol='ageIndex'\n",
    "                            )\n",
    "               .fit(train_df_final)\n",
    "              )\n",
    "\n",
    "indexer_gender = (StringIndexer(inputCol='gender', \n",
    "                                outputCol='genderIndex'\n",
    "                               )\n",
    "                  .fit(train_df_final)\n",
    "                 )\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "rf_age = RandomForestClassifier(featuresCol = 'rawFeatures', \n",
    "                                labelCol = 'ageIndex',\n",
    "                                predictionCol='age_index_prediction', \n",
    "                                rawPredictionCol='age_index_raw_prediction',\n",
    "                                probabilityCol='age_probability'\n",
    "                               )\n",
    "\n",
    "rf_gender = RandomForestClassifier(featuresCol = 'rawFeatures', \n",
    "                                   labelCol = 'genderIndex',\n",
    "                                   predictionCol='gender_index_prediction', \n",
    "                                   rawPredictionCol='gender_index_raw_prediction',\n",
    "                                   probabilityCol='gender_probability'\n",
    "                                  )\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "converter_age = IndexToString(inputCol='age_index_prediction', \n",
    "                              outputCol='PredictedAge', \n",
    "                              labels=indexer_age.labels\n",
    "                             )\n",
    "\n",
    "converter_gender = IndexToString(inputCol='gender_index_prediction', \n",
    "                                 outputCol='PredictedGender', \n",
    "                                 labels=indexer_gender.labels\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T16:58:12.050467Z",
     "start_time": "2022-11-04T16:56:38.032022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- uid: string (nullable = true)\n",
      " |-- hosts: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- rawFeatures: vector (nullable = true)\n",
      " |-- ageIndex: double (nullable = false)\n",
      " |-- genderIndex: double (nullable = false)\n",
      " |-- age_index_raw_prediction: vector (nullable = true)\n",
      " |-- age_probability: vector (nullable = true)\n",
      " |-- age_index_prediction: double (nullable = false)\n",
      " |-- gender_index_raw_prediction: vector (nullable = true)\n",
      " |-- gender_probability: vector (nullable = true)\n",
      " |-- gender_index_prediction: double (nullable = false)\n",
      " |-- PredictedAge: string (nullable = true)\n",
      " |-- PredictedGender: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = (\n",
    "    Pipeline(\n",
    "        stages=[hashing_TF, indexer_age, indexer_gender, rf_age, rf_gender, converter_age, converter_gender]\n",
    "    )\n",
    ")\n",
    "\n",
    "model = pipeline.fit(train_df)\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T16:59:28.057861Z",
     "start_time": "2022-11-04T16:59:27.880642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------------+---------------+\n",
      "|gender|age|PredictedAge|PredictedGender|\n",
      "+------+---+------------+---------------+\n",
      "|     -|  -|       25-34|              M|\n",
      "|     -|  -|       25-34|              M|\n",
      "|     -|  -|       25-34|              M|\n",
      "+------+---+------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select('gender', 'age', 'PredictedAge', 'PredictedGender').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T16:59:43.094036Z",
     "start_time": "2022-11-04T16:59:38.919771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for age: 0.37466955058880075\n",
      "accuracy for gender: 0.48125450612833454\n"
     ]
    }
   ],
   "source": [
    "evaluator_age = (MulticlassClassificationEvaluator(\n",
    "    labelCol='ageIndex', \n",
    "    predictionCol='age_index_prediction', \n",
    "    metricName='accuracy')\n",
    "                )\n",
    "\n",
    "accuracy_age = evaluator_age.evaluate(predictions)\n",
    "\n",
    "evaluator_gender = (\n",
    "    MulticlassClassificationEvaluator(\n",
    "        labelCol='genderIndex', \n",
    "        predictionCol='gender_index_prediction', \n",
    "        metricName='accuracy')\n",
    ")\n",
    "accuracy_gender = evaluator_gender.evaluate(predictions)\n",
    "\n",
    "print('accuracy for age: ' + str(accuracy_age))\n",
    "print('accuracy for gender: ' + str(accuracy_gender))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T17:10:30.273626Z",
     "start_time": "2022-11-04T17:10:26.457363Z"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = 'dmitry.tikhonov/lab04'\n",
    "model.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T17:11:07.255291Z",
     "start_time": "2022-11-04T17:11:05.360188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxr-xr-x   - dmitriy.tikhonov dmitriy.tikhonov          0 2022-11-04 20:10 dmitry.tikhonov/lab04/metadata\r\n",
      "drwxr-xr-x   - dmitriy.tikhonov dmitriy.tikhonov          0 2022-11-04 20:10 dmitry.tikhonov/lab04/stages\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls dmitry.tikhonov/lab04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Kafka "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T20:25:29.772089Z",
     "start_time": "2022-11-04T20:25:29.769196Z"
    }
   },
   "outputs": [],
   "source": [
    "KAFKA_BOOTSTRAP_SERVER = 'spark-node-1.newprolab.com:6667'\n",
    "KAFKA_INPUT_TOPIC = 'input_dmitriy.tikhonov'\n",
    "KAFKA_OUTPUT_TOPIC = 'dmitriy.tikhonov'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T20:25:33.813225Z",
     "start_time": "2022-11-04T20:25:30.769307Z"
    }
   },
   "outputs": [],
   "source": [
    "# чтение в статическом режиме\n",
    "\n",
    "kafka_read_df = (\n",
    "    spark.read\n",
    "    .format('kafka')\n",
    "    .option('kafka.bootstrap.servers', KAFKA_BOOTSTRAP_SERVER)\n",
    "    .option('subscribe', KAFKA_INPUT_TOPIC)\n",
    "    .option('startingOffsets', 'earliest')\n",
    "    .option('failOnDataLoss', 'False')\n",
    "    .load()\n",
    "    .cache()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T20:25:41.996127Z",
     "start_time": "2022-11-04T20:25:33.814970Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count 35000\n",
      "+----+--------------------+--------------------+---------+------+--------------------+-------------+\n",
      "| key|               value|               topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+--------------------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 75 69 64 2...|input_dmitriy.tik...|        0|     0|2022-11-04 20:23:...|            0|\n",
      "|null|[7B 22 75 69 64 2...|input_dmitriy.tik...|        0|     1|2022-11-04 20:23:...|            0|\n",
      "|null|[7B 22 75 69 64 2...|input_dmitriy.tik...|        0|     2|2022-11-04 20:23:...|            0|\n",
      "+----+--------------------+--------------------+---------+------+--------------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('count',kafka_read_df.count())\n",
    "kafka_read_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T20:25:42.348189Z",
     "start_time": "2022-11-04T20:25:41.997643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 uid|              visits|\n",
      "+--------------------+--------------------+\n",
      "|bd7a30e1-a25d-4cb...|[[http://www.inte...|\n",
      "|bd7a6f52-45db-49b...|[[https://www.pac...|\n",
      "|bd7a7fd9-ab06-42f...|[[http://www.mk.r...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Парсинг бинарного файла из кафки\n",
    "\n",
    "event_schema = t.StructType([\n",
    "    t.StructField('uid', t.StringType(), True),\n",
    "    t.StructField('visits', t.StringType(), True),\n",
    "])\n",
    "\n",
    "\n",
    "visit_schema = t.ArrayType(\n",
    "    t.StructType([\n",
    "        t.StructField('url', t.StringType(), True),\n",
    "        t.StructField('timestamp', t.LongType(), True)\n",
    "    ])\n",
    ")\n",
    "\n",
    "\n",
    "clean_df = (\n",
    "    kafka_read_df\n",
    "    .select(f.col('value').cast('string').alias('value'))\n",
    "    .select(f.from_json(f.col('value'), event_schema).alias('event'))\n",
    "    .select(\n",
    "        'event.uid', \n",
    "        f.from_json(f.col('event.visits'), visit_schema).alias('visits')\n",
    "    )\n",
    ")\n",
    "\n",
    "clean_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T20:26:07.455334Z",
     "start_time": "2022-11-04T20:25:42.349786Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uid: string (nullable = true)\n",
      " |-- hosts: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|                 uid|               hosts|\n",
      "+--------------------+--------------------+\n",
      "|0108d217-e476-493...|[kvartblog.ru, kv...|\n",
      "|0192cc54-559c-4c8...|[metanol.lv, meta...|\n",
      "|019acd5e-be9a-4cd...|[www.russianfood....|\n",
      "+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# извлечение url\n",
    "prep_df = (\n",
    "    clean_df\n",
    "    .withColumn('visit', explode('visits').alias('visit'))\n",
    "    .withColumn('host', expr('parse_url(visit.url, \"HOST\")').alias('host'))\n",
    "    .drop('visits', 'visit')\n",
    "    .groupBy('uid')\n",
    "    .agg(collect_list('host').alias('hosts'))\n",
    ")\n",
    "\n",
    "prep_df.printSchema()\n",
    "prep_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T20:27:39.246115Z",
     "start_time": "2022-11-04T20:26:07.457720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----+\n",
      "|                 uid|gender|  age|\n",
      "+--------------------+------+-----+\n",
      "|0108d217-e476-493...|     M|25-34|\n",
      "|0192cc54-559c-4c8...|     M|25-34|\n",
      "|019acd5e-be9a-4cd...|     M|25-34|\n",
      "+--------------------+------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"uid\":\"0108d217-...|\n",
      "|{\"uid\":\"0192cc54-...|\n",
      "|{\"uid\":\"019acd5e-...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# применение модели, сохранение предсказаний в predictions_df  \n",
    "inf_model = PipelineModel.load(MODEL_PATH)\n",
    "\n",
    "predictions_df = (\n",
    "    inf_model.transform(prep_df)\n",
    "    .select('uid', 'PredictedGender', 'PredictedAge')\n",
    "    .withColumnRenamed('PredictedAge', 'age')\n",
    "    .withColumnRenamed('PredictedGender', 'gender')\n",
    ")\n",
    "\n",
    "predictions_df.show(3)\n",
    "# Оборачивание предсказания обратно в json\n",
    "\n",
    "kafka_out_df = (\n",
    "    predictions_df\n",
    "    .select(f.to_json(f.struct(*predictions_df.columns)).alias('value')).limit(5)\n",
    ")\n",
    "kafka_out_df.show(3)\n",
    "# Запись в выходной топик\n",
    "\n",
    "(\n",
    "    kafka_out_df\n",
    "    .write\n",
    "    .format('kafka')\n",
    "    .option('kafka.bootstrap.servers', KAFKA_BOOTSTRAP_SERVER)\n",
    "    .option('topic', KAFKA_OUTPUT_TOPIC)\n",
    "    .save()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T20:30:52.987912Z",
     "start_time": "2022-11-04T20:30:52.985039Z"
    }
   },
   "outputs": [],
   "source": [
    "# чтение стрима\n",
    "read_kafka_params = {\n",
    "    'kafka.bootstrap.servers': KAFKA_BOOTSTRAP_SERVER,\n",
    "    'subscribe': KAFKA_INPUT_TOPIC,\n",
    "    'startingOffsets': 'latest'\n",
    "}\n",
    "\n",
    "write_kafka_params = {\n",
    "    'kafka.bootstrap.servers': KAFKA_BOOTSTRAP_SERVER,\n",
    "    'topic': KAFKA_OUTPUT_TOPIC\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T20:30:53.442476Z",
     "start_time": "2022-11-04T20:30:53.296640Z"
    }
   },
   "outputs": [],
   "source": [
    "eventType = StructType([\n",
    "    StructField('uid', StringType(), True),\n",
    "    StructField('visits', StringType(), True)\n",
    "])\n",
    "\n",
    "visitType = ArrayType(\n",
    "    StructType([\n",
    "        StructField('url', StringType(), True),\n",
    "        StructField('timestamp', LongType(), True)  \n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T20:32:36.275146Z",
     "start_time": "2022-11-04T20:32:23.417297Z"
    }
   },
   "outputs": [],
   "source": [
    "inf_model = PipelineModel.load(MODEL_PATH)\n",
    "def write_in_batch(batch_df, batch_id):\n",
    "    clean_df = (\n",
    "    batch_df\n",
    "    .select(f.col('value').cast('string').alias('value'))\n",
    "    .select(f.from_json(f.col('value'), event_schema).alias('event'))\n",
    "    .select('event.uid',f.from_json(f.col('event.visits'), visit_schema).alias('visits')\n",
    "    ))\n",
    "    \n",
    "    prep_df = (\n",
    "    clean_df\n",
    "    .withColumn('visit', explode('visits').alias('visit'))\n",
    "    .withColumn('host', expr('parse_url(visit.url, \"HOST\")').alias('host'))\n",
    "    .drop('visits', 'visit')\n",
    "    .groupBy('uid')\n",
    "    .agg(collect_list('host').alias('hosts')))\n",
    "    \n",
    "    predictions_df = (\n",
    "    inf_model.transform(prep_df)\n",
    "    .select('uid', 'PredictedGender', 'PredictedAge')\n",
    "    .withColumnRenamed('PredictedAge', 'age')\n",
    "    .withColumnRenamed('PredictedGender', 'gender'))\n",
    "        \n",
    "    kafka_df = (\n",
    "        predictions_df\n",
    "        .select(to_json(struct(*predictions_df.columns)).alias('value'))\n",
    "    )\n",
    "       \n",
    "    kafka_df\\\n",
    "     .write\\\n",
    "     .format('kafka')\\\n",
    "     .options(**write_kafka_params)\\\n",
    "     .mode('append')\\\n",
    "     .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T20:32:36.280079Z",
     "start_time": "2022-11-04T20:32:36.277273Z"
    }
   },
   "outputs": [],
   "source": [
    "def writestream_batch(df):\n",
    "    return df.writeStream\\\n",
    "            .foreachBatch(write_in_batch)\\\n",
    "            .option(\"checkpointLocation\", \"streaming/chk/chk_kafka_dmitriy.tikhonov_lab04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T20:32:36.309165Z",
     "start_time": "2022-11-04T20:32:36.281691Z"
    }
   },
   "outputs": [],
   "source": [
    "kafka_test_df = (spark\n",
    "    .readStream\n",
    "    .format('kafka')\n",
    "    .options(**read_kafka_params)\n",
    "    .option(\"failOnDataLoss\", 'False')\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T20:34:44.999965Z",
     "start_time": "2022-11-04T20:34:44.968354Z"
    }
   },
   "outputs": [],
   "source": [
    "wb = writestream_batch(kafka_test_df)\n",
    "wbs = wb.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T20:34:52.246325Z",
     "start_time": "2022-11-04T20:34:52.241833Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Waiting for data to arrive',\n",
       " 'isDataAvailable': True,\n",
       " 'isTriggerActive': True}"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wbs.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T20:34:54.669197Z",
     "start_time": "2022-11-04T20:34:53.600805Z"
    }
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
