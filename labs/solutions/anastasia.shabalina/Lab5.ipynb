{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@5f46d224"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.col\n",
    "import org.apache.spark.sql.types.{DoubleType, IntegerType}\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.classification.{GBTClassificationModel, GBTClassifier}\n",
    "import org.apache.spark.mllib.tree.configuration.BoostingStrategy\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
    "import org.apache.spark.sql.functions.{col, udf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "train_df = [_c0: string, ID: string ... 115 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[_c0: string, ID: string ... 115 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val train_df = spark.read\n",
    ".option(\"delimiter\", \",\")\n",
    ".option(\"header\", true)\n",
    ".csv(\"/labs/slaba05/lab05_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_df = [_c0: string, ID: string ... 114 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[_c0: string, ID: string ... 114 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val test_df = spark.read\n",
    ".option(\"delimiter\", \",\")\n",
    ".option(\"header\", true)\n",
    ".csv(\"/labs/slaba05/lab05_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit model on train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fields_X_y = Array(CR_PROD_CNT_IL, AMOUNT_RUB_CLO_PRC, PRC_ACCEPTS_A_EMAIL_LINK, APP_REGISTR_RGN_CODE, PRC_ACCEPTS_A_POS, PRC_ACCEPTS_A_TK, TURNOVER_DYNAMIC_IL_1M, CNT_TRAN_AUT_TENDENCY1M, SUM_TRAN_AUT_TENDENCY1M, AMOUNT_RUB_SUP_PRC, PRC_ACCEPTS_A_AMOBILE, SUM_TRAN_AUT_TENDENCY3M, CLNT_TRUST_RELATION, PRC_ACCEPTS_TK, PRC_ACCEPTS_A_MTP, REST_DYNAMIC_FDEP_1M, CNT_TRAN_AUT_TENDENCY3M, CNT_ACCEPTS_TK, APP_MARITAL_STATUS, REST_DYNAMIC_SAVE_3M, CR_PROD_CNT_VCU, REST_AVG_CUR, CNT_TRAN_MED_TENDENCY1M, APP_KIND_OF_PROP_HABITATION, CLNT_JOB_POSITION_TYPE, AMOUNT_RUB_NAS_PRC, CLNT_JOB_POSITION, APP_DRIVING_LICENSE, TRANS_COUNT_SUP_PRC, APP_EDUCATION, CNT_TRAN_CLO_TENDENCY1M, SUM_TRAN_MED_TENDENCY1M, PRC_ACCEPTS_A_ATM, PRC_ACCEPTS_MTP, TRANS_COUNT_NAS_PRC, APP_TRAVEL_PASS, CNT_ACCEPT...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(CR_PROD_CNT_IL, AMOUNT_RUB_CLO_PRC, PRC_ACCEPTS_A_EMAIL_LINK, APP_REGISTR_RGN_CODE, PRC_ACCEPTS_A_POS, PRC_ACCEPTS_A_TK, TURNOVER_DYNAMIC_IL_1M, CNT_TRAN_AUT_TENDENCY1M, SUM_TRAN_AUT_TENDENCY1M, AMOUNT_RUB_SUP_PRC, PRC_ACCEPTS_A_AMOBILE, SUM_TRAN_AUT_TENDENCY3M, CLNT_TRUST_RELATION, PRC_ACCEPTS_TK, PRC_ACCEPTS_A_MTP, REST_DYNAMIC_FDEP_1M, CNT_TRAN_AUT_TENDENCY3M, CNT_ACCEPTS_TK, APP_MARITAL_STATUS, REST_DYNAMIC_SAVE_3M, CR_PROD_CNT_VCU, REST_AVG_CUR, CNT_TRAN_MED_TENDENCY1M, APP_KIND_OF_PROP_HABITATION, CLNT_JOB_POSITION_TYPE, AMOUNT_RUB_NAS_PRC, CLNT_JOB_POSITION, APP_DRIVING_LICENSE, TRANS_COUNT_SUP_PRC, APP_EDUCATION, CNT_TRAN_CLO_TENDENCY1M, SUM_TRAN_MED_TENDENCY1M, PRC_ACCEPTS_A_ATM, PRC_ACCEPTS_MTP, TRANS_COUNT_NAS_PRC, APP_TRAVEL_PASS, CNT_ACCEPT..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fields_X_y = train_df.schema.fieldNames.filter(x => !(x == \"ID\" || x == \"_c0\"))\n",
    "\n",
    "val train_df_casted = train_df.schema.filter(x => x.name != \"TARGET\").map(x => x.name).foldLeft(train_df) {\n",
    "    case (folded_df, column_name) => folded_df.withColumn(column_name, col(column_name).cast(DoubleType))}\n",
    "\n",
    "val train_df_casted_filled = train_df_casted.na.fill(0)\n",
    "\n",
    "val feature_names = train_df_casted_filled.drop(\"_c0\", \"ID\", \"TARGET\").schema.fieldNames\n",
    "val assembler: VectorAssembler = new VectorAssembler()\n",
    "                                        .setInputCols(feature_names)\n",
    "                                        .setOutputCol(\"features\")\n",
    "                                        .setHandleInvalid(\"error\")\n",
    "\n",
    "val model: GBTClassifier = new GBTClassifier()\n",
    "                                .setLabelCol(\"TARGET\")\n",
    "                                .setFeaturesCol(\"features\")\n",
    "                                .setPredictionCol(\"prediction_\")\n",
    "                                .setProbabilityCol(\"probability_\")\n",
    "                                .setRawPredictionCol(\"raw_prediction_\")\n",
    "\n",
    "val train_df_casted_filled_with_features = assembler.transform(train_df_casted_filled.drop(\"_c0\", \"ID\"))\n",
    "\n",
    "val train_df_casted_filled_with_features2 = train_df_casted_filled_with_features.drop(feature_names: _*)\n",
    "                                                .withColumn(\"TARGET\", col(\"TARGET\").cast(DoubleType))\n",
    "                                                .filter(col(\"TARGET\").isNotNull)\n",
    "\n",
    "val model_fitted = model.fit(train_df_casted_filled_with_features2)\n",
    "\n",
    "val train_to_check = model_fitted.transform(train_df_casted_filled_with_features2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate model on train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vecToArray = UserDefinedFunction(<function1>,ArrayType(DoubleType,false),Some(List(org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7)))\n",
       "metrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@49463220\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8265829492695888"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vecToArray = udf((xs: org.apache.spark.ml.linalg.Vector) => xs.toArray)\n",
    "\n",
    "val metrics = new BinaryClassificationMetrics(\n",
    "    train_to_check.select(vecToArray(col(\"probability_\")).getItem(1).alias(\"prob\"), col(\"TARGET\"))\n",
    "        .rdd.map(row => (row.getAs[Double](\"prob\"), row.getAs[Double](\"TARGET\"))))\n",
    "\n",
    "metrics.areaUnderROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply model on test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_df_casted = [_c0: double, ID: double ... 114 more fields]\n",
       "test_df_casted_filled = [_c0: double, ID: double ... 114 more fields]\n",
       "test_df_casted_filled_with_features = [_c0: double, ID: double ... 115 more fields]\n",
       "test_df_casted_filled_with_features2 = [_c0: double, ID: double ... 1 more field]\n",
       "test_to_check = [_c0: double, ID: double ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[_c0: double, ID: double ... 4 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val test_df_casted = test_df.schema.filter(x => x.name != \"TARGET\").map(x => x.name).foldLeft(test_df) {\n",
    "    case (folded_df, column_name) => folded_df.withColumn(column_name, col(column_name).cast(DoubleType))}\n",
    "\n",
    "val test_df_casted_filled = test_df_casted.na.fill(0)\n",
    "\n",
    "val test_df_casted_filled_with_features = assembler.transform(test_df_casted_filled)\n",
    "\n",
    "val test_df_casted_filled_with_features2 = test_df_casted_filled_with_features.drop(feature_names: _*)\n",
    "\n",
    "val test_to_check = model_fitted.transform(test_df_casted_filled_with_features2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------------------+--------------------+--------------------+-----------+\n",
      "|     _c0|      ID|            features|     raw_prediction_|        probability_|prediction_|\n",
      "+--------+--------+--------------------+--------------------+--------------------+-----------+\n",
      "|372289.0|519130.0|(114,[19,21,43,44...|[0.77871996434737...|[0.82598569244220...|        0.0|\n",
      "| 87204.0|234045.0|(114,[1,9,21,25,2...|[1.46755700663867...|[0.94955520086183...|        0.0|\n",
      "|254415.0|401256.0|(114,[1,7,8,9,11,...|[1.50742241243035...|[0.95324027872665...|        0.0|\n",
      "|404229.0|551070.0|(114,[43,44,45,46...|[1.34033211650541...|[0.93587599716558...|        0.0|\n",
      "|220444.0|367285.0|(114,[21,46,61,67...|[1.47093893863527...|[0.94987820736100...|        0.0|\n",
      "+--------+--------+--------------------+--------------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_to_check.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save results to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.sql.AnalysisException\n",
       "Message: path hdfs://spark-master-1.newprolab.com:8020/user/anastasia.shabalina/lab05_test.csv already exists.;\n",
       "StackTrace:   at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:114)\n",
       "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
       "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
       "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\n",
       "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)\n",
       "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n",
       "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:696)\n",
       "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:305)\n",
       "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:291)\n",
       "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n",
       "  at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:684)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_to_check.select(col(\"ID\"),vecToArray(col(\"probability_\")).getItem(1).alias(\"target\"))\n",
    "    .write.option(\"header\",true).csv(\"lab05_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collected = Array([519130,0.17401430755779135], [234045,0.05044479913816702], [401256,0.04675972127334482], [551070,0.06412400283441888], [367285,0.05012179263899408], [497998,0.04980183358732948], [413082,0.09826151101411551], [349893,0.049872318800841864], [346337,0.13800344865519498], [289979,0.04879098018686989], [510818,0.1774711476452263], [235935,0.08371084224434533], [532135,0.24896083373726408], [564760,0.05405249493615094], [277391,0.14240831920696695], [336830,0.35779679612356996], [356053,0.04672938292276185], [293302,0.08213315353638717], [322368,0.04928306710303443], [406041,0.04672938292276185], [569179,0.05713474373137284], [191405,0.05536715881150078], [489011,0.04906123933359585], [265952,0.1915450574919848], [193718,0.07066298921978975...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array([519130,0.17401430755779135], [234045,0.05044479913816702], [401256,0.04675972127334482], [551070,0.06412400283441888], [367285,0.05012179263899408], [497998,0.04980183358732948], [413082,0.09826151101411551], [349893,0.049872318800841864], [346337,0.13800344865519498], [289979,0.04879098018686989], [510818,0.1774711476452263], [235935,0.08371084224434533], [532135,0.24896083373726408], [564760,0.05405249493615094], [277391,0.14240831920696695], [336830,0.35779679612356996], [356053,0.04672938292276185], [293302,0.08213315353638717], [322368,0.04928306710303443], [406041,0.04672938292276185], [569179,0.05713474373137284], [191405,0.05536715881150078], [489011,0.04906123933359585], [265952,0.1915450574919848], [193718,0.07066298921978975..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val collected = test_to_check.select(col(\"ID\").cast(IntegerType), vecToArray(col(\"probability_\")).getItem(1).alias(\"target\")).collect()\n",
    "\n",
    "val string_to_write = \"id\\ttarget\" + \"\\n\" + collected.map(x => Array(x(0), x(1))).map(y => y.mkString(\"\\t\")).mkString(\"\\n\")\n",
    "\n",
    "import java.io.PrintWriter\n",
    "new PrintWriter(\"lab05.csv\") { write(string_to_write); close }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
