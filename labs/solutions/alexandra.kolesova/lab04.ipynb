{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 2 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import Row\n",
    "from pyspark.sql.window import Window\n",
    "import json\n",
    "import re\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .config(conf=conf)\n",
    "         .appName(\"lab04\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master-4.newprolab.com:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f21444e0630>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "-rw-r--r--   3 hdfs hdfs  655090069 2022-01-06 18:46 /labs/slaba04/gender_age_dataset.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /labs/slaba04/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import & preproc data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"gender\", StringType(),True), \n",
    "    StructField(\"age\", StringType(),True), \n",
    "    StructField(\"uid\", StringType(),True), \n",
    "    StructField(\"user_json\", StringType(), True)\n",
    "])\n",
    "\n",
    "visit_schema = StructType([\n",
    "    StructField(\"visits\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"url\", StringType(),True), \n",
    "            StructField(\"timestamp\", LongType(),True)\n",
    "        ])\n",
    "    )\n",
    "               )\n",
    "])\n",
    "\n",
    "data = spark.read.csv('/labs/slaba04/gender_age_dataset.txt', schema = schema, sep = '\\t', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41138"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn('temp', F.from_json('user_json', schema=visit_schema))\n",
    "data = data.withColumn(\"visit\", F.explode(\"temp.visits\"))\n",
    "data = data.withColumn(\"host\", F.expr(\"parse_url(visit.url, 'HOST')\").alias(\"host\"))\n",
    "data = data.drop(*['temp', 'visit','user_json'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+------+\n",
      "|gender|age|uid|  host|\n",
      "+------+---+---+------+\n",
      "|     0|  0|  0|141843|\n",
      "+------+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#проверяем пропущенные значения\n",
    "from pyspark.sql.functions import col,isnan, when, count\n",
    "data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|  age|  count|\n",
      "+-----+-------+\n",
      "| >=55| 267941|\n",
      "|45-54| 797479|\n",
      "|    -| 516555|\n",
      "|35-44|1703954|\n",
      "|25-34|2057045|\n",
      "|18-24| 486533|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('age').groupBy('age').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|gender|  count|\n",
      "+------+-------+\n",
      "|     F|1950980|\n",
      "|     M|3361972|\n",
      "|     -| 516555|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('gender').groupBy('gender').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.filter((F.col('host').isNotNull())&(F.col('age')!= '-')&(F.col('age')!= '-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (data.groupBy(\"gender\", \"age\", \"uid\")\n",
    "        .agg(F.collect_list(\"host\")\n",
    "             .alias(\"hosts\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, HashingTF, IndexToString\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []\n",
    "gender_stringIndexer = StringIndexer(inputCol=\"gender\", outputCol=\"gender_indexed\")\n",
    "age_stringIndexer = StringIndexer(inputCol=\"age\", outputCol=\"age_indexed\")\n",
    "gender_labels = gender_stringIndexer.fit(data).labels\n",
    "age_labels = age_stringIndexer.fit(data).labels\n",
    "stages += [gender_stringIndexer, age_stringIndexer]\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"hosts\", outputCol=\"features\").setNumFeatures(10000)\n",
    "stages += [hashingTF]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_rf = RandomForestClassifier(labelCol = 'gender_indexed', \n",
    "                                   predictionCol='gender_prediction', \n",
    "                                   probabilityCol='gender_probability',\n",
    "                                   rawPredictionCol=\"gender_raw_prediction\")\n",
    "\n",
    "age_rf = RandomForestClassifier(labelCol = 'age_indexed', \n",
    "                                predictionCol='age_prediction', \n",
    "                                probabilityCol='age_probability',\n",
    "                                rawPredictionCol=\"age_raw_prediction\")\n",
    "\n",
    "stages += [gender_rf, age_rf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert indexed labels back to original labels.\n",
    "gender_converter = IndexToString(inputCol=\"gender_prediction\", outputCol=\"predictedGender\", labels=gender_labels)\n",
    "age_converter = IndexToString(inputCol=\"age_prediction\", outputCol=\"predictedAge\", labels=age_labels)\n",
    "\n",
    "stages += [gender_converter, age_converter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = stages)\n",
    "model = pipeline.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write().overwrite().save(\"/user/alexandra.kolesova/lab04_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_downloaded = PipelineModel.load(\"/user/alexandra.kolesova/lab04_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred= model_downloaded.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------------+--------------------+--------------+-----------+--------------------+---------------------+--------------------+-----------------+--------------------+--------------------+--------------+---------------+------------+\n",
      "|gender|  age|                 uid|               hosts|gender_indexed|age_indexed|            features|gender_raw_prediction|  gender_probability|gender_prediction|  age_raw_prediction|     age_probability|age_prediction|predictedGender|predictedAge|\n",
      "+------+-----+--------------------+--------------------+--------------+-----------+--------------------+---------------------+--------------------+-----------------+--------------------+--------------------+--------------+---------------+------------+\n",
      "|     F|18-24|09b1ecd3-b2d2-4c1...|[tankionline.com,...|           1.0|        2.0|(10000,[9509],[3.0])| [10.2228411483972...|[0.51114205741986...|              0.0|[8.73774176617548...|[0.43688708830877...|           0.0|              M|       25-34|\n",
      "|     F|18-24|15faf063-5e44-4b6...|[allods.mail.ru, ...|           1.0|        2.0|(10000,[444,974,1...| [10.4148423470896...|[0.52074211735448...|              0.0|[8.65781006879687...|[0.43289050343984...|           0.0|              M|       25-34|\n",
      "|     F|18-24|560142d9-6c9c-439...|[http, http, http...|           1.0|        2.0|(10000,[665,1844,...| [10.0122248156916...|[0.50061124078458...|              0.0|[8.48873354301635...|[0.42443667715081...|           0.0|              M|       25-34|\n",
      "|     F|18-24|6709f443-7ddd-423...|[muzofon.com, muz...|           1.0|        2.0|(10000,[5696],[5.0])| [10.2228411483972...|[0.51114205741986...|              0.0|[8.73774176617548...|[0.43688708830877...|           0.0|              M|       25-34|\n",
      "|     F|18-24|67e9bd68-ef03-49c...|[tempfile.ru, tem...|           1.0|        2.0|(10000,[1402,2706...| [10.2228411483972...|[0.51114205741986...|              0.0|[8.73774176617548...|[0.43688708830877...|           0.0|              M|       25-34|\n",
      "+------+-----+--------------------+--------------------+--------------+-----------+--------------------+---------------------+--------------------+-----------------+--------------------+--------------------+--------------+---------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_pred.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Age = 0.25945436190937554\n",
      "Accuracy Gender = 0.42633283966404983\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"age_indexed\", predictionCol=\"age_prediction\")\n",
    "accuracy_age = evaluator.evaluate(data_pred)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"gender_indexed\", predictionCol=\"gender_prediction\")\n",
    "accuracy_gender = evaluator.evaluate(data_pred)\n",
    "\n",
    "print(\"Accuracy Age = %s\" % (accuracy_age))\n",
    "print(\"Accuracy Gender = %s\" % (accuracy_gender))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 'alexandra.kolesova'\n",
    "kafka_bootstrap_servers = 'spark-master-1.newprolab.com:6667'\n",
    "subscribe = f'input_{topic}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_kafka_params = {\n",
    "   \"kafka.bootstrap.servers\": kafka_bootstrap_servers,\n",
    "   \"topic\": topic\n",
    "}\n",
    "\n",
    "read_kafka_params = {\n",
    "    \"kafka.bootstrap.servers\": kafka_bootstrap_servers,\n",
    "    \"subscribe\": subscribe,\n",
    "    \"startingOffsets\": \"latest\",\n",
    "    \"failOnDataLoss\": \"False\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PipelineModel.load(\"/user/alexandra.kolesova/lab04_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "  .option(\"subscribe\", subscribe) \\\n",
    ".option(\"failOnDataLoss\", 'False')\\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+---------+------+--------------------+-------------+\n",
      "| key|               value|               topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+--------------------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 75 69 64 2...|input_alexandra.k...|        0|     0|2022-11-05 08:34:...|            0|\n",
      "|null|[7B 22 75 69 64 2...|input_alexandra.k...|        0|     1|2022-11-05 08:34:...|            0|\n",
      "|null|[7B 22 75 69 64 2...|input_alexandra.k...|        0|     2|2022-11-05 08:34:...|            0|\n",
      "|null|[7B 22 75 69 64 2...|input_alexandra.k...|        0|     3|2022-11-05 08:34:...|            0|\n",
      "|null|[7B 22 75 69 64 2...|input_alexandra.k...|        0|     4|2022-11-05 08:34:...|            0|\n",
      "+----+--------------------+--------------------+---------+------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = df.selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"uid\", StringType(),True), \n",
    "    StructField(\"visits\", StringType(), True)\n",
    "])\n",
    "\n",
    "visit_schema = ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"url\", StringType(),True), \n",
    "            StructField(\"timestamp\", LongType(),True)\n",
    "        ])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = (\n",
    "    test_data\n",
    "    .select(F.from_json(F.col('value'), schema).alias('user'))\n",
    "    .select(\n",
    "        'user.uid', \n",
    "        F.from_json(F.col('user.visits'), visit_schema).alias('visits')\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.withColumn(\"visit\", F.explode(\"visits\"))\n",
    "test_data = test_data.withColumn(\"host\", F.expr(\"parse_url(visit.url, 'HOST')\").alias(\"host\"))\n",
    "test_data = test_data.drop(*['visits', 'visit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.filter(F.col('host').isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = (test_data.groupBy(\"uid\")\n",
    "             .agg(F.collect_list(\"host\")\n",
    "             .alias(\"hosts\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 uid|               hosts|\n",
      "+--------------------+--------------------+\n",
      "|0108d217-e476-493...|[kvartblog.ru, kv...|\n",
      "|0192cc54-559c-4c8...|[metanol.lv, meta...|\n",
      "|019acd5e-be9a-4cd...|[www.russianfood....|\n",
      "|02e7f830-da57-4d5...|[maxpark.com, max...|\n",
      "|1d160259-73d8-451...|[ua.sinoptik.ua, ...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PipelineModel.load(\"/user/alexandra.kolesova/lab04_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = test_pred.selectExpr('uid', 'predictedGender as gender', 'predictedAge as age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_json = (\n",
    "    test_pred.select(F.to_json(F.struct(*test_pred.columns)).alias('value'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"uid\":\"0108d217-...|\n",
      "|{\"uid\":\"0192cc54-...|\n",
      "|{\"uid\":\"019acd5e-...|\n",
      "|{\"uid\":\"02e7f830-...|\n",
      "|{\"uid\":\"1d160259-...|\n",
      "|{\"uid\":\"1e14a504-...|\n",
      "|{\"uid\":\"1eb313db-...|\n",
      "|{\"uid\":\"1eff6e4f-...|\n",
      "|{\"uid\":\"3e75c432-...|\n",
      "|{\"uid\":\"47565df3-...|\n",
      "|{\"uid\":\"4766a8ab-...|\n",
      "|{\"uid\":\"50637c81-...|\n",
      "|{\"uid\":\"5a023519-...|\n",
      "|{\"uid\":\"5a781caa-...|\n",
      "|{\"uid\":\"5ab3c7b8-...|\n",
      "|{\"uid\":\"7302e78a-...|\n",
      "|{\"uid\":\"73081df3-...|\n",
      "|{\"uid\":\"89fe85cb-...|\n",
      "|{\"uid\":\"8affa6ce-...|\n",
      "|{\"uid\":\"b2e4450d-...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_pred_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_sdf = spark.readStream.format(\"kafka\").options(**read_kafka_params).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test_data = kafka_sdf.selectExpr(\"CAST(value AS STRING)\")\n",
    "kafka_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = kafka_sdf.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"uid\", StringType(),True), \n",
    "    StructField(\"visits\", StringType(), True)\n",
    "])\n",
    "\n",
    "visit_schema = ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"url\", StringType(),True), \n",
    "            StructField(\"timestamp\", LongType(),True)\n",
    "        ])\n",
    "    )\n",
    "\n",
    "test_data = (\n",
    "    test_data\n",
    "    .select(F.from_json(F.col('value'), schema).alias('user'))\n",
    "    .select(\n",
    "        'user.uid', \n",
    "        F.from_json(F.col('user.visits'), visit_schema).alias('visits')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.withColumn(\"visit\", F.explode(\"visits\"))\n",
    "test_data = test_data.withColumn(\"host\", F.expr(\"parse_url(visit.url, 'HOST')\").alias(\"host\"))\n",
    "test_data = test_data.drop(*['visits', 'visit'])\n",
    "\n",
    "test_data = test_data.filter(F.col('host').isNotNull())\n",
    "\n",
    "test_data = (test_data.groupBy(\"uid\")\n",
    "             .agg(F.collect_list(\"host\")\n",
    "             .alias(\"hosts\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.transform(test_data)\n",
    "test_pred = test_pred.selectExpr('uid', 'predictedGender as gender', 'predictedAge as age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_json = (\n",
    "    test_pred.select(F.to_json(F.struct(*test_pred.columns)).alias('value'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f2133b28e80>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_pred_json.writeStream.format(\"kafka\")\n",
    " .options(**write_kafka_params)\n",
    " .option(\"checkpointLocation\", \"checkpoints/checkpoint_lab04/3\")\n",
    " .outputMode(\"complete\").start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
