{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 --executor-memory 5g --driver-memory 2g pyspark-shell'\n",
    " \n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "    \n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"GP Lab03\") \n",
    "conf.set(\"spark.sql.crossJoin.enabled\", \"True\") \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).appName(\"GP Lab03\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master-5.newprolab.com:4052\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>GP Lab03</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3851375400>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "-rw-r--r--   3 hdfs hdfs   91066524 2022-01-06 18:46 /labs/slaba03/laba03_items.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs   29965581 2022-01-06 18:46 /labs/slaba03/laba03_test.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs   74949368 2022-01-06 18:46 /labs/slaba03/laba03_train.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs  871302535 2022-01-06 18:46 /labs/slaba03/laba03_views_programmes.csv\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /labs/slaba03/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Читаем данные\n",
    "# items = spark.read.csv('/labs/slaba03/laba03_items.csv', header=True)\n",
    "# test = spark.read.csv('/labs/slaba03/laba03_test.csv')\n",
    "# train = spark.read.csv('/labs/slaba03/laba03_train.csv')\n",
    "# views_programmes = spark.read.csv('/labs/slaba03/laba03_views_programmes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_schema = StructType(fields = [\n",
    "    StructField('item_id', IntegerType()),\n",
    "    StructField('channel_id', IntegerType()),\n",
    "    StructField('datetime_availability_start', StringType()),                                    \n",
    "    StructField('datetime_availability_stop', StringType()), \n",
    "    StructField('datetime_show_start', StringType()),\n",
    "    StructField('datetime_show_stop', StringType()), \n",
    "    StructField('content_type', IntegerType()),                                   \n",
    "    StructField('title', StringType(), nullable=True),  \n",
    "    StructField('year', FloatType(), nullable=True), \n",
    "    StructField('genres', StringType()), \n",
    "    StructField('region_id', IntegerType(), nullable=True),                    \n",
    "]) \n",
    "\n",
    "# items = spark.read.csv('/labs/slaba03/laba03_items.csv', header=True, schema=items_schema) #\n",
    "\n",
    "df_items = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .schema(items_schema) \\\n",
    "    .load(\"/labs/slaba03/laba03_items.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items = df_items.filter(df_items.genres.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------------\n",
      " item_id                     | 65667                \n",
      " channel_id                  | null                 \n",
      " datetime_availability_start | 1970-01-01T00:00:00Z \n",
      " datetime_availability_stop  | 2018-01-01T00:00:00Z \n",
      " datetime_show_start         | null                 \n",
      " datetime_show_stop          | null                 \n",
      " content_type                | 1                    \n",
      " title                       | на пробах только ... \n",
      " year                        | 2013.0               \n",
      " genres                      | Эротика              \n",
      " region_id                   | null                 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_items.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType() \\\n",
    "    .add(\"user_id\", IntegerType(), True) \\\n",
    "    .add(\"item_id\", IntegerType(), True) \\\n",
    "    .add(\"purchase\", IntegerType(), True) \\\n",
    "\n",
    "df_user_trian = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(schema) \\\n",
    "    .load(\"/labs/slaba03/laba03_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_test = StructType() \\\n",
    "    .add(\"user_id\", IntegerType(), True) \\\n",
    "    .add(\"item_id\", IntegerType(), True) \\\n",
    "\n",
    "df_user_test = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(schema_test) \\\n",
    "    .load(\"/labs/slaba03/laba03_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "views_schema = StructType(fields = [StructField('user_id', IntegerType()),\n",
    "StructField('item_id', IntegerType()),\n",
    "StructField('ts_start', IntegerType()),\n",
    "StructField('ts_end', IntegerType()),\n",
    "StructField('item_type', StringType()),\n",
    "])\n",
    "\n",
    "df_views_programmes = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(views_schema) \\\n",
    "    .load(\"/labs/slaba03/laba03_views_programmes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+\n",
      "|user_id|item_id|purchase|\n",
      "+-------+-------+--------+\n",
      "|   1654|  74107|       0|\n",
      "|   1654|  89249|       0|\n",
      "|   1654|  99982|       0|\n",
      "|   1654|  89901|       0|\n",
      "|   1654| 100504|       0|\n",
      "+-------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_user_trian.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_trian.createOrReplaceTempView(\"df_user_trian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "querry = \"\"\" \n",
    "select \n",
    "a.purchase, count(*) as cnt\n",
    "from df_user_trian a \n",
    "group by a.purchase\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|purchase|    cnt|\n",
      "+--------+-------+\n",
      "|       1|  10904|\n",
      "|       0|5021720|\n",
      "+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(querry).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5032624"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user_trian.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w_feats = df_user_trian.join(df_items, on=\"item_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w_feats = train_w_feats.filter(\"content_type == 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w_feats.createOrReplaceTempView(\"train_w_feats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "querry = \"\"\" \n",
    "select \n",
    "a.purchase, a.content_type, count(*) as cnt\n",
    "from train_w_feats a \n",
    "group by a.purchase, a.content_type\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+-------+\n",
      "|purchase|content_type|    cnt|\n",
      "+--------+------------+-------+\n",
      "|       1|           1|  10576|\n",
      "|       0|           1|4977299|\n",
      "+--------+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(querry).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_w_feats[['genres']].map(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4987875"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_w_feats.select(f.split(\"genres\", \",\").alias(\"genres_list\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w_feats = train_w_feats.withColumn(\"genres_list\", f.split(\"genres\", \",\").alias(\"genres_list\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_purchase_df = train_w_feats.select(\"title\", 'purchase').groupBy(\"title\").agg(\n",
    "    f.sum(\"purchase\").alias(\"sum_item_purchase\")).orderBy(\"sum_item_purchase\", ascending=False)\n",
    "\n",
    "\n",
    "user_purchase_df = train_w_feats.select(\"user_id\", 'purchase').groupBy(\"user_id\").agg(\n",
    "    f.sum(\"purchase\").alias(\"sum_user_purchase\")).orderBy(\"sum_user_purchase\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w_feats = train_w_feats.join(item_purchase_df, how='left', on=\"title\")\n",
    "train_w_feats = train_w_feats.join(user_purchase_df, how='left', on=\"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "favor_genres_by_user = train_w_feats.select(\"user_id\", \"purchase\", \"genres_list\")\n",
    "favor_genres_by_user = favor_genres_by_user.filter(\"purchase = 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "favor_genres_by_user = favor_genres_by_user.select(\n",
    "    \"user_id\", f.explode(\"genres_list\").alias(\"genres_of_purchase\")).groupBy(\n",
    "    \"user_id\").pivot(\"genres_of_purchase\").agg(f.count('*')).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.linalg import DenseVector, SparseVector\n",
    "w = Window.partitionBy('user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "favor_genres_by_user = train_w_feats.filter(\"purchase = 1\").select(\"user_id\", \"genres_list\").select(\n",
    "    \"user_id\", f.explode(\"genres_list\").alias(\"genres_of_purchase\")).groupBy(\n",
    "    'user_id').agg(f.collect_list('genres_of_purchase').alias('all_genres_of_purchase'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol=\"all_genres_of_purchase\", outputCol=\"favoriete_genres_vector\", vocabSize=80, minDF=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cv.fit(favor_genres_by_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.transform(favor_genres_by_user)\n",
    "# result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark.sql.types as T\n",
    "\n",
    "# to_array = f.udf(lambda v: v.toArray().tolist(), T.ArrayType(T.FloatType()))\n",
    "# df = df.withColumn('features', to_array('features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_fpr_vector = df_items.filter(\"content_type = 1\").select(\n",
    "    \"item_id\", f.split(\"genres\", \",\").alias(\"all_genres_of_purchase\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_result = model.transform(items_fpr_vector)\n",
    "\n",
    "item_result = item_result.withColumnRenamed(\"all_genres_of_purchase\",\"film_genre\")\\\n",
    ".withColumnRenamed(\"favoriete_genres_vector\",\"film_genre_vector\")\n",
    "\n",
    "# item_result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w_feats_s1 = train_w_feats.join(result, how=\"left\", on='user_id')\n",
    "train_w_feats_s2 = train_w_feats_s1.join(item_result, how=\"left\", on='item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция косинусного расстояния\n",
    "@f.udf(returnType=DoubleType())\n",
    "def vector_dot_val(v1,v2):\n",
    "    try:\n",
    "        v1_sparse = DenseVector(v1)\n",
    "        v2_sparse = DenseVector(v2)\n",
    "        return float(v1_sparse.dot(v2_sparse))\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w_feats_s2 = train_w_feats_s2.withColumn('vector_dot_val', vector_dot_val(train_w_feats_s2[\"favoriete_genres_vector\"], \n",
    "                                                                          train_w_feats_s2[\"film_genre_vector\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4987875"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_w_feats_s2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w_feats_s2.createOrReplaceTempView(\"train_w_feats_s2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['item_id',\n",
       " 'user_id',\n",
       " 'title',\n",
       " 'purchase',\n",
       " 'channel_id',\n",
       " 'datetime_availability_start',\n",
       " 'datetime_availability_stop',\n",
       " 'datetime_show_start',\n",
       " 'datetime_show_stop',\n",
       " 'content_type',\n",
       " 'year',\n",
       " 'genres',\n",
       " 'region_id',\n",
       " 'genres_list',\n",
       " 'sum_item_purchase',\n",
       " 'sum_user_purchase',\n",
       " 'all_genres_of_purchase',\n",
       " 'favoriete_genres_vector',\n",
       " 'film_genre',\n",
       " 'film_genre_vector',\n",
       " 'vector_dot_val']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_w_feats_s2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "querry = \"\"\" \n",
    "select \n",
    "a.user_id, a.item_id,\n",
    "a.sum_item_purchase, a.sum_user_purchase, a.vector_dot_val\n",
    "from train_w_feats_s2 a\n",
    "limit 15\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# spark.sql(querry).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w_feats_s3 = train_w_feats_s2[['user_id', 'item_id', 'sum_item_purchase', 'sum_user_purchase', 'vector_dot_val', \n",
    "                                    'purchase']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"sum_item_purchase\", \"sum_user_purchase\", \"vector_dot_val\"],\n",
    "                            outputCol=\"features\", handleInvalid='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w_feats_s4 = assembler.transform(train_w_feats_s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, item_id: int, sum_item_purchase: bigint, sum_user_purchase: bigint, vector_dot_val: double, purchase: int, features: vector]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_w_feats_s4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assembler = VectorAssembler(inputCols=[\"sum_item_purchase\", \"sum_user_purchase\", \"vector_dot_val\"],\n",
    "#                             outputCol=\"features\")\n",
    "\n",
    "# train_w_feats_s3 = assembler.transform(train_w_feats_s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"purchase\", maxIter=15, regParam=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = lr.fit(train_w_feats_s4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lr_model.transform(train_w_feats_s4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------------------\n",
      " item_id       | 100402                                     \n",
      " user_id       | 754230                                     \n",
      " purchase      | 0                                          \n",
      " prediction    | 0.0                                        \n",
      " probability   | [0.9924295884413139,0.0075704115586860184] \n",
      " rawPrediction | [4.875908633371725,-4.875908633371725]     \n",
      "-RECORD 1---------------------------------------------------\n",
      " item_id       | 93961                                      \n",
      " user_id       | 754230                                     \n",
      " purchase      | 0                                          \n",
      " prediction    | 0.0                                        \n",
      " probability   | [0.9939779621991357,0.006022037800864319]  \n",
      " rawPrediction | [5.10628932841722,-5.10628932841722]       \n",
      "-RECORD 2---------------------------------------------------\n",
      " item_id       | 90009                                      \n",
      " user_id       | 754230                                     \n",
      " purchase      | 0                                          \n",
      " prediction    | 0.0                                        \n",
      " probability   | [0.992941401251068,0.007058598748932003]   \n",
      " rawPrediction | [4.946425096154649,-4.946425096154649]     \n",
      "-RECORD 3---------------------------------------------------\n",
      " item_id       | 9832                                       \n",
      " user_id       | 754230                                     \n",
      " purchase      | 1                                          \n",
      " prediction    | 0.0                                        \n",
      " probability   | [0.992941401251068,0.007058598748932003]   \n",
      " rawPrediction | [4.946425096154649,-4.946425096154649]     \n",
      "-RECORD 4---------------------------------------------------\n",
      " item_id       | 89632                                      \n",
      " user_id       | 754230                                     \n",
      " purchase      | 0                                          \n",
      " prediction    | 0.0                                        \n",
      " probability   | [0.9952082885169308,0.004791711483069244]  \n",
      " rawPrediction | [5.3360643994840045,-5.3360643994840045]   \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"item_id\", \"user_id\", \"purchase\", \"prediction\", \"probability\", \"rawPrediction\").show(5, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"purchase\", metricName='areaUnderROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166672945571924"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Скоринг тестовой выборки "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_w_feats = df_user_test.join(df_items, on=\"item_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_w_feats = test_w_feats.filter(\"content_type == 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2137536"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_w_feats.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_w_feats = test_w_feats.withColumn(\"genres_list\", f.split(\"genres\", \",\").alias(\"genres_list\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_w_feats = test_w_feats.join(item_purchase_df, how='left', on=\"title\")\n",
    "test_w_feats = test_w_feats.join(user_purchase_df, how='left', on=\"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_w_feats_s1 = test_w_feats.join(result, how=\"left\", on='user_id')\n",
    "test_w_feats_s2 = test_w_feats_s1.join(item_result, how=\"left\", on='item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2137536"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_w_feats_s2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import isnan, when, count, col\n",
    "# test_w_feats_s2.select([count(when(isnan(c), c)).alias(c) for c in test_w_feats_s2.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_w_feats_s2.show(5, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_w_feats_s2 = test_w_feats_s2.withColumn('vector_dot_val', vector_dot_val(train_w_feats_s2[\"favoriete_genres_vector\"], \n",
    "                                                                          train_w_feats_s2[\"film_genre_vector\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_w_feats_s3 = test_w_feats_s2[['user_id', 'item_id', 'sum_item_purchase', 'sum_user_purchase', 'vector_dot_val']].cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_w_feats_s4 = assembler.transform(test_w_feats_s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = lr_model.transform(test_w_feats_s4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstelement = f.udf(lambda v:float(v[1]),FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = test_predictions.select(\"user_id\", \"item_id\", firstelement(\"probability\").alias(\"purchase\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = res.orderBy(\"user_id\", \"item_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------+\n",
      "|user_id|item_id|    purchase|\n",
      "+-------+-------+------------+\n",
      "|   1654|    336|0.0019944052|\n",
      "|   1654|    678|0.0019944052|\n",
      "|   1654|    691|0.0019848566|\n",
      "|   1654|    696|0.0020420174|\n",
      "|   1654|    763|0.0020612953|\n",
      "+-------+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1.repartition(1).write.option(\"header\",True).csv('/user/georgiy.krupenchenkov/lab03.csv', mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/home/georgiy.krupenchenkov\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -get /user/georgiy.krupenchenkov/lab03.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv lab03.csv/part-00000-6bac2eed-c058-40e7-93c9-1f86efb25d8d-c000.csv lab03_fin.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1836"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
