{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "config = SparkConf()\n",
    "config.set(\"spark.app.name\", \"Sergey Grishaev Spark RDD app\")\n",
    "\n",
    "sc = SparkContext(conf=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master-5.newprolab.com:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Sergey Grishaev Spark RDD app</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=Sergey Grishaev Spark RDD app>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.history.kerberos.keytab', 'none'),\n",
       " ('spark.eventLog.enabled', 'true'),\n",
       " ('spark.history.fs.cleaner.maxAge', '7d'),\n",
       " ('spark.history.ui.port', '18081'),\n",
       " ('spark.driver.extraLibraryPath',\n",
       "  '/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64'),\n",
       " ('spark.shuffle.io.serverThreads', '128'),\n",
       " ('spark.sql.streaming.streamingQueryListeners', ''),\n",
       " ('spark.executor.extraLibraryPath',\n",
       "  '/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64'),\n",
       " ('spark.jars.repositories', 'https://repos.spark-packages.org/'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  'http://spark-master-1.newprolab.com:8088/proxy/application_1665753715559_0141'),\n",
       " ('spark.shuffle.file.buffer', '1m'),\n",
       " ('spark.sql.hive.convertMetastoreOrc', 'true'),\n",
       " ('spark.yarn.dist.files', ''),\n",
       " ('spark.sql.autoBroadcastJoinThreshold', '26214400'),\n",
       " ('spark.history.fs.cleaner.interval', '1d'),\n",
       " ('spark.yarn.dist.jars',\n",
       "  'file:///data/home/petr.manannikov/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.11-2.4.7.jar,file:///data/home/petr.manannikov/.ivy2/jars/graphframes_graphframes-0.8.1-spark2.4-s_2.11.jar,file:///data/home/petr.manannikov/.ivy2/jars/databricks_spark-sklearn-0.2.3.jar,file:///data/home/petr.manannikov/.ivy2/jars/org.apache.kafka_kafka-clients-2.0.0.jar,file:///data/home/petr.manannikov/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,file:///data/home/petr.manannikov/.ivy2/jars/org.lz4_lz4-java-1.4.0.jar,file:///data/home/petr.manannikov/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar,file:///data/home/petr.manannikov/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar'),\n",
       " ('spark.ui.filters',\n",
       "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " ('spark.eventLog.dir', 'hdfs:///spark2-history/'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1665753715559_0141'),\n",
       " ('spark.driver.port', '40171'),\n",
       " ('spark.yarn.historyServer.address', 'spark-master-1.newprolab.com:18081'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.sql.orc.impl', 'native'),\n",
       " ('spark.history.fs.logDirectory', 'hdfs:///spark2-history/'),\n",
       " ('spark.yarn.dist.pyFiles',\n",
       "  'file:///data/home/petr.manannikov/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.11-2.4.7.jar,file:///data/home/petr.manannikov/.ivy2/jars/graphframes_graphframes-0.8.1-spark2.4-s_2.11.jar,file:///data/home/petr.manannikov/.ivy2/jars/databricks_spark-sklearn-0.2.3.jar,file:///data/home/petr.manannikov/.ivy2/jars/org.apache.kafka_kafka-clients-2.0.0.jar,file:///data/home/petr.manannikov/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,file:///data/home/petr.manannikov/.ivy2/jars/org.lz4_lz4-java-1.4.0.jar,file:///data/home/petr.manannikov/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar,file:///data/home/petr.manannikov/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  'spark-master-1.newprolab.com'),\n",
       " ('spark.extraListeners', ''),\n",
       " ('spark.executorEnv.PYTHONPATH',\n",
       "  '{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.7-src.zip<CPS>{{PWD}}/org.apache.spark_spark-sql-kafka-0-10_2.11-2.4.7.jar<CPS>{{PWD}}/graphframes_graphframes-0.8.1-spark2.4-s_2.11.jar<CPS>{{PWD}}/databricks_spark-sklearn-0.2.3.jar<CPS>{{PWD}}/org.apache.kafka_kafka-clients-2.0.0.jar<CPS>{{PWD}}/org.spark-project.spark_unused-1.0.0.jar<CPS>{{PWD}}/org.lz4_lz4-java-1.4.0.jar<CPS>{{PWD}}/org.xerial.snappy_snappy-java-1.1.7.5.jar<CPS>{{PWD}}/org.slf4j_slf4j-api-1.7.16.jar'),\n",
       " ('spark.sql.warehouse.dir', '/apps/spark/warehouse'),\n",
       " ('spark.history.store.path', '/var/lib/spark2/shs_db'),\n",
       " ('spark.port.maxRetries', '100'),\n",
       " ('spark.executor.instances', '3'),\n",
       " ('spark.driver.host', 'spark-master-5.newprolab.com'),\n",
       " ('spark.yarn.secondary.jars',\n",
       "  'org.apache.spark_spark-sql-kafka-0-10_2.11-2.4.7.jar,graphframes_graphframes-0.8.1-spark2.4-s_2.11.jar,databricks_spark-sklearn-0.2.3.jar,org.apache.kafka_kafka-clients-2.0.0.jar,org.spark-project.spark_unused-1.0.0.jar,org.lz4_lz4-java-1.4.0.jar,org.xerial.snappy_snappy-java-1.1.7.5.jar,org.slf4j_slf4j-api-1.7.16.jar'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///data/home/petr.manannikov/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.11-2.4.7.jar,file:///data/home/petr.manannikov/.ivy2/jars/graphframes_graphframes-0.8.1-spark2.4-s_2.11.jar,file:///data/home/petr.manannikov/.ivy2/jars/databricks_spark-sklearn-0.2.3.jar,file:///data/home/petr.manannikov/.ivy2/jars/org.apache.kafka_kafka-clients-2.0.0.jar,file:///data/home/petr.manannikov/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,file:///data/home/petr.manannikov/.ivy2/jars/org.lz4_lz4-java-1.4.0.jar,file:///data/home/petr.manannikov/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar,file:///data/home/petr.manannikov/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar'),\n",
       " ('spark.sql.statistics.fallBackToHdfs', 'true'),\n",
       " ('spark.app.name', 'Sergey Grishaev Spark RDD app'),\n",
       " ('spark.history.provider',\n",
       "  'org.apache.spark.deploy.history.FsHistoryProvider'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.extraClassPath', ''),\n",
       " ('spark.sql.hive.metastore.jars',\n",
       "  '/usr/hdp/current/spark2-client/standalone-metastore/*'),\n",
       " ('spark.submit.pyFiles',\n",
       "  '/data/home/petr.manannikov/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.11-2.4.7.jar,/data/home/petr.manannikov/.ivy2/jars/graphframes_graphframes-0.8.1-spark2.4-s_2.11.jar,/data/home/petr.manannikov/.ivy2/jars/databricks_spark-sklearn-0.2.3.jar,/data/home/petr.manannikov/.ivy2/jars/org.apache.kafka_kafka-clients-2.0.0.jar,/data/home/petr.manannikov/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,/data/home/petr.manannikov/.ivy2/jars/org.lz4_lz4-java-1.4.0.jar,/data/home/petr.manannikov/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar,/data/home/petr.manannikov/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar'),\n",
       " ('spark.yarn.queue', 'default'),\n",
       " ('spark.app.id', 'application_1665753715559_0141'),\n",
       " ('spark.history.fs.cleaner.enabled', 'true'),\n",
       " ('spark.sql.queryExecutionListeners', ''),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.io.compression.lz4.blockSize', '128kb'),\n",
       " ('spark.history.kerberos.principal', 'none'),\n",
       " ('spark.jars.packages',\n",
       "  'org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.7,graphframes:graphframes:0.8.1-spark2.4-s_2.11,databricks:spark-sklearn:0.2.3'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.sql.orc.filterPushdown', 'true'),\n",
       " ('spark.shuffle.io.backLog', '8192'),\n",
       " ('spark.unsafe.sorter.spill.reader.buffer.size', '1m'),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.shuffle.unsafe.file.output.buffer', '5m'),\n",
       " ('spark.driver.appUIAddress', 'http://spark-master-5.newprolab.com:4043'),\n",
       " ('spark.executor.extraJavaOptions', '-XX:+UseNUMA'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.sql.hive.metastore.version', '3.0')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/home/petr.manannikov\r\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/labs/laba01/ml-100k/u.data MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.textFile(\"/labs/laba01/ml-100k/u.data\")\n",
    "rdd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['196\\t242\\t3\\t881250949',\n",
       " '186\\t302\\t3\\t891717742',\n",
       " '22\\t377\\t1\\t878887116',\n",
       " '244\\t51\\t2\\t880606923']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = (rdd1.filter(lambda x: x.split(\"\\t\")[1]==\"96\")\n",
    "         .map(lambda x: (x.split(\"\\t\")[2], 1))\\\n",
    "          .groupByKey()\n",
    "          .map(lambda x: (x[0], len(x[1])))\n",
    "          .collect()\n",
    "          \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 6), ('2', 20), ('3', 43), ('4', 123), ('5', 103)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "result ={}\n",
    "dic = dict(rdd2)\n",
    "dic.values()\n",
    "result['hist_film'] = list(dic.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3 = (rdd1.filter(lambda x: x)\n",
    "         .map(lambda x: (x.split(\"\\t\")[2], 1))\\\n",
    "          .groupByKey()\n",
    "          .map(lambda x: (x[0], len(x[1])))\n",
    "          .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 6110), ('2', 11370), ('3', 27145), ('4', 34174), ('5', 21201)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic2 = dict(rdd3)\n",
    "dic2.values()\n",
    "result['hist_all'] = list(dic2.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6110, 11370, 27145, 34174, 21201]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dic2.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"lab01.json\", \"w\") as f:\n",
    "    f.write(json.dumps(result))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
