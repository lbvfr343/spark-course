{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Лаба 4. Прогнозирование пола и возрастной категории — Spark Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 6 --executor-memory 12g pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master-5.newprolab.com:4048\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb96c0ed2e8>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Данные и библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Библиотеки \n",
    "import pyspark.sql.types as t\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StringIndexer, IndexToString\n",
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"sep\", \"\\t\").load(\"/labs/slaba04/gender_age_dataset.txt\")\n",
    "# df.select('user_json').show(1,False,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# датасет для обучения модели\n",
    "# Note: проверить таргет на наличие пустых значений\n",
    "# !hdfs dfs -cat /labs/slaba04/gender_age_dataset.txt | head -n2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# формируем spark DF\n",
    "path = '/labs/slaba04/gender_age_dataset.txt'\n",
    "\n",
    "schema = t.StructType(fields=[\n",
    "    t.StructField('gender', t.StringType()),\n",
    "    t.StructField('age', t.StringType()),\n",
    "    t.StructField('uid', t.StringType()),\n",
    "    t.StructField('user_json', t.StringType()),\n",
    "])\n",
    "\n",
    "train_data = spark.read.csv(path, header=True, schema=schema, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------------+--------------------+\n",
      "|gender|  age|                 uid|           user_json|\n",
      "+------+-----+--------------------+--------------------+\n",
      "|     F|18-24|d50192e5-c44e-4ae...|{\"visits\": [{\"url...|\n",
      "+------+-----+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql.functions import explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.types import DateType\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Формируем фичи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|gender|age  |uid                                 |user_json                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "+------+-----+------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|F     |18-24|d50192e5-c44e-4ae8-ae7a-7cfe67c8b777|{\"visits\": [{\"url\": \"http://zebra-zoya.ru/200028-chehol-organayzer-dlja-macbook-11-grid-it.html?utm_campaign=397720794&utm_content=397729344&utm_medium=cpc&utm_source=begun\", \"timestamp\": 1419688144068}, {\"url\": \"http://news.yandex.ru/yandsearch?cl4url=chezasite.com/htc/htc-one-m9-delay-86327.html&lr=213&rpt=story\", \"timestamp\": 1426666298001}, {\"url\": \"http://www.sotovik.ru/news/240283-htc-one-m9-zaderzhivaetsja.html\", \"timestamp\": 1426666298000}, {\"url\": \"http://news.yandex.ru/yandsearch?cl4url=chezasite.com/htc/htc-one-m9-delay-86327.html&lr=213&rpt=story\", \"timestamp\": 1426661722001}, {\"url\": \"http://www.sotovik.ru/news/240283-htc-one-m9-zaderzhivaetsja.html\", \"timestamp\": 1426661722000}]}|\n",
      "+------+-----+------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.select('gender','age','uid', train_data[\"user_json\"]).show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# схема для json с визитами\n",
    "visits_schema = t.StructType([\n",
    "    t.StructField('visits', t.ArrayType(\n",
    "        t.StructType([\n",
    "            t.StructField('url', t.StringType(), True),\n",
    "            t.StructField('timestamp', t.LongType(), True)\n",
    "        ])\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|gender|age  |uid                                 |user_json                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "+------+-----+------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|F     |18-24|d50192e5-c44e-4ae8-ae7a-7cfe67c8b777|[[http://zebra-zoya.ru/200028-chehol-organayzer-dlja-macbook-11-grid-it.html?utm_campaign=397720794&utm_content=397729344&utm_medium=cpc&utm_source=begun, 1419688144068], [http://news.yandex.ru/yandsearch?cl4url=chezasite.com/htc/htc-one-m9-delay-86327.html&lr=213&rpt=story, 1426666298001], [http://www.sotovik.ru/news/240283-htc-one-m9-zaderzhivaetsja.html, 1426666298000], [http://news.yandex.ru/yandsearch?cl4url=chezasite.com/htc/htc-one-m9-delay-86327.html&lr=213&rpt=story, 1426661722001], [http://www.sotovik.ru/news/240283-htc-one-m9-zaderzhivaetsja.html, 1426661722000]]|\n",
      "+------+-----+------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data2=train_data.withColumn(\"user_json\",from_json(col(\"user_json\"),visits_schema)['visits'])\n",
    "train_data2.show(1,False,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- uid: string (nullable = true)\n",
      " |-- user_json: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------------+--------------------+-------------+\n",
      "|gender|  age|                 uid|                 url|    timestamp|\n",
      "+------+-----+--------------------+--------------------+-------------+\n",
      "|     F|18-24|d50192e5-c44e-4ae...|http://zebra-zoya...|1419688144068|\n",
      "|     F|18-24|d50192e5-c44e-4ae...|http://news.yande...|1426666298001|\n",
      "|     F|18-24|d50192e5-c44e-4ae...|http://www.sotovi...|1426666298000|\n",
      "|     F|18-24|d50192e5-c44e-4ae...|http://news.yande...|1426661722001|\n",
      "|     F|18-24|d50192e5-c44e-4ae...|http://www.sotovi...|1426661722000|\n",
      "|     M|25-34|d502331d-621e-472...|http://sweetradin...|1419717886224|\n",
      "|     M|25-34|d502331d-621e-472...|http://sweetradin...|1419717884437|\n",
      "|     M|25-34|d502331d-621e-472...|http://sweetradin...|1419717816375|\n",
      "|     M|25-34|d502331d-621e-472...|http://101.ru/?an...|1419717804934|\n",
      "|     M|25-34|d502331d-621e-472...|http://sweetradin...|1419714194423|\n",
      "|     M|25-34|d502331d-621e-472...|http://sweetradin...|1419713998481|\n",
      "|     M|25-34|d502331d-621e-472...|http://sweetradin...|1419713927085|\n",
      "|     M|25-34|d502331d-621e-472...|http://sweetradin...|1419713908863|\n",
      "|     M|25-34|d502331d-621e-472...|http://sweetradin...|1419713908679|\n",
      "|     M|25-34|d502331d-621e-472...|http://sweetradin...|1419713798879|\n",
      "|     M|25-34|d502331d-621e-472...|http://oesex.ru/9...|1419595564407|\n",
      "|     M|25-34|d502331d-621e-472...|http://www.interf...|1419542965224|\n",
      "|     M|25-34|d502331d-621e-472...|http://101.ru/?an...|1418818241900|\n",
      "|     M|25-34|d502331d-621e-472...|http://www.interf...|1418802080857|\n",
      "|     M|25-34|d502331d-621e-472...|http://www.euroav...|1418722961181|\n",
      "+------+-----+--------------------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data3 = train_data2.withColumn(\"user_json\", explode(\"user_json\")).select(\"*\", col(\"user_json\")[\"url\"].alias(\"url\"), col(\"user_json\")[\"timestamp\"].alias(\"timestamp\"))\n",
    "train_data4 = train_data3.select('gender','age','uid','url','timestamp')\n",
    "train_data4.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------------------------------------+------------------------+------+\n",
      "|gender|age  |uid                                 |host                    |target|\n",
      "+------+-----+------------------------------------+------------------------+------+\n",
      "|M     |35-44|03378baf-0b69-4f6c-b06a-eb539d9679a6|pddmaster.ru            |M35-44|\n",
      "|M     |35-44|03378baf-0b69-4f6c-b06a-eb539d9679a6|tgg.my1.ru              |M35-44|\n",
      "|M     |35-44|03378baf-0b69-4f6c-b06a-eb539d9679a6|www.cleaning-city.ru    |M35-44|\n",
      "|M     |35-44|03378baf-0b69-4f6c-b06a-eb539d9679a6|ledashop.ru             |M35-44|\n",
      "|M     |35-44|03378baf-0b69-4f6c-b06a-eb539d9679a6|alldbases.com           |M35-44|\n",
      "|M     |35-44|03378baf-0b69-4f6c-b06a-eb539d9679a6|motormaran.ru           |M35-44|\n",
      "|M     |35-44|03378baf-0b69-4f6c-b06a-eb539d9679a6|yandex.ru               |M35-44|\n",
      "|M     |35-44|03378baf-0b69-4f6c-b06a-eb539d9679a6|rusfishing.ru           |M35-44|\n",
      "|M     |35-44|03378baf-0b69-4f6c-b06a-eb539d9679a6|scootermag.ru           |M35-44|\n",
      "|M     |35-44|03378baf-0b69-4f6c-b06a-eb539d9679a6|www.motolegion.ru       |M35-44|\n",
      "|M     |35-44|03378baf-0b69-4f6c-b06a-eb539d9679a6|www.shop9val.ru         |M35-44|\n",
      "|M     |35-44|03378baf-0b69-4f6c-b06a-eb539d9679a6|www.mirkuzova.ru        |M35-44|\n",
      "|M     |35-44|03378baf-0b69-4f6c-b06a-eb539d9679a6|riafan.ru               |M35-44|\n",
      "|M     |35-44|03378baf-0b69-4f6c-b06a-eb539d9679a6|zacarbonim.ru           |M35-44|\n",
      "|M     |35-44|03378baf-0b69-4f6c-b06a-eb539d9679a6|alfisti.ru              |M35-44|\n",
      "|M     |35-44|03378baf-0b69-4f6c-b06a-eb539d9679a6|www.karcher.ru          |M35-44|\n",
      "|M     |35-44|03378baf-0b69-4f6c-b06a-eb539d9679a6|kolesa.kz               |M35-44|\n",
      "|M     |35-44|03378baf-0b69-4f6c-b06a-eb539d9679a6|lodki-viking.nethouse.ru|M35-44|\n",
      "|M     |35-44|03378baf-0b69-4f6c-b06a-eb539d9679a6|www.chisto.ru           |M35-44|\n",
      "|M     |35-44|03378baf-0b69-4f6c-b06a-eb539d9679a6|www.nikandr-motors.ru   |M35-44|\n",
      "|M     |35-44|03378baf-0b69-4f6c-b06a-eb539d9679a6|autodubok.ru            |M35-44|\n",
      "|M     |35-44|03378baf-0b69-4f6c-b06a-eb539d9679a6|www.youtube.com         |M35-44|\n",
      "|M     |35-44|03378baf-0b69-4f6c-b06a-eb539d9679a6|101prikaz.ru            |M35-44|\n",
      "|M     |35-44|03378baf-0b69-4f6c-b06a-eb539d9679a6|n51.adshostnet.com      |M35-44|\n",
      "|M     |35-44|03378baf-0b69-4f6c-b06a-eb539d9679a6|frezerovka.ru           |M35-44|\n",
      "|M     |35-44|03378baf-0b69-4f6c-b06a-eb539d9679a6|www.ebay.com            |M35-44|\n",
      "|M     |35-44|03378baf-0b69-4f6c-b06a-eb539d9679a6|uroki-online.com        |M35-44|\n",
      "|M     |35-44|03378baf-0b69-4f6c-b06a-eb539d9679a6|i-androidym.ru          |M35-44|\n",
      "|M     |35-44|03378baf-0b69-4f6c-b06a-eb539d9679a6|vseproaveo.ru           |M35-44|\n",
      "|M     |35-44|03378baf-0b69-4f6c-b06a-eb539d9679a6|www.maxivinyl.ru        |M35-44|\n",
      "+------+-----+------------------------------------+------------------------+------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data55 = train_data4.withColumn(\"host\", expr(\"regexp_extract(parse_url(url, 'HOST'),'(.*)')\"))\\\n",
    ".withColumn(\"target\", concat(col(\"gender\"), col(\"age\")))\\\n",
    ".select('gender','age','uid','host','target').where(\"host!='null'\").where(\"host!='https'\").where(\"host!='http'\").distinct()\n",
    "\n",
    "train_data55.where(\"uid ='03378baf-0b69-4f6c-b06a-eb539d9679a6'\").show(30,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------------------------------------+------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|gender|age  |uid                                 |target|host_list                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "+------+-----+------------------------------------+------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|M     |35-44|03378baf-0b69-4f6c-b06a-eb539d9679a6|M35-44|[auto-him.ru, www.avito.ru, enjoyflowers.ru, pddmaster.ru, ledashop.ru, motormaran.ru, rusfishing.ru, scootermag.ru, www.motolegion.ru, www.mirkuzova.ru, riafan.ru, zacarbonim.ru, 101prikaz.ru, www.ebay.com, uroki-online.com, www.maxivinyl.ru, www.vasko.ru, www.svyaznoy.ru, www.mgwm.ru, www.p-plotter.ru, www.gardenstock.ru, moto.infocar.ua, www.pddrussia.com, www.snowmobile.ru, allnokia.ru, www.neglers.net, otzovik.com, www.cleaning-city.ru, alldbases.com, alfisti.ru, www.karcher.ru, kolesa.kz, www.chisto.ru, i-androidym.ru, vseproaveo.ru, dic.academic.ru, ru.trovit.com, svetodiod.do.am, interweb.spb.ru, www.rusfishing.ru, tgg.my1.ru, yandex.ru, www.shop9val.ru, lodki-viking.nethouse.ru, www.nikandr-motors.ru, autodubok.ru, www.youtube.com, n51.adshostnet.com, frezerovka.ru, sata25.ru, baskino.com]|\n",
      "+------+-----+------------------------------------+------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data66 = train_data55.where(\"target!='--'\").groupBy('gender','age',\"uid\",'target')\\\n",
    ".agg(collect_list(\"host\").alias('host_list'))\n",
    "\n",
    "\n",
    "train_data66.where(\"uid ='03378baf-0b69-4f6c-b06a-eb539d9679a6'\").show(1,False,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF\n",
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht = HashingTF(inputCol=\"host_list\", outputCol=\"features\", numFeatures=10000)\n",
    "train_data7 = ht.transform(train_data66)\n",
    "# train_data7.show(1,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"target_\")\n",
    "pipeline = Pipeline(stages=[label_stringIdx])\n",
    "# Fit the pipeline to training documents.\n",
    "pipelineFit = pipeline.fit(train_data7)\n",
    "dataset = pipelineFit.transform(train_data7)\n",
    "# dataset.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_age_dic = dataset.select('gender','age',col('target_').alias('prediction')).distinct()\n",
    "# gender_age_dic.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset.sampleBy(\"target_\", fractions={0: 0.096, 1: 0.1207, 2: 0.151, 3: 0.185, 4: 0.265, 5:0.311, 6: 0.365, 7: 0.375, 8: 0.865, 9: 1}, seed=5757)\n",
    "# train = dataset.sampleBy(\"target_\", fractions={0: 0.4, 1: 0.5, 2: 0.7, 3: 0.6, 4: 1, 5:0.311, 6: 0.365, 7: 0.375, 8: 0.865, 9: 1}, seed=5757)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = train.select('uid','features','target_')\n",
    "# train2 = train2.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, ts = dataset.randomSplit(weights=[0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "841"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train2.where(\"target_==0\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "793"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train2.where(\"target_==1\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "794"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train2.where(\"target_==2\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "785"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train2.where(\"target_==3\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "747"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train2.where(\"target_==4\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "794"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train2.where(\"target_==5\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "764"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train2.where(\"target_==6\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "791"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train2.where(\"target_==7\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "775"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train2.where(\"target_==8\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "779"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train2.where(\"target_==9\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier, DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|                 uid|            features|target_|\n",
      "+--------------------+--------------------+-------+\n",
      "|07591afc-f28e-41d...|(10000,[893,3581,...|    4.0|\n",
      "|8ccf503e-2b77-46f...|(10000,[3427],[1.0])|    4.0|\n",
      "|f9b5782f-4be2-4cc...|(10000,[771,4361,...|    4.0|\n",
      "|416477cd-c500-495...|(10000,[43,93,196...|    1.0|\n",
      "|260efd0c-5fb5-4ca...|(10000,[38,154,45...|    3.0|\n",
      "|4b21a7d3-99cc-40e...|(10000,[973,1790,...|    3.0|\n",
      "|a3d6f72f-cde4-4eb...|(10000,[9,93,439,...|    3.0|\n",
      "|ff0255ad-9d0d-416...|(10000,[3055,9357...|    3.0|\n",
      "|d1d60f28-9f6a-467...|(10000,[267,1001,...|    5.0|\n",
      "|d23982af-56e6-4b2...|(10000,[23,31,35,...|    5.0|\n",
      "|eb9eb062-2a3e-4ad...|(10000,[3433,4811...|    5.0|\n",
      "|f738e48b-f177-4c5...|(10000,[91,450,54...|    5.0|\n",
      "|fc9e3e88-71f9-440...|(10000,[9,93,865,...|    5.0|\n",
      "|6d3ec8cb-a6e7-42e...|(10000,[6830],[1.0])|    7.0|\n",
      "|72183331-6898-49f...|(10000,[392,1790,...|    7.0|\n",
      "|09531aa4-91d2-4ff...|(10000,[8459,9506...|    0.0|\n",
      "|19377e29-5425-487...|(10000,[2864,6593...|    0.0|\n",
      "|69cabbb1-74ce-467...|(10000,[2694,3433...|    0.0|\n",
      "|b043c56c-b8e1-415...| (10000,[585],[1.0])|    0.0|\n",
      "|d0d0123c-749c-46e...|(10000,[3433],[1.0])|    0.0|\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train2 = train.select('uid','features','target_')\n",
    "train2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    \n",
    "      featuresCol='features'\n",
    "    , labelCol='target_'\n",
    "    , maxDepth=15\n",
    "    , maxBins=32\n",
    "    , minInstancesPerNode=1\n",
    "    , minInfoGain=0.0\n",
    "    , cacheNodeIds=False\n",
    "    , checkpointInterval=10\n",
    "    , impurity='entropy'\n",
    "    , numTrees=500\n",
    "    , featureSubsetStrategy='auto'\n",
    "    , seed=None\n",
    "    , subsamplingRate=1.0\n",
    "\n",
    ")\n",
    "\n",
    "model = rf.fit(train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.select('uid','target','target_','prediction').show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2577774246096388"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol = 'target_', predictionCol = 'prediction', metricName = 'accuracy')\n",
    "mlpacc = evaluator.evaluate(pred)\n",
    "mlpacc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Kafka "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_BOOTSTRAP_SERVER = 'spark-node-1.newprolab.com:6667'\n",
    "KAFKA_INPUT_TOPIC = 'input_petr.manannikov'\n",
    "KAFKA_OUTPUT_TOPIC = 'petr.manannikov'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_df.count()\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1784.load.\n: java.lang.NullPointerException\n\tat org.apache.spark.sql.kafka010.KafkaMicroBatchReader.<init>(KafkaMicroBatchReader.scala:72)\n\tat org.apache.spark.sql.kafka010.KafkaSourceProvider.createMicroBatchReader(KafkaSourceProvider.scala:143)\n\tat org.apache.spark.sql.kafka010.KafkaSourceProvider.createMicroBatchReader(KafkaSourceProvider.scala:44)\n\tat org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:182)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-64633467f938>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'subscribe'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKAFKA_INPUT_TOPIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'startingOffsets'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'earliest'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'failOnDataLoss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'False'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m )\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1784.load.\n: java.lang.NullPointerException\n\tat org.apache.spark.sql.kafka010.KafkaMicroBatchReader.<init>(KafkaMicroBatchReader.scala:72)\n\tat org.apache.spark.sql.kafka010.KafkaSourceProvider.createMicroBatchReader(KafkaSourceProvider.scala:143)\n\tat org.apache.spark.sql.kafka010.KafkaSourceProvider.createMicroBatchReader(KafkaSourceProvider.scala:44)\n\tat org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:182)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "# чтение стрима\n",
    "\n",
    "kafka_sdf = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format('kafka')\n",
    "    .option('kafka.bootstrap.servers', KAFKA_BOOTSTRAP_SERVER)\n",
    "    .option('subscribe', KAFKA_INPUT_TOPIC)\n",
    "    .option('startingOffsets', 'earliest')\n",
    "    .option('failOnDataLoss', 'False')\n",
    "    .load()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_schema = StructType([\n",
    "    StructField('uid', StringType(), True),\n",
    "    StructField('visits', StringType(), True),\n",
    "])\n",
    "\n",
    "\n",
    "visit_schema = ArrayType(\n",
    "    StructType([\n",
    "        StructField('url', StringType(), True),\n",
    "        StructField('timestamp', LongType(), True)\n",
    "    ])\n",
    ")\n",
    "\n",
    "\n",
    "clean_df = (\n",
    "    kafka_sdf\n",
    "    .select(f.col('value').cast('string').alias('value'))\n",
    "    .select(f.from_json(f.col('value'), event_schema).alias('event'))\n",
    "    .select(\n",
    "        'event.uid', \n",
    "        f.from_json(f.col('event.visits'), visit_schema).alias('visits')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1855,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Queries with streaming sources must be executed with writeStream.start();;\\nkafka'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o32121.showString.\n: org.apache.spark.sql.AnalysisException: Queries with streaming sources must be executed with writeStream.start();;\nkafka\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.org$apache$spark$sql$catalyst$analysis$UnsupportedOperationChecker$$throwError(UnsupportedOperationChecker.scala:389)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$$anonfun$checkForBatch$1.apply(UnsupportedOperationChecker.scala:38)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$$anonfun$checkForBatch$1.apply(UnsupportedOperationChecker.scala:36)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.checkForBatch(UnsupportedOperationChecker.scala:36)\n\tat org.apache.spark.sql.execution.QueryExecution.assertSupported(QueryExecution.scala:52)\n\tat org.apache.spark.sql.execution.QueryExecution.withCachedData$lzycompute(QueryExecution.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withCachedData(QueryExecution.scala:61)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:67)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:67)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:73)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:69)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:78)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3365)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.GeneratedMethodAccessor636.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1855-98da3b2fb6cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mclean_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \"\"\"\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Queries with streaming sources must be executed with writeStream.start();;\\nkafka'"
     ]
    }
   ],
   "source": [
    "# Парсинг бинарного файла из кафки\n",
    "\n",
    "event_schema = t.StructType([\n",
    "    t.StructField('uid', t.StringType(), True),\n",
    "    t.StructField('visits', t.StringType(), True),\n",
    "])\n",
    "\n",
    "visit_schema = t.ArrayType(\n",
    "    t.StructType([\n",
    "        t.StructField('url', t.StringType(), True),\n",
    "        t.StructField('timestamp', t.LongType(), True)\n",
    "    ])\n",
    ")\n",
    "clean_df = (\n",
    "    kafka_sdf\n",
    "    .select(f.col('value').cast('string').alias('value'))\n",
    "    .select(f.from_json(f.col('value'), event_schema).alias('event'))\n",
    "    .select('event.uid',f.from_json(f.col('event.visits'), visit_schema).alias('visits'))\n",
    ")\n",
    "\n",
    "clean_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1641,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_schema = t.ArrayType(\n",
    "    t.StructType([\n",
    "        t.StructField('url', t.StringType(), True),\n",
    "        t.StructField('timestamp', t.LongType(), True)\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 uid|                 url|\n",
      "+--------------------+--------------------+\n",
      "|bd7a30e1-a25d-4cb...|http://www.interf...|\n",
      "|bd7a30e1-a25d-4cb...|http://amerikan-g...|\n",
      "|bd7a30e1-a25d-4cb...|http://amerikan-g...|\n",
      "|bd7a30e1-a25d-4cb...|http://amerikan-g...|\n",
      "|bd7a30e1-a25d-4cb...|http://amerikan-g...|\n",
      "|bd7a30e1-a25d-4cb...|http://amerikan-g...|\n",
      "|bd7a30e1-a25d-4cb...|http://amerikan-g...|\n",
      "|bd7a30e1-a25d-4cb...|http://amerikan-g...|\n",
      "|bd7a30e1-a25d-4cb...|http://amerikan-g...|\n",
      "|bd7a30e1-a25d-4cb...|http://amerikan-g...|\n",
      "|bd7a30e1-a25d-4cb...|http://amerikan-g...|\n",
      "|bd7a30e1-a25d-4cb...|http://amerikan-g...|\n",
      "|bd7a30e1-a25d-4cb...|http://amerikan-g...|\n",
      "|bd7a30e1-a25d-4cb...|http://amerikan-g...|\n",
      "|bd7a30e1-a25d-4cb...|http://amerikan-g...|\n",
      "|bd7a30e1-a25d-4cb...|http://amerikan-g...|\n",
      "|bd7a30e1-a25d-4cb...|http://amerikan-g...|\n",
      "|bd7a30e1-a25d-4cb...|http://tv.jampo.t...|\n",
      "|bd7a30e1-a25d-4cb...|http://www.sovsek...|\n",
      "|bd7a30e1-a25d-4cb...|http://marketgid....|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_df2 = clean_df.withColumn(\"visits\", explode(\"visits\")).select(\"*\", col(\"visits\")[\"url\"].alias(\"url\"))\n",
    "clean_df3 = clean_df2.select('uid','url')\n",
    "clean_df3.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+------------------------+\n",
      "|uid                                 |host                    |\n",
      "+------------------------------------+------------------------+\n",
      "|bd7a30e1-a25d-4cbf-a03f-61748cbe540e|www.eer.ru              |\n",
      "|bd7a6f52-45db-49bf-90f2-a3b07a9b7bcd|films.imhonet.ru        |\n",
      "|bd9778f1-e2dc-4630-b449-1d7265cb5e41|zx-spectrum.narod.ru    |\n",
      "|bda1d504-87c3-4cf7-8ea8-325f60288918|trinixy.ru              |\n",
      "|bda8be2f-c4ff-4be5-b41c-b02fb59a69bc|www.rusbonds.ru         |\n",
      "|bda8be2f-c4ff-4be5-b41c-b02fb59a69bc|www.npp-sadovod.ru      |\n",
      "|bdae9068-92ed-434a-a0ed-f2afb183a02f|www.tonnel.ru           |\n",
      "|bdb3f69d-61fb-41db-92d5-19f579c29921|cache.betweendigital.com|\n",
      "|bdbb4a6a-05a3-4d4a-9ae4-c507477d63fa|www.p-lib.ru            |\n",
      "|bdc34323-c6a4-4bc1-8b5c-a0e17ee0ffd3|ogo.micropromo.ru       |\n",
      "+------------------------------------+------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_df4 = clean_df3.withColumn(\"host\", expr(\"regexp_extract(parse_url(url, 'HOST'),'(.*)')\"))\\\n",
    ".select('uid','host').distinct()  #.where(\"host!='null'\")\n",
    "\n",
    "clean_df4.show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df5 = clean_df4.groupBy(\"uid\")\\\n",
    ".agg(collect_list(\"host\").alias('host_list'))\n",
    "\n",
    "# clean_df5.show(5,False,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht = HashingTF(inputCol=\"host_list\", outputCol=\"features\", numFeatures=10000)\n",
    "clean_df6 = ht.transform(clean_df5)\n",
    "# clean_df6.show(1,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = model.transform(clean_df6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|                 uid|           host_list|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|0192cc54-559c-4c8...|[vestiprim.ru, n5...|(10000,[120,1058,...|[51.8969246743789...|[0.10379384934875...|       1.0|\n",
      "|019acd5e-be9a-4cd...|[zhenskoe-mnenie....|(10000,[2155,3273...|[49.7621020049179...|[0.09952420400983...|       1.0|\n",
      "|1e14a504-276e-448...|[www.gismeteo.ru,...|(10000,[2304,3016...|[55.3047357805689...|[0.11060947156113...|       7.0|\n",
      "|1eb313db-34ff-4bf...|     [zamok.gidm.ru]|(10000,[7792],[1.0])|[52.2175670241649...|[0.10443513404832...|       1.0|\n",
      "|47565df3-13e3-460...|[www.grosmet.ru, ...|(10000,[266,3166,...|[51.1658327815553...|[0.10233166556311...|       1.0|\n",
      "|4766a8ab-e9b6-4e0...|[www.st54.ru, www...|(10000,[979,1856,...|[57.6702378896652...|[0.11534047577933...|       7.0|\n",
      "|7302e78a-ec04-47e...|[melochi-jizni.ru...|(10000,[40,69,104...|[43.4032017671052...|[0.08680640353421...|       5.0|\n",
      "|8affa6ce-24c7-4ed...|[www.spb.aif.ru, ...|(10000,[771,5195,...|[52.7334083819059...|[0.10546681676381...|       1.0|\n",
      "|be1f10cb-548e-4f0...|[pixelov.net, www...|(10000,[335,359,4...|[41.9288208008147...|[0.08385764160162...|       8.0|\n",
      "|dba484a6-e857-474...|[qroom.ru, gotovi...|(10000,[9,166,225...|[37.3497982965743...|[0.07469959659314...|       9.0|\n",
      "|dc51a512-17db-411...|       [kinostok.tv]|(10000,[3372],[1.0])|[52.2175670241649...|[0.10443513404832...|       1.0|\n",
      "|fb64c40a-9b4f-471...|[chuvashia.fas.go...|(10000,[3221,6076...|[52.1148329727979...|[0.10422966594559...|       1.0|\n",
      "|fbaa6a33-f78e-4a5...|[mirdopov.ru, www...|(10000,[343,799,1...|[49.7869646123225...|[0.09957392922464...|       9.0|\n",
      "|fd7428f0-32d0-4da...|[rudiplom.ru, yan...|(10000,[3124,4563...|[52.5307555074529...|[0.10506151101490...|       4.0|\n",
      "|fdf4c6d6-6c67-46d...|[mg.dt00.net, www...|(10000,[35,356,63...|[40.0777031466840...|[0.08015540629336...|       8.0|\n",
      "|edb6cbfa-23b8-45c...|[starterix.ru, me...|(10000,[1388,6327...|[41.0983731227839...|[0.08219674624556...|       8.0|\n",
      "|73081df3-8f41-435...|[www.dvdvideosoft...|(10000,[2495,3757...|[51.5833114949269...|[0.10316662298985...|       1.0|\n",
      "|b2e4450d-c582-441...|[www.mamba.ru, ya...|(10000,[1046,1362...|[54.8510688325069...|[0.10970213766501...|       0.0|\n",
      "|5ab3c7b8-c550-493...|[k-istine.ru, www...|(10000,[261,979,1...|[56.9739667331846...|[0.11394793346636...|       0.0|\n",
      "|02e7f830-da57-4d5...|[gidonlinekino.co...|(10000,[1186,5874...|[53.0034909132883...|[0.10600698182657...|       7.0|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+\n",
      "|gender|  age|prediction|\n",
      "+------+-----+----------+\n",
      "|     M|45-54|       6.0|\n",
      "|     M| >=55|       9.0|\n",
      "|     F|25-34|       1.0|\n",
      "|     M|35-44|       2.0|\n",
      "|     F| >=55|       8.0|\n",
      "|     M|18-24|       7.0|\n",
      "|     F|18-24|       4.0|\n",
      "|     F|45-54|       5.0|\n",
      "|     F|35-44|       3.0|\n",
      "|     M|25-34|       0.0|\n",
      "+------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# gender_age_dic.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df =pr.join(f.broadcast(gender_age_dic), on='prediction', how=\"inner\").select('uid','gender','age')\n",
    "# transform.show()\n",
    "# predictions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"'writeStream' can be called only on streaming Dataset/DataFrame;\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1957.writeStream.\n: org.apache.spark.sql.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame;\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.Dataset.writeStream(Dataset.scala:3186)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-477681d57136>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m kafka_write_stream = (\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mkafka_out_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kafka\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwriteStream\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataStreamWriter\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \"\"\"\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataStreamWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjsq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"'writeStream' can be called only on streaming Dataset/DataFrame;\""
     ]
    }
   ],
   "source": [
    "kafka_out_df = predictions_df.select(F.to_json(F.struct(*predictions_df.columns)).alias('value'))\n",
    "\n",
    "kafka_write_stream = (\n",
    "    kafka_out_df\n",
    "    .writeStream\n",
    "    .format(\"kafka\")\n",
    "    .outputMode(\"complete\")\n",
    "    .option(\"checkpointLocation\", \"tmp/lab04/checkpointLocation\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVER)\n",
    "    .option(\"topic\", KAFKA_OUTPUT_TOPIC)\n",
    ")\n",
    "kafka_write_stream.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
