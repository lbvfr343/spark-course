{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conf = org...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.{SparkContext, SparkConf}\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.types.StringType\n",
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "\n",
    "import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier, LogisticRegression}\n",
    "import org.apache.spark.ml.feature.{HashingTF, Tokenizer}\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "import org.apache.spark.sql.functions.col\n",
    "import org.apache.spark.sql.types.DoubleType\n",
    "\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "\n",
    "import org.apache.hadoop.conf.Configuration\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "//Create Hadoop Configuration from Spark\n",
    "\n",
    "// val cityList: Vector[String] = Vector(\"Moscow\", \"Paris\", \"Madrid\", \"London\", \"New York\")\n",
    "\n",
    "// метод toDF изначально отсутствует у Vector[T], он добавляется через import spark.implicits._\n",
    "// val df: DataFrame = cityList.toDF\n",
    "\n",
    "val conf = new SparkConf().set(\"spark.driver.memory\", \"4g\")\n",
    "val sc = new SparkContext(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fs = DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_1546231273_57, ugi=artem.kropis (auth:SIMPLE)]]\n",
       "result_path = lab05.csv\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lab05.csv"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fs = FileSystem.get(spark.sparkContext.hadoopConfiguration)\n",
    "val result_path = new Path(\"lab05.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "csvOptions = Map(header -> true, inferSchema -> true)\n",
       "train = [_c0: int, ID: int ... 115 more fields]\n",
       "test = [_c0: int, ID: int ... 114 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[_c0: int, ID: int ... 114 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val csvOptions = Map(\"header\" -> \"true\", \"inferSchema\" -> \"true\")\n",
    "val train = spark.read.options(csvOptions).csv(\"/labs/slaba05/lab05_train.csv\")\n",
    "// train.printSchema\n",
    "// train.show(numRows = 1, truncate = 100, vertical = true)\n",
    "val test = spark.read.options(csvOptions).csv(\"/labs/slaba05/lab05_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_features: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_features(df : DataFrame) : DataFrame = {\n",
    "\n",
    "    val to_index_pipe_array = Array(\n",
    "        new StringIndexer().setInputCol(\"CLNT_TRUST_RELATION\").setOutputCol(\"CLNT_TRUST_RELATION_IND\").setHandleInvalid(\"keep\"),\n",
    "        new StringIndexer().setInputCol(\"APP_MARITAL_STATUS\").setOutputCol(\"APP_MARITAL_STATUS_IND\").setHandleInvalid(\"keep\"),\n",
    "        new StringIndexer().setInputCol(\"APP_KIND_OF_PROP_HABITATION\").setOutputCol(\"APP_KIND_OF_PROP_HABITATION_IND\").setHandleInvalid(\"keep\"),\n",
    "        new StringIndexer().setInputCol(\"CLNT_JOB_POSITION_TYPE\").setOutputCol(\"CLNT_JOB_POSITION_TYPE_IND\").setHandleInvalid(\"keep\"),\n",
    "        new StringIndexer().setInputCol(\"CLNT_JOB_POSITION\").setOutputCol(\"CLNT_JOB_POSITION_IND\").setHandleInvalid(\"keep\"),\n",
    "        new StringIndexer().setInputCol(\"APP_DRIVING_LICENSE\").setOutputCol(\"APP_DRIVING_LICENSE_IND\").setHandleInvalid(\"keep\"),\n",
    "        new StringIndexer().setInputCol(\"APP_EDUCATION\").setOutputCol(\"APP_EDUCATION_IND\").setHandleInvalid(\"keep\"),\n",
    "        new StringIndexer().setInputCol(\"APP_TRAVEL_PASS\").setOutputCol(\"APP_TRAVEL_PASS_IND\").setHandleInvalid(\"keep\"),\n",
    "        new StringIndexer().setInputCol(\"APP_CAR\").setOutputCol(\"APP_CAR_IND\").setHandleInvalid(\"keep\"),\n",
    "        new StringIndexer().setInputCol(\"APP_POSITION_TYPE\").setOutputCol(\"APP_POSITION_TYPE_IND\").setHandleInvalid(\"keep\"),\n",
    "        new StringIndexer().setInputCol(\"APP_EMP_TYPE\").setOutputCol(\"APP_EMP_TYPE_IND\").setHandleInvalid(\"keep\"),\n",
    "        new StringIndexer().setInputCol(\"APP_COMP_TYPE\").setOutputCol(\"APP_COMP_TYPE_IND\").setHandleInvalid(\"keep\"),\n",
    "        new StringIndexer().setInputCol(\"PACK\").setOutputCol(\"PACK_IND\").setHandleInvalid(\"keep\")\n",
    "    )\n",
    "\n",
    "    val to_index_pipe = new Pipeline().setStages(to_index_pipe_array)\n",
    "    val to_index_pipe_model = to_index_pipe.fit(df)\n",
    "    \n",
    "    val df2 = to_index_pipe_model.transform(df)\n",
    "    \n",
    "    val df3 = df2.withColumn(\"CR_PROD_CNT_IL\",col(\"CR_PROD_CNT_IL\").cast(DoubleType))\n",
    "        .withColumn(\"CR_PROD_CNT_VCU\",col(\"CR_PROD_CNT_VCU\").cast(DoubleType))\n",
    "        .withColumn(\"CR_PROD_CNT_TOVR\",col(\"CR_PROD_CNT_TOVR\").cast(DoubleType))\n",
    "        .withColumn(\"CR_PROD_CNT_PIL\",col(\"CR_PROD_CNT_PIL\").cast(DoubleType))\n",
    "        .withColumn(\"AGE\",col(\"AGE\").cast(DoubleType))\n",
    "        .withColumn(\"CR_PROD_CNT_CC\",col(\"CR_PROD_CNT_CC\").cast(DoubleType))\n",
    "        .withColumn(\"CR_PROD_CNT_CCFP\",col(\"CR_PROD_CNT_CCFP\").cast(DoubleType))\n",
    "    \n",
    "    val col_df = df3.columns\n",
    "    val buf = collection.mutable.ArrayBuffer(col_df: _*)\n",
    "    buf --= Array(\"_c0\", \"ID\", \"CLNT_TRUST_RELATION\", \"APP_MARITAL_STATUS\", \"APP_KIND_OF_PROP_HABITATION\"\n",
    "                  , \"CLNT_JOB_POSITION_TYPE\", \"CLNT_JOB_POSITION\", \"APP_DRIVING_LICENSE\", \"APP_EDUCATION\"\n",
    "                  , \"APP_TRAVEL_PASS\", \"APP_CAR\", \"APP_POSITION_TYPE\", \"APP_EMP_TYPE\", \"APP_COMP_TYPE\", \"PACK\"\n",
    "                  , \"TARGET\")\n",
    "    val array_col = buf.toArray\n",
    "    val assembler = new VectorAssembler().\n",
    "        setInputCols(array_col).\n",
    "        setOutputCol(\"features\")\n",
    "//         setHandleInvalid(\"keep\")\n",
    "    \n",
    "    return assembler.transform(df3.na.fill(0))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_faetures_non_balance = [_c0: int, ID: int ... 129 more fields]\n",
       "sample_count = 0.08154281652554526\n",
       "train_faetures = [_c0: int, ID: int ... 129 more fields]\n",
       "test_faetures = [_c0: int, ID: int ... 128 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[_c0: int, ID: int ... 128 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val train_faetures_non_balance = create_features(train)\n",
    "\n",
    "val sample_count = (train_faetures_non_balance.filter(col(\"TARGET\") === 1).count() + 0.0) / (train_faetures_non_balance.count() + 0.0)\n",
    "val train_faetures = train_faetures_non_balance.filter(col(\"TARGET\") === 1)\n",
    "    .union(train_faetures_non_balance.filter(col(\"TARGET\") === 0).sample(sample_count))\n",
    "\n",
    "val test_faetures = create_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val lr = new LogisticRegression()\n",
    "  .setMaxIter(30)\n",
    "  .setLabelCol(\"TARGET\")\n",
    "//   .setRegParam(0.3)\n",
    "//   .setElasticNetParam(0.8)\n",
    "\n",
    "// Fit the model\n",
    "val lrModel = lr.fit(train_faetures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rf = rfc_753d03e6cb6c\n",
       "rfModel = RandomForestClassificationModel (uid=rfc_753d03e6cb6c) with 50 trees\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassificationModel (uid=rfc_753d03e6cb6c) with 50 trees"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rf = new RandomForestClassifier()\n",
    "    .setLabelCol(\"TARGET\")\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .setNumTrees(50)\n",
    "    .setMaxDepth(30)\n",
    "    .setMaxBins(64)\n",
    "\n",
    "val rfModel = rf.fit(train_faetures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions = [_c0: int, ID: int ... 131 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[_c0: int, ID: int ... 131 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val predictions = lrModel.transform(test_faetures)\n",
    "val predictions = rfModel.transform(test_faetures)\n",
    "\n",
    "if(fs.exists(result_path))\n",
    "    fs.delete(result_path, true)\n",
    "\n",
    "predictions.selectExpr(\"ID as id\", \"prediction as target\")\n",
    "    .coalesce(1).write.format(\"csv\")\n",
    "    .option(\"delimiter\", \"\\t\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .save(\"lab05.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.select(\"ID\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_faetures.select(\"ID\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.groupBy(\"prediction\").count().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_faetures.groupBy(\"TARGET\").count().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sample_count = (train_faetures.filter(col(\"TARGET\") === 1).count() + 0.0) / (train_faetures.count() + 0.0)\n",
    "train_faetures.sample(sample_count).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У любого DF есть схема:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотреть содержимое DF можно с помощью метода `show()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также можно вывести содержимое в вертикальной ориентации - это удобно при большое количестве столбцов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(numRows = 20, truncate = 100, vertical=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсчет количества элементов в DF с помощью `count()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отфильтровать данные можно с помощью метода `filter`. В отличие от RDD, он принимает SQL выражение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Требует наличия import spark.implicits._\n",
    "\n",
    "df.filter(('value !== \"Moscow\").and( ('value !== \"Moscow\"))).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter('value.!==(\"Moscow\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$\"value\" toString ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// Требует наличия import spark.implicits._\n",
    "\n",
    "df.filter($\"value\" === \"Moscow\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// sugar free & type safe\n",
    "// Три знака равно здесь используются, тк на самом деле это метод,\n",
    "// применяемый к колонке org.apache.spark.sql.Column\n",
    "\n",
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "df.filter(col(\"value\") === \"Moscow\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// легко ошибиться и получить ошибку в рантайме\n",
    "import org.apache.spark.sql.functions.lit\n",
    "\n",
    "val qE = \n",
    "    df.filter(\"value = 'Moscow'\")\n",
    "        .localCheckpoint\n",
    "        .withColumn(\"foo\", lit(\"foo\"))\n",
    "        .withColumn(\"bar\", lit(\"bar\"))\n",
    "        .queryExecution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qE.logical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sqlParser = spark.sessionState.sqlParser\n",
    "sqlParser.parsePlan(\"\"\"SELECT FOO FROM BAR\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qE.analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qE.optimizedPlan // CollapseProject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qE.executedPlan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.catalyst.InternalRow\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "val rdd: RDD[InternalRow] = qE.toRdd\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// промежуточный вариант между col и обычной строкой\n",
    "// expr также может использоваться для вызова SQL builtin функций, \n",
    "// отсутствующих в org.apache.spark.sql.functions\n",
    "\n",
    "import org.apache.spark.sql.functions.expr\n",
    "\n",
    "df.filter(expr(\"value = 'Moscow'\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавить новую колонку можно с помощью метода `withColumn`. Необходимо помнить, что данный метод, как и другие, является трансформацией и не изменяет оригинальный DF, а создает новый."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.upper\n",
    "df.withColumn(\"upperCity\", upper('value)).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогичный результат получить, используя метод `select`. Данный метод может быть использован не только для выбора определенных колонок, но и для создания новых."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val withUpper = df.select('value, upper('value).alias(\"upperCity\"))\n",
    "withUpper.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если передать `col(\"*\")` в `select`, то вы получите DF со всеми колонками. Это полезно, когда вы не знаете список всех колонок (например вы получили его через API), но вам нужно их все выбрать и добавить новую колонку. Это можно сделать следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// методы name, as и alias часто являются взаимозаменяемыми\n",
    "\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "withUpper.select(\n",
    "    col(\"*\"), \n",
    "    lower($\"value\").name(\"lowerCity\"), \n",
    "    (length('value) + 1).as(\"length\"),\n",
    "    lit(\"foo\").alias(\"bar\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При необходимости в `select` можно передать список колонок, используя обычные строки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "withUpper.select(\"value\", \"upperCity\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалить колонку из DF можно с помощью метода `drop`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// drop не будет выдавать ошибку, если будет указана несуществующая колонка\n",
    "\n",
    "withUpper.drop(\"upperCity\", \"abraKadabra\", \"2323\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Жертвуем отказоустойчивостью для повышения производительности\n",
    "withUpper.localCheckpoint.drop(\"upperCity\", \"abraKadabra\").explain(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(col(\"abc\") === col(\"cde\")) alias \"123\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "+ методы `filter` и `select` принимают в качестве аргументов колонки [org.apache.spark.sql.Column](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column). Это может быть либо ссылка на существующую колонку, либо функцию из [org.apache.spark.sql.functions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$)\n",
    "+ любые трансформации возвращают новый DF, не меняя существующий\n",
    "+ тип [org.apache.spark.sql.Column](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column) играет важную роль в DF API - на его основе создаются ссылки на существующие колонки, а также функции, принимающие [org.apache.spark.sql.Column](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column) и возвращающие [org.apache.spark.sql.Column](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column). По этой причине обычное сравнение `==` не будет работать в DF API, тк `filter` принимает [org.apache.spark.sql.Column](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column), а не `Boolean`\n",
    "+ Класс DataFrame в последних версиях Spark представляет собой `org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]`, поэтому его описание следует искать в [org.apache.spark.sql.Dataset](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)\n",
    "\n",
    "## Очистка данных\n",
    "\n",
    "Одной из задач обработки данных является их очистка. DF API содержит класс функций \"not available\", описанный в пакете [org.apache.spark.sql.DataFrameNaFunctions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameNaFunctions). В данном пакете есть три функции:\n",
    "+ `na.drop`\n",
    "+ `na.fill`\n",
    "+ `na.replace`\n",
    "\n",
    "Для демонстрации работы данных функций создадим новый датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.Column\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.Dataset\n",
    "\n",
    "val testData =\n",
    "\"\"\"{ \"name\":\"Moscow\", \"country\":\"Rossiya\", \"continent\": \"Europe\", \"population\": 12380664}\n",
    "{ \"name\":\"Madrid\", \"country\":\"Spain\" }\n",
    "{ \"name\":\"Paris\", \"country\":\"France\", \"continent\": \"Europe\", \"population\" : 2196936}\n",
    "{ \"name\":\"Berlin\", \"country\":\"Germany\", \"continent\": \"Europe\", \"population\": 3490105}\n",
    "{ \"name\":\"Barselona\", \"country\":\"Spain\", \"continent\": \"Europe\" }\n",
    "{ \"name\":\"Cairo\", \"country\":\"Egypt\", \"continent\": \"Africa\", \"population\": 11922948 }\n",
    "{ \"name\":\"Cairo\", \"country\":\"Egypt\", \"continent\": \"Africa\", \"population\": 11922948 }\n",
    "{ \"name\":\"New York, \"country\":\"USA\",\"\"\"\n",
    "\n",
    "// Создаем DF из одной строки и добавляем данные в виде новой колонки\n",
    "val raw = spark.range(0,1).select(lit(testData).alias(\"value\"))\n",
    "\n",
    "// Создаем новую колонку, разибая наши данные по \\n\n",
    "val jsonStrings: Column = split(col(\"value\"), \"\\n\").alias(\"value\")\n",
    "\n",
    "// Используем функцию explode для того, чтобы развернуть массив мехом наружу и используем темную магию \n",
    "// для превращения DataFrame в Dataset[String]\n",
    "val splited: Dataset[String] = raw.select(explode(jsonStrings)).as[String]\n",
    "\n",
    "// splited.show(numRows = 10, truncate = false)\n",
    "\n",
    "\n",
    "// Создаем новый датафре... датасет, в котором наши JSON строки будут распарсены\n",
    "val df: Dataset[Row] = spark.read.json(splited)\n",
    "df.printSchema\n",
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для очистки датасета:\n",
    "+ удалим строку с навалидным JSON, сохраним ее в отдельное место\n",
    "+ удалим дубликаты\n",
    "+ заполним `null`ы в колонках\n",
    "+ исправим `Rossiya` на `Russia`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val corruptData = df.select(col(\"_corrupt_record\")).na.drop(\"all\").collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val fillData: Map[String, Any] = Map(\"continent\" -> \"Undefined\", \"population\" -> 0)\n",
    "val replaceData: Map[Any, Any] = Map(\"Rossiya\" -> \"Russia\")\n",
    "\n",
    "val cleanData = \n",
    "    df\n",
    "    .drop(col(\"_corrupt_record\"))\n",
    "    .na.drop(\"all\")\n",
    "    .na.fill(fillData)\n",
    "    .na.replace(\"country\", replaceData)\n",
    "    .dropDuplicates\n",
    "    .localCheckpoint\n",
    "\n",
    "\n",
    "cleanData.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "+ DF API обладает удобным API для очистки данных, позволяющим разработчику сконцентрироваться разработчику на бизнес логике, а не на написании функций для обработки всех возможных исключительных ситуаций\n",
    "+ метод `spark.read.json` позволяет читать не только файлы, но и `Dataset[String]`, содержащие JSON строки.\n",
    "\n",
    "## Агрегаты\n",
    "Посчитаем суммарное население и количество городов с разбивкой по континентам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val aggCount = cleanData.groupBy('continent).count\n",
    "aggCount.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val aggSum = cleanData.groupBy('continent).sum(\"population\")\n",
    "aggSum.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы совместить несколько агрегатов в одном DF, мы можем использовать метод `agg()`. Данный метод позволяет использовать любые `Aggregate functions` из пакета [org.apache.spark.sql.functions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val agg = cleanData.groupBy('continent).agg(count(\"*\").alias(\"count\"), sum(\"population\").alias(\"sumPop\"))\n",
    "agg.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью агрегатов мы можем выполнять такие действия, как, например, `collect_list` и `collect_set`. Стоит отметить, что колонки в Spark могут иметь не только скалярные типы, но и структуры, словари и массивы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val aggList = cleanData.groupBy('continent).agg(collect_list(\"country\").alias(\"countries\"))\n",
    "aggList.printSchema\n",
    "aggList.show(numRows = 10, truncate = 100, vertical = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя методы `struct` и `to_json`, мы можем превратить произвольный набор колонок в JSON строку. Этот методы часто используется перед отправкой данных в Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val withStruct = aggList.select(struct('continent, 'countries).alias(\"s\"))\n",
    "withStruct.printSchema\n",
    "\n",
    "withStruct.show(10, false)\n",
    "withStruct.select(col(\"`s`.`continent`\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "withStruct.withColumn(\"s\", to_json('s)).show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если необходимо превратить все колонки DF в JSON String, можно воспользоваться функций `toJSON`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val jString: Dataset[String] = aggList.toJSON\n",
    "jString.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если нам необходимо создать колонки из значений текущих колонок, мы можем воспользоваться функцией `pivot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanData.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanData.groupBy(col(\"country\")).pivot(\"continent\").agg(sum(\"population\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanData.groupBy(col(\"country\")).pivot(\"continent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "+ DF API позволяет строить большое количество агрегатов. При этом необходимо помнить, что операции `groupBy`, `cube`, `rollup` возвращают [org.apache.spark.sql.RelationalGroupedDataset](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset), к которому затем необходимо применить одну из функций агрегации - `count`, `sum`, `agg` и т. п.\n",
    "+ При вычислении агрегатов необходимо помнить, что эта операция требует перемешивания данных между воркерами, что, в случае перекошенных данных, может привести к OOM на воркере.\n",
    "\n",
    "## Кеширование\n",
    "По умолчанию при применении каждого действия Spark пересчитывает весь граф, что может негативно сказать на производительности приложения. Для демонстрации возьмем датасет [Airport Codes](https://datahub.io/core/airport-codes)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val csvOptions = Map(\"header\" -> \"true\", \"inferSchema\" -> \"true\")\n",
    "val airports = spark.read.options(csvOptions).csv(\"/tmp/airport-codes_csv.csv\")\n",
    "airports.printSchema\n",
    "airports.show(numRows = 1, truncate = 100, vertical = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем несколько агрегатов. Несмотря на то, что `onlyRu` является общим для всех действий, он пересчитывается при вызове каждого действия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val onlyRuAndHigh = airports.filter('iso_country === \"RU\" and 'elevation_ft > 1000)\n",
    "onlyRuAndHigh.show(numRows = 1, truncate = 100, vertical = true)\n",
    "onlyRuAndHigh.count\n",
    "onlyRuAndHigh.collect\n",
    "onlyRuAndHigh.groupBy('municipality).count.orderBy('count.desc).na.drop(\"any\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для решения этой проблемы следует использовать методы `cache`, либо `persist`. Данные методы сохраняют состояние графа после первого действия, и следующие обращаются к нему. Разница между методами заключается в том, что `persist` позволяет выбрать, куда сохранить данные, а `cache` использует значение по умолчанию. В текущей версии Spark это [StorageLevel.MEMORY_ONLY](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence). Важно помнить, что данный кеш не предназначен для обмена данными между разными Spark приложения - он является внутренним для приложения. После того, как работа с данными окончена, необходимо выполнить `unpersist` для очистки памяти"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyRuAndHigh.cache\n",
    "onlyRuAndHigh.count\n",
    "// при вычислении count данные будут помещены в cache\n",
    "onlyRuAndHigh.show(numRows = 1, truncate = 100, vertical = true)\n",
    "onlyRuAndHigh.collect\n",
    "onlyRuAndHigh.groupBy('municipality).count.orderBy('count.desc).na.drop(\"any\").explain()\n",
    "\n",
    "onlyRuAndHigh.unpersist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "+ Использование `cache` и `persist` позволяет существенно сократить время обработки данных, однако следует помнить и об увеличении потребляемой памяти на воркерах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Репартиционирование\n",
    "RDD и DF являются представляют собой классы, описывающие распределенные коллекции данных. Они (коллекции) разбиты на крупные блоки, которые называются партициями. В графе вычисления, который называется в Spark DAG (Direct Acyclic Graph), есть три основных компонента - `job`, `stage`, `task`.\n",
    "\n",
    "`job` представляет собой весь граф целиком, от момента создания DF, до применения `action` к нему. Состоит из одной или более `stage`. Когда возникает необходимость сделать `shuffle` данных, Spark создает новый `stage`. Каждый `stage` состоит из большого количества `task`. `task` это базовая операция над данными. Одновременно Spark выполняет N `task`, которые обрабатывают N партиций, где N - это суммарное число доступных потоков на всех воркерах.\n",
    "\n",
    "Исходя из этого, важно обеспечивать:\n",
    "+ достаточное количество партиций для распределения нагрузки по всем воркерам\n",
    "+ равномерное распределение данных между партициями\n",
    "\n",
    "Создадим датасет с перекосом данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val skewColumn = when(col(\"id\") < 900, lit(0)).otherwise(lit(1))\n",
    "\n",
    "val skewDf = spark.range(0,1000).repartition(10, skewColumn)\n",
    "\n",
    "def printItemPerPartition[T](ds: Dataset[T]): Unit = {\n",
    "    ds.mapPartitions { x => Iterator(x.length) }\n",
    "    .withColumnRenamed(\"value\", \"itemPerPartition\")\n",
    "    .show(50, false)\n",
    "}\n",
    "\n",
    "printItemPerPartition[java.lang.Long](skewDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Любые операции с таким датасетом будут работать медленно, т.к.\n",
    "+ если суммарное количество потоков на всех воркерах больше 10, то в один момент времени работать будут максимум 10, остальные будут простаивать\n",
    "+ из 10 партицийи только в 2 есть данные и это означает, что только 2 потока будут обрабатывать данные, при этом из-за перекоса данных между ними (900 vs 100) первый станет bottleneck'ом\n",
    "\n",
    "Обычно перекошенные датасеты возникают после вычисления агрегатов, оконных функций и соединений, но также могут возникать и при чтении источников.\n",
    "\n",
    "Для устранения проблемы перекоса данных, следует использовать метод `repartition`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// здесь мы передаем только новое количество партиций и Spark выполнит RoundRobinPartitioning\n",
    "val repartitionedDf = skewDf.repartition(20)\n",
    "repartitionedDf.explain()\n",
    "printItemPerPartition[java.lang.Long](repartitionedDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// здесь мы добавляем к числу партиций колонку, по которой необходимо сделать репартиционирование,\n",
    "// поэтому Spark выполнит HashPartitioning\n",
    "val repartitionedDf = skewDf.repartition(20, col(\"id\"))\n",
    "repartitionedDf.explain()\n",
    "printItemPerPartition[java.lang.Long](repartitionedDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"200\" height=\"200\" src=\"https://pngimage.net/wp-content/uploads/2018/06/соленья-png-4.png\">\n",
    "\n",
    "### Соленья\n",
    "Часто при вычислении агрегатов приходится работать с перекошенными данными:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.printSchema\n",
    "\n",
    "airports.groupBy('type).count.orderBy('count.desc).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку при вычислении агрегата происходит неявный `HashPartitioning` по ключу (ключам) агрегата, то при выполнении определенных условий происходит нехватка памяти на воркере, которую нельзя исправить, не изменив подход к построению агрегата.\n",
    "\n",
    "Один из вариантов устранение - соление ключей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val saltModTen = pmod(round((rand() * 100), 0), lit(10)).cast(\"int\")\n",
    "\n",
    "val salted = airports.withColumn(\"salt\", saltModTen)\n",
    "salted.show(numRows = 1, truncate = 200, vertical = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это позволяет нам существенно снизить объем данных в каждой партиции (30к vs 3к):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val firstStep = salted.groupBy('type, 'salt).count()\n",
    "\n",
    "firstStep.orderBy('count.desc).show(200, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вторым шагом мы делаем еще один агрегат, суммируя предыдущие значения `count`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val secondStep = firstStep.groupBy('type).agg(sum(\"count\").alias(\"count\"))\n",
    "\n",
    "secondStep.orderBy('count.desc).show(200, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Несмотря на то, что мы сделали две группировки вместо одной, распределение данных по воркерам было более равномерным, что позволило избежать OOM на воркерах.\n",
    "\n",
    "### Выводы:\n",
    "+ Партиционирование - важный аспект распределенных вычислений, от которого напрямую зависит стабильность и скорость вычислений\n",
    "+ В Spark всегда работает правило 1 TASK = 1 THREAD (= vCore = 1 virtual CPU core) = 1 PARTITION\n",
    "+ Репартиционирование и соление данных позволяет решить проблему перекоса данных и вычислений\n",
    "+ Важно помнить, что репартиционирование использует дисковую и сетевую подсистемы - обмен данными происходит **по сети**, а результат записывается **на диск**, что может стать узким местом при выполнении репартиционирования\n",
    "\n",
    "## Встроенные функции\n",
    "Помимо базовых SQL операторов, в Spark существует большой набор встроенных функций:\n",
    "+ API методы из [org.apache.spark.sql.functions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$)\n",
    "+ [SQL built-in functions](https://spark.apache.org/docs/latest/api/sql/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.range(0,10)\n",
    "\n",
    "// используем org.apache.spark.sql.functions\n",
    "val newCol: Column = pmod(col(\"id\"), lit(2))\n",
    "df.withColumn(\"pmod\", newCol).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.expr\n",
    "\n",
    "// используем SQL built-in functions\n",
    "val newCol: Column = expr(\"\"\"pmod(id, 2)\"\"\")\n",
    "df.withColumn(\"pmod\", newCol).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы\n",
    "+ Spark обладает широким набором функций для работы с колонками разных типов, включая простые типы - строки, числа, и т. д., а также словари, массивы и структуры\n",
    "+ Встроенные функции принимают колонки `org.apache.spark.sql.Column` и возвращают `org.apache.spark.sql.Column` в большинстве случаев\n",
    "+ Встроенные функции доступны в двух местах - org.apache.spark.sql.functions и SQL built-in functions\n",
    "+ Встроенные функции можно (и нужно) использовать вместе - на вход во встроенные функции могут подаваться результаты встроенной функции, тк все они возвращают `sql.Column` \n",
    "\n",
    "### Пользовательские функции\n",
    "\n",
    "В том случае, если функционала встроенных функций не хватает, можно написать пользовательскую функцию - UDF. Пользовательская функция может принимать до 16 аргументов. Соответствие Spark и Scala типов описано [здесь](https://spark.apache.org/docs/latest/sql-reference.html#data-types)\n",
    "\n",
    "Необходимо помнить, что `null` в Spark превращается в `null` внутри UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{udf, col}\n",
    "\n",
    "val df = spark.range(0,10)\n",
    "\n",
    "val plusOne = udf { (value: Long, v2: Long, v3: Long) => value + v2 + v3 }\n",
    "\n",
    "df.withColumn(\"idPlusOne\", plusOne(col(\"id\"), col(\"id\"), lit(10+2+2))).show()\n",
    "df.withColumn(\"idPlusOne\", plusOne(col(\"id\"), col(\"id\"), lit(10+2+2))).explain(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пользовательская функция может возвращать:\n",
    "+ простой тип - `String`, `Long`, `Float`, `Boolean` и т.д.\n",
    "+ массив - любые коллекции, наследующие `Seq[T]` - `List[T]`, `Vector[T]` и т. д.\n",
    "+ словарь - `Map[A,B]`\n",
    "+ инстанс `case class`'а\n",
    "+ Option[T]\n",
    "\n",
    "Реализуем функцию, которая возвращает имя хоста, на котором работает воркер:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.net.InetAddress\n",
    "\n",
    "val hostname = udf { () => InetAddress.getLocalHost().getHostName() }\n",
    "\n",
    "df.withColumn(\"hostname\", hostname()).show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы также можем использовать монады `Try[T]` и `Option[T]` и для написания пользовательской функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.util.Try\n",
    "import org.apache.spark.sql.functions.{udf, col}\n",
    "\n",
    "val df = spark.range(0,10)\n",
    "\n",
    "val divideTwoBy = udf { (inputValue: Long) => Try(2L / inputValue).toOption }\n",
    "\n",
    "val result = df.withColumn(\"divideTwoBy\", divideTwoBy(col(\"id\")))\n",
    "result.printSchema\n",
    "result.show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы\n",
    "+ Пользовательские функции позволяют реализовать произвольный алгоритм и использовать его в DF API\n",
    "+ Пользовательские функции работают медленнее встроенных, поскольку при использовании встроенных функций Spark использует ряд оптимизаций, например векторизацию вычислений на уровне CPU\n",
    "\n",
    "## Соединения\n",
    "\n",
    "Join'ы позволяют соединять два DF в один по заданным условиям.\n",
    "\n",
    "По типу условия join'ы делятся на:\n",
    "+ equ-join - соединение по равенству одного или более ключей\n",
    "+ non-equ join - соединение по условию, отличному от равенства одного или более ключей\n",
    "\n",
    "#### Виды соединений\n",
    "+ **BroadcastHashJoin**\n",
    "  - equ join\n",
    "  - broadcast\n",
    "+ **SortMergeJoin**\n",
    "  - equ join\n",
    "  - sortable keys\n",
    "+ **BroadcastNestedLoopJoin**\n",
    "  - non-equ join\n",
    "  - using broadcast\n",
    "+ **CartesianProduct**\n",
    "  - non-equ join\n",
    "\n",
    "Добавим новую колонку к датасету `airports`, в которой будет процент заданного типа аэропорта ко всем типам аэропорта по каждой стране. Первым шагом посчитаем число аэропортов каждого типа по стране:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{count, round, lit}\n",
    "\n",
    "val aggTypeCountry = airports.groupBy('type, 'iso_country).agg(count(\"*\").alias(\"cnt_country_type\"))\n",
    "\n",
    "aggTypeCountry.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посчитаем количество аэропортов по каждой стране:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val aggCountry = airports.groupBy('iso_country).agg(count(\"*\").alias(\"cnt_country\"))\n",
    "aggCountry.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соединим получившиеся датасеты и получим процентное распределение типов аэропорта по стране"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val percent = \n",
    "    aggTypeCountry\n",
    "        .join(aggCountry, Seq(\"iso_country\"), \"inner\")\n",
    "        .select('iso_country, 'type, (round(lit(100) * 'cnt_country_type / 'cnt_country, 2).alias(\"percent\")))\n",
    "percent.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соединим полученный датасет с изначальным:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val result = airports.join(percent, Seq(\"iso_country\", \"type\"), \"left\")\n",
    "result.select('ident, 'iso_country, 'type, 'percent).sample(0.2).show(20, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во всех наших джойнах присутствует массив `Seq[String]`. Это синтаксических сахар, позволяющий не переименовывать колонки датасетов, а просто указать, что соединение будет делаться по колонкам с именами, входящим в массив.\n",
    "\n",
    "В общем случае условие джойна должно быть выражено в виде колонки `sql.Column`, например:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.Column\n",
    "val joinCondition: Column = (airports(\"iso_country\") === percent(\"iso_country\")) and (airports(\"type\") === percent(\"type\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val result2 = airports.join(percent, joinCondition, \"left\")\n",
    "result2.select(airports(\"ident\"), airports(\"iso_country\"), airports(\"type\"), percent(\"percent\")).sample(0.2).show(20, false)\n",
    "result2.select(airports(\"ident\"), airports(\"iso_country\"), airports(\"type\"), percent(\"percent\")).sample(0.2).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При этом в данном выражении допускается использование встроенных функций, пользовательских функций и операторов сравнения. Однако следует помнить, что мы выполняем джойн двух распределенных датасетов и если условие соединения будет плохо составлено, то Spark выполнит `cross join`, производительность которого будет \"крайне мала\" &copy;\n",
    "\n",
    "### Выводы:\n",
    "+ Spark поддерживает большое число типов соединений\n",
    "+ Условием соединения может быть `Seq[String]`, либо `sql.Column`\n",
    "+ При использовании сложных условий соединения следует избегать тех, которые приведут к `cross join`\n",
    "\n",
    "## Оконные функции\n",
    "Оконные функции позволяют делать функции над \"окнами\" (кто бы мог подумать) данных\n",
    "\n",
    "Окно создается из класса [org.apache.spark.sql.expressions.Window](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.expressions.Window) с указанием полей, определяющих границы окон и полей, определяющих порядок сортировки внутри окна:\n",
    "\n",
    "```val window = Window.partitionBy(\"a\", \"b\").orderBy(\"a\")```\n",
    "\n",
    "Применяя окна, можно использовать такие полезные функции из [org.apache.spark.sql.functions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$), как ```lag()``` и ```lead()```, а также эффективно работать с данными time-series данными.\n",
    "\n",
    "Выполним задачу с вычисление процента отношения типов аэропортов, используя оконные функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit(1).as(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "val w = Window.partitionBy()\n",
    "val windowCountry = Window.partitionBy(\"iso_country\")\n",
    "val windowTypeCountry = Window.partitionBy(\"type\", \"iso_country\")\n",
    "\n",
    "val result = airports\n",
    "                .withColumn(\"cnt_country\", count(\"*\").over(windowCountry))\n",
    "                .withColumn(\"cnt_country_type\", count(\"*\").over(windowTypeCountry))\n",
    "                .withColumn(\"cnt_all\", count(lit(1)).over(w))\n",
    "                .withColumn(\"percent\", round(lit(100) * 'cnt_country_type / 'cnt_country, 2))\n",
    "                            \n",
    "result.select('ident, 'iso_country, 'type, 'percent, 'cnt_all).sample(0.2).show(20, false)\n",
    "result.select('ident, 'iso_country, 'type, 'percent, 'cnt_all).sample(0.2).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "+ Оконные функции позволяют применять функции, применительно к окнам данных\n",
    "+ Окно определяется списком колонок и сортировкой\n",
    "+ Применение оконных функций приводит к `shuffle`\n",
    "\n",
    "После завершения работы не забывайте останавливать `SparkSession`, чтобы освободить ресурсы кластера!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
