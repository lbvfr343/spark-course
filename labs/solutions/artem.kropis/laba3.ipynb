{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 8 --executor-memory 4g --executor-cores 2 --driver-memory 2g pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"KAM_lab3\") \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master-4.newprolab.com:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>KAM_lab3</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9e7c2683c8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "\n",
    "from pyspark.ml import Transformer, Pipeline\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, Normalizer, StopWordsRemover, CountVectorizer, VectorAssembler\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, FloatType, ArrayType, StringType, IntegerType\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import udf, col, when, isnan, isnull, broadcast, desc, lower, pandas_udf, row_number, explode, split\n",
    "from pyspark.sql.functions import array, collect_set, lit\n",
    "\n",
    "from pyspark.mllib.linalg import SparseVector, DenseVector\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType() \\\n",
    "      .add(\"user_id\", IntegerType(), True) \\\n",
    "      .add(\"item_id\", IntegerType(), True) \\\n",
    "      .add(\"purchase\", IntegerType(), True)\n",
    "      \n",
    "df_user = spark.read.format(\"csv\") \\\n",
    "      .option(\"header\", True) \\\n",
    "      .schema(schema) \\\n",
    "      .load(\"/labs/slaba03/laba03_train.csv\")\n",
    "\n",
    "df_user.createOrReplaceTempView(\"t_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT 999999 as user_id\n",
    "                , 999999 as item_id\n",
    "                , 1 as purchase\n",
    "            UNION ALL \n",
    "            SELECT user_id\n",
    "                , item_id\n",
    "                , purchase\n",
    "            FROM t_train\n",
    "            \"\"\").createOrReplaceTempView(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>buy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851486</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>901457</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>927211</td>\n",
       "      <td>0.000392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>928140</td>\n",
       "      <td>0.000387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>825061</td>\n",
       "      <td>0.001931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id       buy\n",
       "0   851486  0.000000\n",
       "1   901457  0.000000\n",
       "2   927211  0.000392\n",
       "3   928140  0.000387\n",
       "4   825061  0.001931"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user_buy = spark.sql(f\"\"\"SELECT user_id\n",
    "            , sum(purchase)/count(1) buy\n",
    "        FROM train\n",
    "        GROUP BY user_id\"\"\")\n",
    "df_user_buy.createOrReplaceTempView(\"user_buy\")\n",
    "df_user_buy.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>buy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99817</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93486</td>\n",
       "      <td>0.002141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8389</td>\n",
       "      <td>0.005979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72820</td>\n",
       "      <td>0.000739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>95080</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id       buy\n",
       "0    99817  0.000000\n",
       "1    93486  0.002141\n",
       "2     8389  0.005979\n",
       "3    72820  0.000739\n",
       "4    95080  0.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_item_buy = spark.sql(f\"\"\"SELECT item_id\n",
    "            , sum(purchase)/count(1) buy\n",
    "        FROM train\n",
    "        GROUP BY item_id\"\"\")\n",
    "df_item_buy.createOrReplaceTempView(\"item_buy\")\n",
    "df_item_buy.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg(buy)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   avg(buy)\n",
       "0  0.002435"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"SELECT avg(buy)\n",
    "        FROM item_buy\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = spark.sql(f\"\"\"SELECT tr.user_id\n",
    "            , tr.item_id\n",
    "            , coalesce(item_buy.buy, 0.001) as item_buy\n",
    "            , coalesce(user_buy.buy, 0.001) as user_buy\n",
    "            , purchase\n",
    "        FROM train tr\n",
    "        LEFT JOIN item_buy\n",
    "            ON tr.item_id = item_buy.item_id\n",
    "        LEFT JOIN user_buy\n",
    "            ON tr.user_id = user_buy.user_id        \n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_items_schema = StructType(fields=[\n",
    "    StructField('item_id', IntegerType()), \n",
    "    StructField('channel_id', FloatType()),\n",
    "    StructField('datetime_availability_start', StringType()),\n",
    "    StructField('datetime_availability_stop', StringType()),\n",
    "    StructField('datetime_show_start', StringType()),\n",
    "    StructField('datetime_show_stop', StringType()),\n",
    "    StructField('content_type', IntegerType()),\n",
    "    StructField('title', StringType(), nullable=True),\n",
    "    StructField('year', FloatType(), nullable=True),\n",
    "    StructField('genres', StringType()),\n",
    "    StructField('region_id', IntegerType()),\n",
    "]) \n",
    "\n",
    "df_items = (spark.read.format(\"csv\")\n",
    "            .option(\"header\", True)\n",
    "            .option(\"sep\", \"\\t\")\n",
    "            .schema(read_items_schema)\n",
    "            .load(\"/labs/slaba03/laba03_items.csv\")\n",
    "           )\n",
    "            \n",
    "    \n",
    "df_items = (df_items\n",
    "            .withColumn(\"year\", \n",
    "                        when(df_items.item_id == 103377, 2008.0)\n",
    "                        .when(df_items.item_id == 95141, 2014.0)\n",
    "                        .when(df_items.item_id == 72544, 2009.0)\n",
    "                        .when(df_items.item_id == 8544, 1994.0)\n",
    "                        .otherwise(df_items.year))\n",
    "            .withColumn(\"genres\", \n",
    "                        when(df_items.item_id == 103377, 'Анимация, Короткометражные')\n",
    "                        .otherwise(df_items.genres))\n",
    "           )\n",
    "    \n",
    "df_items = (df_items\n",
    "            .filter(~df_items.item_id.isNull())\n",
    "            .repartition(32)\n",
    "            .cache())\n",
    "df_items.createOrReplaceTempView(\"t_item\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT item_id, genres\n",
    "                FROM t_item\n",
    "            UNION ALL\n",
    "            SELECT 999999 as item_id, 'нет' as genres\n",
    "            \"\"\").createOrReplaceTempView(\"item\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items = spark.sql(f\"\"\"SELECT item_id\n",
    "            , regexp_replace(coalesce(genres, 'пусто'), \"[^\\d\\w a-zA-Zа-яА-Я]\", \" \") genres_prep\n",
    "        FROM item\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, HashingTF\n",
    "tokenizer = Tokenizer(inputCol=\"genres_prep\", outputCol=\"genres_words\")\n",
    "df_items = tokenizer.transform(df_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer\n",
    "stop_words = (StopWordsRemover.loadDefaultStopWords(\"russian\") +\n",
    "            StopWordsRemover.loadDefaultStopWords(\"english\") + [' ', ''])\n",
    "swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"genres_filtered\", stopWords=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items = swr.transform(df_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items_genres = df_items.select(df_items.item_id, explode(df_items.genres_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items_genres.createOrReplaceTempView(\"items_genres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [col]\n",
       "Index: []"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylist = [\n",
    "    {\"general\":\"general\",\"general_norm\":\"general\"}\n",
    "    , {\"general\":\"песни\",\"general_norm\":\"музыка\"}\n",
    "    , {\"general\":\"мюзиклы\",\"general_norm\":\"музыка\"}\n",
    "    , {\"general\":\"мультсериалы\",\"general_norm\":\"мультики\"}\n",
    "    , {\"general\":\"кулинария\",\"general_norm\":\"кулинария\"}\n",
    "    , {\"general\":\"боевики\",\"general_norm\":\"боевик\"}\n",
    "    , {\"general\":\"семьи\",\"general_norm\":\"семейные\"}\n",
    "    , {\"general\":\"семейные\",\"general_norm\":\"семейные\"}\n",
    "    , {\"general\":\"семейный\",\"general_norm\":\"семейные\"}\n",
    "    , {\"general\":\"аниме\",\"general_norm\":\"фэнтези\"}\n",
    "    , {\"general\":\"фэнтези\",\"general_norm\":\"фэнтези\"}\n",
    "    , {\"general\":\"драма\",\"general_norm\":\"драма\"}\n",
    "    , {\"general\":\"драмы\",\"general_norm\":\"драма\"}\n",
    "    , {\"general\":\"арт\",\"general_norm\":\"арт\"}\n",
    "    , {\"general\":\"артхаус\",\"general_norm\":\"арт\"}\n",
    "    , {\"general\":\"хаус\",\"general_norm\":\"арт\"}\n",
    "    , {\"general\":\"здоровье\",\"general_norm\":\"передачи\"}\n",
    "    , {\"general\":\"реалити\",\"general_norm\":\"передачи\"}\n",
    "    , {\"general\":\"полнометражные\",\"general_norm\":\"полнометражные\"}\n",
    "    , {\"general\":\"вестерн\",\"general_norm\":\"вестерн\"}\n",
    "    , {\"general\":\"романтические\",\"general_norm\":\"роман\"}\n",
    "    , {\"general\":\"музыкальные\",\"general_norm\":\"музыка\"}\n",
    "    , {\"general\":\"спектакли\",\"general_norm\":\"спектакли\"}\n",
    "    , {\"general\":\"экранизации\",\"general_norm\":\"экранизации\"}\n",
    "    , {\"general\":\"боевик\",\"general_norm\":\"боевик\"}\n",
    "    , {\"general\":\"фильмы\",\"general_norm\":\"полнометражные\"}\n",
    "    , {\"general\":\"приключение\",\"general_norm\":\"приключение\"}\n",
    "    , {\"general\":\"документальный\",\"general_norm\":\"документальный\"}\n",
    "    , {\"general\":\"документальные\",\"general_norm\":\"документальный\"}\n",
    "    , {\"general\":\"исторический\",\"general_norm\":\"документальный\"}\n",
    "    , {\"general\":\"научная\",\"general_norm\":\"документальный\"}\n",
    "    , {\"general\":\"видеоигры\",\"general_norm\":\"игры\"}\n",
    "    , {\"general\":\"игры\",\"general_norm\":\"игры\"}\n",
    "    , {\"general\":\"игры\",\"general_norm\":\"игры\"}\n",
    "    , {\"general\":\"мелодрамы\",\"general_norm\":\"мелодрамы\"}\n",
    "    , {\"general\":\"мелодрама\",\"general_norm\":\"мелодрамы\"}\n",
    "    , {\"general\":\"биография\",\"general_norm\":\"документальный\"}\n",
    "    , {\"general\":\"детские\",\"general_norm\":\"мультики\"}\n",
    "    , {\"general\":\"маленьких\",\"general_norm\":\"мультики\"}\n",
    "    , {\"general\":\"союзмультфильм\",\"general_norm\":\"мультики\"}\n",
    "    , {\"general\":\"мультфильмы\",\"general_norm\":\"мультики\"}\n",
    "    , {\"general\":\"анимация\",\"general_norm\":\"мультики\"}\n",
    "    , {\"general\":\"мультфильм\",\"general_norm\":\"мультики\"}\n",
    "    , {\"general\":\"приключения\",\"general_norm\":\"приключение\"}\n",
    "    , {\"general\":\"военный\",\"general_norm\":\"военные\"}\n",
    "    , {\"general\":\"военные\",\"general_norm\":\"военные\"}\n",
    "    , {\"general\":\"детективы\",\"general_norm\":\"детективы\"}\n",
    "    , {\"general\":\"развлекательные\",\"general_norm\":\"передачи\"}\n",
    "    , {\"general\":\"передачи\",\"general_norm\":\"передачи\"}\n",
    "    , {\"general\":\"познавательные\",\"general_norm\":\"передачи\"}\n",
    "    , {\"general\":\"знать\",\"general_norm\":\"передачи\"}\n",
    "    , {\"general\":\"эротика\",\"general_norm\":\"эротика\"}\n",
    "    , {\"general\":\"мистические\",\"general_norm\":\"фэнтези\"}\n",
    "    , {\"general\":\"юмористические\",\"general_norm\":\"комедии\"}\n",
    "    , {\"general\":\"охота\",\"general_norm\":\"передачи\"}\n",
    "    , {\"general\":\"фантастика\",\"general_norm\":\"фантастика\"}\n",
    "    , {\"general\":\"прочие\",\"general_norm\":\"прочие\"}\n",
    "    , {\"general\":\"детей\",\"general_norm\":\"мультики\"}\n",
    "    , {\"general\":\"музыкальный\",\"general_norm\":\"музыка\"}\n",
    "    , {\"general\":\"рыбалка\",\"general_norm\":\"передачи\"}\n",
    "    , {\"general\":\"сказка\",\"general_norm\":\"мультики\"}\n",
    "    , {\"general\":\"исторические\",\"general_norm\":\"передачи\"}\n",
    "    , {\"general\":\"спортивные\",\"general_norm\":\"передачи\"}\n",
    "    , {\"general\":\"пусто\",\"general_norm\":\"прочие\"}\n",
    "    , {\"general\":\"сказки\",\"general_norm\":\"мультики\"}\n",
    "    , {\"general\":\"фантастические\",\"general_norm\":\"фантастика\"}\n",
    "    , {\"general\":\"триллер\",\"general_norm\":\"триллер\"}\n",
    "    , {\"general\":\"ужасы\",\"general_norm\":\"ужасы\"}\n",
    "    , {\"general\":\"шоу\",\"general_norm\":\"передачи\"}\n",
    "    , {\"general\":\"развивающие\",\"general_norm\":\"передачи\"}\n",
    "    , {\"general\":\"спорт\",\"general_norm\":\"спорт\"}\n",
    "    , {\"general\":\"комедия\",\"general_norm\":\"комедии\"}\n",
    "    , {\"general\":\"криминал\",\"general_norm\":\"боевик\"}\n",
    "    , {\"general\":\"комедии\",\"general_norm\":\"комедии\"}\n",
    "    , {\"general\":\"животных\",\"general_norm\":\"передачи\"}\n",
    "    , {\"general\":\"триллеры\",\"general_norm\":\"триллер\"}\n",
    "    , {\"general\":\"взрослых\",\"general_norm\":\"эротика\"}\n",
    "    , {\"general\":\"западные\",\"general_norm\":\"зарубежные\"}\n",
    "    , {\"general\":\"русские\",\"general_norm\":\"русские\"}\n",
    "    , {\"general\":\"зарубежные\",\"general_norm\":\"зарубежные\"}\n",
    "    , {\"general\":\"сериалы\",\"general_norm\":\"короткометражные\"}\n",
    "    , {\"general\":\"короткометражные\",\"general_norm\":\"короткометражные\"}\n",
    "    , {\"general\":\"короткометражки\",\"general_norm\":\"короткометражные\"}\n",
    "    , {\"general\":\"наши\",\"general_norm\":\"русские\"}\n",
    "    , {\"general\":\"советские\",\"general_norm\":\"русские\"}\n",
    "    , {\"general\":\"кино\",\"general_norm\":\"полнометражные\"}\n",
    "    , {\"general\":\"советское\",\"general_norm\":\"русские\"}\n",
    "    , {\"general\":\"нет\",\"general_norm\":\"нет\"}\n",
    "\n",
    "]\n",
    "genres_schema = StructType(fields=[\n",
    "        StructField('general', StringType())\n",
    "        , StructField('general_norm', StringType())])\n",
    "#         , StructField('general_seg', StringType())\n",
    "#         , StructField('general_reg', StringType())\n",
    "#         , StructField('general_long', StringType())])\n",
    "df = spark.createDataFrame(mylist, schema=genres_schema)\n",
    "df.createOrReplaceTempView(\"genres\")\n",
    "spark.sql(f\"\"\"SELECT distinct col\n",
    "        FROM items_genres \n",
    "        left join genres\n",
    "            on items_genres.col = genres.general\n",
    "        where genres.general is null\n",
    "            and col not in ('для', 'про', 'в', 'о', 'вс', 'и', 'всей', 'd', 'самых', 'хочу')\n",
    "            and length(col) > 1\n",
    "        --GROUP BY col\n",
    "        limit 100\n",
    "        \"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>general_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>арт</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>боевик</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>вестерн</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>военные</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>детективы</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>документальный</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>драма</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>зарубежные</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>игры</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>комедии</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>короткометражные</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>кулинария</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>мелодрамы</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>музыка</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>мультики</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>нет</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>передачи</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>полнометражные</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>приключение</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>прочие</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>роман</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>русские</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>семейные</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>спектакли</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>спорт</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>триллер</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ужасы</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>фантастика</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>фэнтези</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>экранизации</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>эротика</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        general_norm\n",
       "0            general\n",
       "1                арт\n",
       "2             боевик\n",
       "3            вестерн\n",
       "4            военные\n",
       "5          детективы\n",
       "6     документальный\n",
       "7              драма\n",
       "8         зарубежные\n",
       "9               игры\n",
       "10           комедии\n",
       "11  короткометражные\n",
       "12         кулинария\n",
       "13         мелодрамы\n",
       "14            музыка\n",
       "15          мультики\n",
       "16               нет\n",
       "17          передачи\n",
       "18    полнометражные\n",
       "19       приключение\n",
       "20            прочие\n",
       "21             роман\n",
       "22           русские\n",
       "23          семейные\n",
       "24         спектакли\n",
       "25             спорт\n",
       "26           триллер\n",
       "27             ужасы\n",
       "28        фантастика\n",
       "29           фэнтези\n",
       "30       экранизации\n",
       "31           эротика"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"SELECT distinct general_norm\n",
    "    from genres\n",
    "    order by general_norm\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_genres = spark.sql(f\"\"\"SELECT DISTINCT item_id, general_norm\n",
    "        FROM items_genres \n",
    "        join genres\n",
    "            on items_genres.col = genres.general\n",
    "        where col not in ('для', 'про', 'в', 'о', 'вс', 'и', 'всей', 'd', 'самых', 'хочу')\n",
    "            and length(col) > 1\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_genres.createOrReplaceTempView(\"items_genres_for_user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_genres_vect = items_genres.groupBy('item_id').agg(collect_set('general_norm').alias('genres'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|item_id|              genres|\n",
      "+-------+--------------------+\n",
      "|   8389| [русские, мультики]|\n",
      "|   8638|[зарубежные, трил...|\n",
      "|  10817|[зарубежные, доку...|\n",
      "|  72820|[драма, зарубежны...|\n",
      "|  74757|[зарубежные, фэнт...|\n",
      "+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items_genres_vect.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(inputCol='genres', outputCol=\"genres_vector\", binary=False)\n",
    "count_vectorizer_model = count_vectorizer.fit(items_genres_vect)\n",
    "items_genres_vect = count_vectorizer_model.transform(items_genres_vect)\n",
    "\n",
    "normalizer = Normalizer(inputCol='genres_vector', outputCol=\"genres_norm\")\n",
    "items_genres_vect = normalizer.transform(items_genres_vect)\n",
    "items_genres_vect = items_genres_vect.drop(\"genres\", \"genres_vector\")\n",
    "items_genres_vect.createOrReplaceTempView(\"items_gender_vect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|item_id|         genres_norm|\n",
      "+-------+--------------------+\n",
      "|   8389|(32,[2,6],[0.7071...|\n",
      "|   8638|(32,[1,4,7,10,17]...|\n",
      "|  10817|(32,[1,19],[0.707...|\n",
      "|  72820|(32,[1,3,8],[0.57...|\n",
      "|  74757|(32,[1,10,18],[0....|\n",
      "+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items_genres_vect.createOrReplaceTempView(\"items_gender_vect\")\n",
    "items_genres_vect.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_gender = spark.sql(f\"\"\"SELECT user_id, general_norm\n",
    "        FROM train tr\n",
    "        JOIN items_genres_for_user g\n",
    "            ON tr.item_id = g.item_id\n",
    "        WHERE purchase = 1\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_gender_vect = user_gender.groupBy('user_id').agg(collect_set('general_norm').alias('genres'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_gender_vect = count_vectorizer_model.transform(user_gender_vect)\n",
    "user_gender_vect = normalizer.transform(user_gender_vect)\n",
    "user_gender_vect = user_gender_vect.drop(\"genres\", \"genres_vector\")\n",
    "user_gender_vect.createOrReplaceTempView(\"user_gender_vect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|user_id|         genres_norm|\n",
      "+-------+--------------------+\n",
      "| 754230|(32,[1,2,3,4,5,6,...|\n",
      "| 833685|(32,[1,2,4,6,9,14...|\n",
      "| 879401|(32,[1,2,3,4,5,6,...|\n",
      "| 776188|(32,[1,3,4,5,7,8,...|\n",
      "| 825061|(32,[1,2,3,4,5,7,...|\n",
      "+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_gender_vect.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vector_fin = spark.sql(f\"\"\"SELECT user_buy.user_id, buy, genres_norm\n",
    "        FROM user_buy            \n",
    "        LEFT JOIN user_gender_vect v\n",
    "            ON user_buy.user_id = v.user_id\n",
    "        \"\"\")\n",
    "user_vector_fin.createOrReplaceTempView(\"user_vector_fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_vector_fin = spark.sql(f\"\"\"SELECT item_buy.item_id, buy, genres_norm\n",
    "        FROM item_buy            \n",
    "        LEFT JOIN items_gender_vect v\n",
    "            ON item_buy.item_id = v.item_id\n",
    "        \"\"\")\n",
    "items_vector_fin.createOrReplaceTempView(\"items_vector_fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = spark.sql(f\"\"\"SELECT coalesce(items_vector_fin.buy, 0.001) as item_buy\n",
    "            , coalesce(user_vector_fin.buy, 0.001) as user_buy\n",
    "            , items_vector_fin.genres_norm as item_vect\n",
    "            , user_vector_fin.genres_norm as user_vect\n",
    "            , purchase\n",
    "        FROM train tr\n",
    "        JOIN user_vector_fin\n",
    "            ON tr.user_id = user_vector_fin.user_id\n",
    "        JOIN items_vector_fin\n",
    "            ON tr.item_id = items_vector_fin.item_id\n",
    "        WHERE items_vector_fin.genres_norm is Not Null\n",
    "            AND user_vector_fin.genres_norm is Not Null\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>purchase</th>\n",
       "      <th>count(1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>5021720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   purchase  count(1)\n",
       "0         1     10905\n",
       "1         0   5021720"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT purchase, count(1)\n",
    "            FROM train\n",
    "            GROUP BY purchase\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_buy</th>\n",
       "      <th>user_buy</th>\n",
       "      <th>item_vect</th>\n",
       "      <th>user_vect</th>\n",
       "      <th>purchase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005979</td>\n",
       "      <td>0.027576</td>\n",
       "      <td>(0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.23570226039551587, 0.23570226039551587...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005979</td>\n",
       "      <td>0.000776</td>\n",
       "      <td>(0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.5, 0.0, 0.0, 0.5, 0.5, 0.0, 0.5, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.005979</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>(0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.57735026...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005979</td>\n",
       "      <td>0.001931</td>\n",
       "      <td>(0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.2886751345948129, 0.2886751345948129, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005979</td>\n",
       "      <td>0.007501</td>\n",
       "      <td>(0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.4082482904638631, 0.4082482904638631, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_buy  user_buy                                          item_vect  \\\n",
       "0  0.005979  0.027576  (0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, ...   \n",
       "1  0.005979  0.000776  (0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, ...   \n",
       "2  0.005979  0.000384  (0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, ...   \n",
       "3  0.005979  0.001931  (0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, ...   \n",
       "4  0.005979  0.007501  (0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                           user_vect  purchase  \n",
       "0  (0.0, 0.23570226039551587, 0.23570226039551587...         0  \n",
       "1  (0.0, 0.5, 0.0, 0.0, 0.5, 0.5, 0.0, 0.5, 0.0, ...         0  \n",
       "2  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.57735026...         0  \n",
       "3  (0.0, 0.2886751345948129, 0.2886751345948129, ...         0  \n",
       "4  (0.0, 0.4082482904638631, 0.4082482904638631, ...         0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"item_buy\", \"user_buy\", \"item_vect\", \"user_vect\"], outputCol=\"features\")\n",
    "train_features = assembler.transform(train_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>purchase</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>(0.005979073243647235, 0.0019402405898331393, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>(0.005979073243647235, 0.004243827160493827, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>(0.005979073243647235, 0.00038880248833592535,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>(0.005979073243647235, 0.009220130618517095, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>(0.005979073243647235, 0.003436426116838488, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   purchase                                           features\n",
       "0         0  (0.005979073243647235, 0.0019402405898331393, ...\n",
       "1         0  (0.005979073243647235, 0.004243827160493827, 0...\n",
       "2         0  (0.005979073243647235, 0.00038880248833592535,...\n",
       "3         0  (0.005979073243647235, 0.009220130618517095, 0...\n",
       "4         0  (0.005979073243647235, 0.003436426116838488, 0..."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features = train_features.drop(\"item_buy\", \"user_buy\", \"item_vect\", \"user_vect\")\n",
    "train_features.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = train_features.sampleBy(\"purchase\", fractions={0: 0.005, 1: 0.8}, seed=5757).coalesce(10).cache()\n",
    "testDf = train_features.join(trainDF\\\n",
    "        , (train_features.features == trainDF.features)\\\n",
    "        , how=\"leftanti\").coalesce(10).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9463523099778809 0.9465536547897528 0.5412371134020618 47 0.07831650606993103\n",
      "0.9426927247324273 0.9428803728618713 0.5651358950328023 714 0.05525195667336894\n",
      "0.942154529891928 0.9423393486469038 0.57029053420806 404 0.05190801262257007\n",
      "0.9351221483471692 0.9352808807678584 0.6157450796626054 388 0.012304952739303954\n",
      "0.9464486822373821 0.9466500749468758 0.5412371134020618 300 0.07886660363914205\n",
      "0.9461732996697737 0.9463743226134613 0.5417057169634489 446 0.07706009096506941\n",
      "0.9384358177046549 0.9386069103907392 0.5941893158388004 680 0.03091689729438936\n",
      "0.939305728655756 0.9394800484802414 0.5885660731021556 586 0.03568653143911351\n",
      "0.9394710047529681 0.9396456396196481 0.5880974695407685 35 0.0365332495256837\n",
      "0.9395303644780232 0.9397052617457917 0.5876288659793815 256 0.036873964101161054\n",
      "0.9442865915459647 0.9444803885126795 0.5543580131208997 343 0.06517733881790237\n",
      "0.9443527019848496 0.9445467647078003 0.5538894095595126 664 0.06560234078921448\n",
      "0.9441026927899118 0.9442959325599226 0.5552952202436738 472 0.06389929946566912\n",
      "0.9421231041551342 0.9423079072913202 0.57029053420806 747 0.05174670896555044\n",
      "0.934493633611292 0.9346480943743729 0.6237113402061856 543 0.009389402485394372\n",
      "0.9482343624369798 0.9484433967097903 0.5276476101218369 195 0.0911426147778209\n",
      "0.9323741422519729 0.932510547992551 0.6579194001874414 245 0.0007173068070032596\n",
      "0.9430065165338949 0.943195718013428 0.5623242736644799 97 0.05715615174339447\n",
      "0.9436105890300435 0.9438024197267257 0.5576382380506092 238 0.060937276113720976\n",
      "0.9372297677614773 0.9373967675491673 0.6012183692596064 80 0.02384274309585003\n",
      "0.9356456778438313 0.9358065336534296 0.6119962511715089 925 0.014893076862262433\n",
      "0.9445573184488629 0.9447531131600007 0.5506091846298032 59 0.06694488663691266\n",
      "0.949408753860128 0.9496216323979181 0.521087160262418 485 0.09929724458323774\n",
      "0.939920974747209 0.9400969977464699 0.5857544517338332 977 0.039001061147119556\n",
      "0.935661507103846 0.9358223707806865 0.6119962511715089 750 0.01499960268441648\n",
      "0.9376073421694746 0.9377763928054725 0.5974695407685099 192 0.02605894856978619\n",
      "0.9484515491957106 0.9486609243106424 0.5271790065604499 774 0.09282691005245058\n",
      "0.938925593632168 0.9390983271335638 0.5913776944704779 641 0.03364940492672793\n",
      "0.9338139065636026 0.933961508328 0.6368322399250235 348 0.005990963167673092\n",
      "0.9388876499647799 0.9390603646079333 0.5913776944704779 358 0.03344369830799642\n",
      "0.9459111857465894 0.9461118455190711 0.5421743205248359 629 0.07548744067909366\n",
      "0.9356470745432444 0.9358079310470111 0.6119962511715089 464 0.014897428432357985\n",
      "0.9433549930374534 0.9435462309034522 0.5585754451733833 913 0.059132325808691245\n",
      "0.9334954590974249 0.9336408065010475 0.6410496719775071 740 0.004539892049458494\n",
      "0.9466828621723052 0.9468855357653572 0.5388940955951266 235 0.08062333820580218\n",
      "0.9465164221589155 0.9467185472323688 0.5398313027179007 964 0.0793646772509952\n",
      "0.942996041288297 0.9431847717637063 0.563261480787254 45 0.05710172070770408\n",
      "0.9361058903004347 0.9362690709289034 0.6077788191190253 797 0.017602198712004215\n",
      "0.9351833703381084 0.9353421331865138 0.6157450796626054 64 0.012643644870472582\n",
      "0.9412103610886993 0.9413923815965128 0.5749765698219307 872 0.0462736478568522\n",
      "0.9365879843811761 0.9367528020070298 0.604967197750703 912 0.01997556058188922\n",
      "0.9364734550293051 0.9366375170365567 0.6063730084348641 460 0.01931615835799492\n",
      "0.949317037265337 0.9495296373204699 0.5215557638238051 645 0.098682535033883\n",
      "0.9454933398388489 0.9456919287478329 0.5459231490159325 841 0.0731069157390938\n",
      "0.9485234792154833 0.9487328900800893 0.5271790065604499 321 0.0933230746084054\n",
      "0.9480041398170603 0.9482121280720534 0.5295220243673852 575 0.08948788517375589\n",
      "0.9469698839016891 0.947173864641005 0.5365510777881912 478 0.0824958847453653\n",
      "0.9486114712785061 0.9488209258757233 0.5271790065604499 171 0.0939421115807365\n",
      "0.933650259949039 0.9337966158853839 0.6391752577319587 110 0.005298358001466209\n",
      "0.9457014480513948 0.9459013048861264 0.5435801312089972 885 0.07415461763948276\n",
      "0.9482581063270018 0.9484671524006756 0.5276476101218369 298 0.09129593887586301\n",
      "0.9469950244911242 0.947199250624402 0.5360824742268041 452 0.08269442393746326\n",
      "0.9391143808361667 0.9392879068627862 0.5899718837863168 688 0.0347134756637464\n",
      "0.9482615980755345 0.9484706458846294 0.5276476101218369 871 0.09131971441791263\n",
      "0.9472087195013225 0.9474139834380912 0.5342080599812559 542 0.08428740303202169\n",
      "0.9429150327223395 0.9431037229359798 0.563261480787254 784 0.05661447712324046\n",
      "0.936054910771858 0.9362178331642487 0.6082474226804123 206 0.017280285939512887\n",
      "0.9376112994844783 0.9377803520872868 0.5974695407685099 17 0.025610516631936076\n",
      "0.9423063045614806 0.9424921303451469 0.5684161199625117 399 0.05276991102704848\n",
      "0.9358407501951888 0.9360026345526989 0.6101218369259607 60 0.016123113656300803\n",
      "0.9409913120640843 0.9411720592084977 0.5773195876288659 149 0.04509487567237324\n",
      "0.9323625030901974 0.9324991359449688 0.6574507966260543 395 0.0006750952660159793\n",
      "0.9372837734721156 0.9374510329999153 0.6007497656982193 869 0.02409347366652509\n",
      "0.9388352737367901 0.9390079623486274 0.5913776944704779 959 0.03321385451628537\n",
      "0.9390547883278761 0.9392280518377123 0.5904404873477038 482 0.03436896680289884\n",
      "0.9349259120796342 0.9350838482728682 0.6171508903467666 845 0.0115061249656089\n",
      "0.9356435827947117 0.9358044375630574 0.6119962511715089 469 0.01488647814048575\n",
      "0.9457680240567506 0.945968379778038 0.542642924086223 235 0.07457430572861813\n",
      "0.9367115922792318 0.9368769371368524 0.6040299906279287 997 0.020653815102884422\n",
      "0.9362083149240591 0.9363717793571431 0.6073102155576382 477 0.018046281949710054\n",
      "0.9444134584093177 0.9446080171264557 0.5529522024367385 658 0.06595844225736425\n",
      "0.943750491754585 0.9439426248827354 0.5571696344892221 508 0.061700825995274294\n",
      "0.9476414635361357 0.9478485728419352 0.5309278350515464 347 0.08712916483754182\n",
      "0.9463874602464429 0.9465888225282204 0.5412371134020618 584 0.07846447193621435\n",
      "0.9404477632091683 0.9406256783181344 0.5824742268041238 138 0.04204899068849968\n",
      "0.9449705086918933 0.9451676732558432 0.5482661668228679 618 0.06965053877526743\n",
      "0.9456113609392525 0.9458107072022597 0.5445173383317713 537 0.07370805520128301\n",
      "0.9452654450512845 0.9454632219983287 0.5473289597000938 589 0.07166148482942021\n",
      "0.9456835237422605 0.9458829058706367 0.5445173383317713 247 0.07405622175270823\n",
      "0.9375684673691443 0.9377372657851908 0.5979381443298969 715 0.02581739732522259\n",
      "0.947069747909723 0.9472740111810118 0.5360824742268041 403 0.08325357858591308\n",
      "0.9432935382632788 0.9434840468890758 0.5599812558575445 292 0.05873856994906462\n",
      "0.9379181077888805 0.938087778675215 0.5965323336457357 774 0.027681339928750395\n",
      "0.9356931656238754 0.9358538121362701 0.612464854732896 371 0.015135684156838858\n",
      "0.9398914112762993 0.9400636931994444 0.5932521087160263 13 0.030185519893765035\n",
      "0.943737223110161 0.9439293496437112 0.5571696344892221 674 0.06160839284201885\n",
      "0.9445538267003303 0.9447496196760469 0.5506091846298032 780 0.06692182368062931\n",
      "0.9463464903969931 0.9465478323164966 0.5412371134020618 736 0.0782701977458091\n",
      "0.9403856100852871 0.940563494303758 0.5824742268041238 51 0.041681086252923584\n",
      "0.9434718502216795 0.9436633790653672 0.5581068416119962 167 0.059924483540830686\n",
      "0.9452787136957086 0.9454764972373529 0.5473289597000938 446 0.07175350959459413\n",
      "0.9372700392612205 0.9374372919630306 0.6007497656982193 880 0.024035588214850134\n",
      "0.9439718686115551 0.9441648104621926 0.555763823805061 183 0.06305705193018461\n",
      "0.9486731588359162 0.9488828769911694 0.5267104029990628 817 0.09440936014573065\n",
      "0.9450370846972491 0.9452342823498943 0.5482661668228679 919 0.07011515646858267\n",
      "0.934493633611292 0.9346480943743729 0.6237113402061856 53 0.00938457792493811\n",
      "0.9373452282462903 0.9375125183175008 0.6007497656982193 229 0.024347979546836908\n",
      "0.9436815879168741 0.9438734539004515 0.5576382380506092 364 0.061253052572437065\n",
      "0.9354175502730314 0.9355771282071347 0.6143392689784443 648 0.01366292373305943\n",
      "0.9345534589028182 0.934708182298377 0.6232427366447985 578 0.009738683515565305\n",
      "0.9345534589028182 0.934708182298377 0.6232427366447985 [578, 0.009738683515565305]\n"
     ]
    }
   ],
   "source": [
    "# 0.8346456692913385 [720, 0.06649297309185655]\n",
    "import random\n",
    "lgp = [0.0, 0.0, 0.0]\n",
    "maxIter = 0\n",
    "regParam = 0\n",
    "lgp_save = [[maxIter, regParam], [maxIter, regParam], [maxIter, regParam]]\n",
    "lMod = [0, 0, 0]\n",
    "for i in range(100):\n",
    "    maxIter, regParam = int(random.uniform(1, 1000)), random.uniform(0, 0.1)\n",
    "    lr = LogisticRegression(featuresCol=\"features\", labelCol=\"purchase\", maxIter=maxIter, regParam=regParam)\n",
    "    model = lr.fit(trainDF)\n",
    "    predictions = model.transform(testDf)\n",
    "    predictions.createOrReplaceTempView(\"predictions\")\n",
    "    df = spark.sql(f\"\"\"SELECT sum(case when purchase = prediction then 1 else 0 end) / count(1) as pr\n",
    "                , sum(case when (purchase = 0 and purchase = prediction) then 1 else 0 end) / (count(1) - sum(purchase)) as pr0\n",
    "                , sum(case when (purchase = 1 and purchase = prediction) then 1 else 0 end) / sum(purchase) as pr1                                    \n",
    "        FROM predictions\n",
    "        \"\"\").toPandas()\n",
    "    res = [df.iloc[0,0], df.iloc[0,0], df.iloc[0,0]]\n",
    "    for i in range(3):\n",
    "        if lgp[i] < res[i]:\n",
    "            lgp[i] = res[i]\n",
    "            lgp_save[i] = [maxIter, regParam]\n",
    "            lMod[i] = model\n",
    "    print(df.iloc[0,0], df.iloc[0,1], df.iloc[0,2], maxIter, regParam)\n",
    "print(df.iloc[0,0], df.iloc[0,1], df.iloc[0,2], [maxIter, regParam])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8719941193785606 129 14 59\n",
      "0.8563502119427658 11 11 63\n",
      "0.8756268945793464 89 22 41\n",
      "0.8776197564053544 159 26 53\n",
      "0.8705181741819135 180 14 50\n",
      "0.8776197564053544 [180, 14, 50]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "lgp = 0.0\n",
    "maxIter = 0\n",
    "maxDepth = 5, \n",
    "maxBins = 32\n",
    "lgp_save = [maxIter, regParam]\n",
    "lMod = None\n",
    "for i in range(5):\n",
    "    numTrees, maxDepth, maxBins = int(random.uniform(10, 200)), int(random.uniform(5, 30)), int(random.uniform(32, 64))\n",
    "    lr = RandomForestClassifier(featuresCol=\"features\", labelCol=\"purchase\", numTrees=numTrees, maxDepth=maxDepth, maxBins=maxBins)\n",
    "    model = lr.fit(trainDF)\n",
    "    predictions = model.transform(testDf)\n",
    "    predictions.createOrReplaceTempView(\"predictions\")\n",
    "    df = spark.sql(f\"\"\"SELECT sum(case when purchase = prediction then 1 else 0 end) / count(1) as pr    \n",
    "        FROM predictions\n",
    "        --WHERE purchase = 1\n",
    "        \"\"\").toPandas()\n",
    "    res = df.iloc[0,0]\n",
    "    if lgp < res:\n",
    "        lgp = res\n",
    "        lgp_save = [numTrees, maxDepth, maxBins]\n",
    "        lMod_RF = model\n",
    "    print(df.iloc[0,0], numTrees, maxDepth, maxBins)\n",
    "print(lgp, [numTrees, maxDepth, maxBins])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8543233082706767 41 15 8\n",
      "0.8411654135338346 78 27 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <object repr() failed>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/hdp/current/spark2-client/python/pyspark/ml/wrapper.py\", line 40, in __del__\n",
      "    if SparkContext._active_spark_context and self._java_obj is not None:\n",
      "AttributeError: 'GBTClassifier' object has no attribute '_java_obj'\n",
      "Exception ignored in: <object repr() failed>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/hdp/current/spark2-client/python/pyspark/ml/wrapper.py\", line 40, in __del__\n",
      "    if SparkContext._active_spark_context and self._java_obj is not None:\n",
      "AttributeError: 'GBTClassifier' object has no attribute '_java_obj'\n",
      "Exception ignored in: <object repr() failed>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/hdp/current/spark2-client/python/pyspark/ml/wrapper.py\", line 40, in __del__\n",
      "    if SparkContext._active_spark_context and self._java_obj is not None:\n",
      "AttributeError: 'GBTClassifier' object has no attribute '_java_obj'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8458646616541353 26 29 15\n",
      "0.856203007518797 91 22 10\n",
      "0.8604323308270677 32 15 11\n",
      "0.8999060150375939 62 10 6\n",
      "0.8383458646616542 60 28 29\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-6f83ad4bfa92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmaxIter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxDepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxBins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGBTClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturesCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"purchase\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxIter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxIter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxDepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxDepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxBins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxBins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainDF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestDf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predictions\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/bd9/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# lMod_GBT  0.8999060150375939 62 10 6\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "lgp = 0.0\n",
    "maxIter = 0\n",
    "maxDepth = 5, \n",
    "maxBins = 32\n",
    "lgp_save = [maxIter, regParam]\n",
    "lMod = None\n",
    "for i in range(100):\n",
    "    maxIter, maxDepth, maxBins = int(random.uniform(10, 100)), int(random.uniform(5, 30)), int(random.uniform(32, 30))\n",
    "    lr = GBTClassifier(featuresCol=\"features\", labelCol=\"purchase\", maxIter=maxIter, maxDepth=maxDepth, maxBins=maxBins)\n",
    "    model = lr.fit(trainDF)\n",
    "    predictions = model.transform(testDf)\n",
    "    predictions.createOrReplaceTempView(\"predictions\")\n",
    "    df = spark.sql(f\"\"\"SELECT sum(case when purchase = prediction then 1 else 0 end) / count(1) as pr    \n",
    "        FROM predictions\n",
    "        WHERE purchase = 1\n",
    "        \"\"\").toPandas()\n",
    "    res = df.iloc[0,0]\n",
    "    if lgp < res:\n",
    "        lgp = res\n",
    "        lgp_save = [maxIter, maxDepth, maxBins]\n",
    "        lMod_GBT = model\n",
    "    print(df.iloc[0,0], maxIter, maxDepth, maxBins)\n",
    "print(lgp, [maxIter, maxDepth, maxBins])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType() \\\n",
    "      .add(\"user_id\", IntegerType(), True) \\\n",
    "      .add(\"item_id\", IntegerType(), True) \n",
    "      \n",
    "      \n",
    "df_user_test = spark.read.format(\"csv\") \\\n",
    "      .option(\"header\", True) \\\n",
    "      .schema(schema) \\\n",
    "      .load(\"/labs/slaba03/laba03_test.csv\")\n",
    "df_user_test.createOrReplaceTempView(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"SELECT tr.user_id\n",
    "            , tr.item_id\n",
    "            , coalesce(items_vector_fin.buy, 0.001) as item_buy\n",
    "            , coalesce(user_vector_fin.buy, 0.001) as user_buy\n",
    "            , items_vector_fin.genres_norm as item_vect\n",
    "            , user_vector_fin.genres_norm as user_vect\n",
    "        FROM test tr\n",
    "        LEFT JOIN user_vector_fin\n",
    "            ON tr.user_id = user_vector_fin.user_id\n",
    "        LEFT JOIN items_vector_fin\n",
    "            ON tr.item_id = items_vector_fin.item_id\n",
    "        WHERE items_vector_fin.genres_norm is null\n",
    "        \"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prep = spark.sql(f\"\"\"SELECT tr.user_id\n",
    "            , tr.item_id\n",
    "            , coalesce(items_vector_fin.buy, 0.001) as item_buy\n",
    "            , coalesce(u1.buy, 0.001) as user_buy\n",
    "            , items_vector_fin.genres_norm as item_vect\n",
    "            , u2.genres_norm as user_vect\n",
    "        FROM test tr\n",
    "        LEFT JOIN user_vector_fin u1\n",
    "            ON tr.user_id = u1.user_id\n",
    "        LEFT JOIN items_vector_fin\n",
    "            ON tr.item_id = items_vector_fin.item_id\n",
    "        LEFT JOIN user_vector_fin u2\n",
    "            ON (tr.user_id != u2.user_id AND u2.user_id = 999999)\n",
    "        WHERE u1.genres_norm is null\n",
    "        \n",
    "        UNION ALL\n",
    "        \n",
    "        SELECT tr.user_id\n",
    "            , tr.item_id\n",
    "            , coalesce(items_vector_fin.buy, 0.001) as item_buy\n",
    "            , coalesce(u1.buy, 0.001) as user_buy\n",
    "            , items_vector_fin.genres_norm as item_vect\n",
    "            , u1.genres_norm as user_vect\n",
    "        FROM test tr\n",
    "        LEFT JOIN user_vector_fin u1\n",
    "            ON tr.user_id = u1.user_id\n",
    "        LEFT JOIN items_vector_fin\n",
    "            ON tr.item_id = items_vector_fin.item_id\n",
    "        WHERE u1.genres_norm is NOT null\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prep = assembler.transform(test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prep = test_prep.drop(\"item_buy\", \"user_buy\", \"item_vect\", \"user_vect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lMod_GBT.transform(test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lMod_RF.transform(test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.classification.GBTClassificationModel"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lMod_GBT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------------------+--------------------+--------------------+----------+\n",
      "|user_id|item_id|            features|       rawPrediction|         probability|prediction|\n",
      "+-------+-------+--------------------+--------------------+--------------------+----------+\n",
      "| 816426|   8389|(66,[0,4,8,64],[0...|[3.34786493254183...|[0.96603485069953...|       0.0|\n",
      "+-------+-------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = predictions\\\n",
    "        .sort(\"user_id\", \"item_id\")\\\n",
    "        .select(\"user_id\", \"item_id\", predictions.prediction.alias(\"purchase\"))\\\n",
    "        .toPandas()\n",
    "df.to_csv('lab03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'Field \"features\" does not exist.\\nAvailable fields: user_id, item_id, item_buy, user_buy, item_vect, user_vect'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o11656.transform.\n: java.lang.IllegalArgumentException: Field \"features\" does not exist.\nAvailable fields: user_id, item_id, item_buy, user_buy, item_vect, user_vect\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\n\tat scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:59)\n\tat org.apache.spark.sql.types.StructType.apply(StructType.scala:273)\n\tat org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:41)\n\tat org.apache.spark.ml.PredictorParams$class.validateAndTransformSchema(Predictor.scala:51)\n\tat org.apache.spark.ml.classification.ClassificationModel.org$apache$spark$ml$classification$ClassifierParams$$super$validateAndTransformSchema(Classifier.scala:141)\n\tat org.apache.spark.ml.classification.ClassifierParams$class.validateAndTransformSchema(Classifier.scala:42)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel.org$apache$spark$ml$classification$ProbabilisticClassifierParams$$super$validateAndTransformSchema(ProbabilisticClassifier.scala:77)\n\tat org.apache.spark.ml.classification.ProbabilisticClassifierParams$class.validateAndTransformSchema(ProbabilisticClassifier.scala:37)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.org$apache$spark$ml$classification$LogisticRegressionParams$$super$validateAndTransformSchema(LogisticRegression.scala:930)\n\tat org.apache.spark.ml.classification.LogisticRegressionParams$class.validateAndTransformSchema(LogisticRegression.scala:266)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.validateAndTransformSchema(LogisticRegression.scala:930)\n\tat org.apache.spark.ml.PredictionModel.transformSchema(Predictor.scala:192)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel.transform(ProbabilisticClassifier.scala:104)\n\tat sun.reflect.GeneratedMethodAccessor143.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-60b3165c45e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlMod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_prep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'Field \"features\" does not exist.\\nAvailable fields: user_id, item_id, item_buy, user_buy, item_vect, user_vect'"
     ]
    }
   ],
   "source": [
    "predictions = lMod.transform(test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>buy</th>\n",
       "      <th>genres_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>999999</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  buy                                        genres_norm\n",
       "0   999999  1.0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ..."
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT *\n",
    "            FROM user_vector_fin\n",
    "            WHERE user_id = 999999 \"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_vect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           item_vect\n",
       "0  (0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, ...\n",
       "1  (0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, ...\n",
       "2  (0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, ...\n",
       "3  (0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, ...\n",
       "4  (0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, ...\n",
       "5  (0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, ...\n",
       "6  (0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, ...\n",
       "7  (0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, ...\n",
       "8  (0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, ...\n",
       "9  (0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, ..."
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT item_vect\n",
    "        FROM test_prep\n",
    "        limit 10\n",
    "        \"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------------------------\n",
      " item_vect | (31,[2,6],[0.7071067811865475,0.7071067811865475]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT item_vect\n",
    "        FROM test_prep\n",
    "        limit 1\n",
    "        \"\"\").show(1, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "295038"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT DISTINCT user_id, item_id\n",
    "        FROM test_prep\n",
    "        WHERE item_buy is null\n",
    "            or user_buy is null\n",
    "            or item_vect is null\n",
    "            or user_vect is null\n",
    "        \"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT DISTINCT user_id\n",
    "        FROM test_prep\n",
    "        WHERE item_buy is null\n",
    "            or user_buy is null\n",
    "            or item_vect is null\n",
    "            or user_vect is null\n",
    "        \"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2156840"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prep.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"purchase\", maxIter=40, regParam=0.1)\n",
    "model = lr.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Расчет категории для года выпуска материала\n",
    "df_items = df_items.withColumn(\"year_cat\", array((((df_items.year - 1900.0)/10) + 1).cast(IntegerType()).cast(StringType())))\\\n",
    "        .withColumn(\"year_cat_str\", (((df_items.year - 1900.0)/10) + 1).cast(IntegerType()).cast(StringType()))\n",
    "\n",
    "count_vectorizer_year = CountVectorizer(inputCol='year_cat', outputCol=\"year_cat_vector\", binary=False)\n",
    "count_vectorizer_year_model = count_vectorizer_year.fit(df_items)\n",
    "df_items = count_vectorizer_year_model.transform(df_items)\n",
    "\n",
    "\n",
    "normalizer_year = Normalizer(inputCol='year_cat_vector', outputCol=\"year_cat_norm\")\n",
    "df_items = normalizer_year.transform(df_items)\n",
    "df_items = df_items.drop(\"year_cat_vector\", \"region_id\", \"channel_id\", \"datetime_show_start\", \"datetime_show_stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "train_features = VectorAssembler(inputCols=[\"item_buy\", \"user_buy\"], outputCol=\"features\").transform(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizer_year = Normalizer(inputCol='features', outputCol=\"train_features_norm\")\n",
    "# train_features = normalizer_year.transform(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.select(\"purchase\", \"features\")\\\n",
    "    .limit(10).show(2, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = train_features.sampleBy(\"purchase\", fractions={0: 0.8, 1: 0.8}, seed=5757)\n",
    "testDf = train_features.join(trainDF\\\n",
    "        , (train_features.user_id == trainDF.user_id) & (train_features.item_id == trainDF.item_id)\\\n",
    "        , how=\"leftanti\").coalesce(10).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'{trainDF.count()} {testDf.count()} {trainDF.count() + testDf.count()} {train_features.count()}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "lgp = 0.0\n",
    "lgp_save = [maxIter, regParam]\n",
    "for i in range(100):\n",
    "    maxIter, regParam = int(random.uniform(10, 1000)), random.uniform(0, 1)\n",
    "    lr = LogisticRegression(featuresCol=\"features\", labelCol=\"purchase\", maxIter=maxIter, regParam=regParam)\n",
    "    model = lr.fit(trainDF)\n",
    "    predictions = model.transform(testDf)\n",
    "    predictions.createOrReplaceTempView(\"predictions\")\n",
    "    df = spark.sql(f\"\"\"SELECT sum(case when purchase = prediction then 1 else 0 end) / count(1) as pr    \n",
    "        FROM predictions\n",
    "        WHERE purchase = 1\n",
    "        \"\"\").toPandas()\n",
    "    res = df.iloc[0,0]\n",
    "    if lgp < res:\n",
    "        lgp = res\n",
    "        lgp_save = [maxIter, regParam]\n",
    "    print(df.iloc[0,0], maxIter, regParam)\n",
    "print(lgp, [maxIter, regParam])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    maxIter, regParam = int(random.uniform(10, 100)), random.uniform(0, 1)\n",
    "    lr = LogisticRegression(featuresCol=\"features\", labelCol=\"purchase\", maxIter=maxIter, regParam=regParam)\n",
    "    model = lr.fit(trainDF)\n",
    "    predictions = model.transform(testDf)\n",
    "    predictions.createOrReplaceTempView(\"predictions\")\n",
    "    df = spark.sql(f\"\"\"SELECT sum(case when purchase = prediction then 1 else 0 end) / count(1) as pr    \n",
    "        FROM predictions\n",
    "        WHERE purchase = 1\n",
    "        \"\"\").toPandas()\n",
    "    res = df.iloc[0,0]\n",
    "    if lgp < res:\n",
    "        lgp = res\n",
    "        lgp_save = [maxIter, regParam]\n",
    "    print(df.iloc[0,0], maxIter, regParam)\n",
    "print(lgp, [maxIter, regParam])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    maxIter, regParam = int(random.uniform(10, 100)), random.uniform(0, 0.1)\n",
    "    lr = LogisticRegression(featuresCol=\"features\", labelCol=\"purchase\", maxIter=maxIter, regParam=regParam)\n",
    "    model = lr.fit(trainDF)\n",
    "    predictions = model.transform(testDf)\n",
    "    predictions.createOrReplaceTempView(\"predictions\")\n",
    "    df = spark.sql(f\"\"\"SELECT sum(case when purchase = prediction then 1 else 0 end) / count(1) as pr    \n",
    "        FROM predictions\n",
    "        WHERE purchase = 1\n",
    "        \"\"\").toPandas()\n",
    "    res = df.iloc[0,0]\n",
    "    if lgp < res:\n",
    "        lgp = res\n",
    "        lgp_save = [maxIter, regParam]\n",
    "    print(df.iloc[0,0], maxIter, regParam)\n",
    "print(lgp, [maxIter, regParam])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.005968778696051423 [29, 0.020436539365475917]maxIter=maxIter, regParam=regParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(f\"\"\"SELECT sum(case when purchase = prediction then 1 else 0 end) / count(1) as pr    \n",
    "    FROM predictions\n",
    "    WHERE purchase = 1\n",
    "    \"\"\").toPandas()\n",
    "print(df.iloc[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(testDf)\n",
    "predictions.createOrReplaceTempView(\"predictions\")\n",
    "df = spark.sql(f\"\"\"SELECT purchase, prediction, sum(case when prediction = prediction then 1 else 0 end) as pr\n",
    "        FROM predictions\n",
    "        GROUP BY purchase, prediction\n",
    "        limit 10\n",
    "        \"\"\").toPandas()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "numFolds = 3 # more (10 or so) in practice\n",
    "kf = KFold(n_splits=numFolds)\n",
    "alphas = [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "tasks = []\n",
    "for alpha in alphas:\n",
    "    for fold in range(numFolds):\n",
    "        tasks = tasks + [(alpha, fold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "tasksRDD = sc.parallelize(tasks, numSlices = len(tasks))\n",
    "tasksRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingFeaturesBroadcast = sc.broadcast(trainDF)\n",
    "trainingLabelsBroadcast = sc.broadcast(testDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"purchase\", maxIter=10, regParam=0.01)\n",
    "model = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsDf = model.transform(testDf)\n",
    "predictionsDf.registerTempTable('Predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"purchase\", maxIter=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lr.fit(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType() \\\n",
    "      .add(\"user_id\", IntegerType(), True) \\\n",
    "      .add(\"item_id\", IntegerType(), True) \n",
    "      \n",
    "      \n",
    "df_user_test = spark.read.format(\"csv\") \\\n",
    "      .option(\"header\", True) \\\n",
    "      .schema(schema) \\\n",
    "      .load(\"/labs/slaba03/laba03_test.csv\")\n",
    "df_user_test.createOrReplaceTempView(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = spark.sql(f\"\"\"SELECT test.user_id\n",
    "            , test.item_id\n",
    "            , coalesce(item_buy.buy, 0.002166) as item_buy\n",
    "            , coalesce(user_buy.buy, 0.002165) as user_buy\n",
    "        FROM test\n",
    "        LEFT JOIN item_buy\n",
    "            ON test.item_id = item_buy.item_id\n",
    "        LEFT JOIN user_buy\n",
    "            ON test.user_id = user_buy.user_id        \n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = VectorAssembler(inputCols=[\"item_buy\", \"user_buy\"], outputCol=\"features\").transform(df_test)\n",
    "# df_test = normalizer_year.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.groupby('prediction').count().limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.groupby('prediction').count().limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = predictions\\\n",
    "        .sort(\"user_id\", \"item_id\")\\\n",
    "        .select(\"user_id\", \"item_id\", predictions.prediction.alias(\"purchase\"))\\\n",
    "        .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('lab03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "VectorAssembler(inputCols=[\"buy\"], outputCol=\"features\").transform(df_user_buy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_buy.select((1.0, Vectors.dense([0.0, 1.1, 0.1]))).limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1.0, Vectors.dense([0.0, 1.1, 0.1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 8544\n",
    "# df_user.filter((df_user.item_id == 8544) & (df_user.purchase == 1)).limit(100).toPandas()\n",
    "# 824754\n",
    "item_ids = df_user.filter((df_user.user_id == 824754) & (df_user.purchase == 1)).collect()\n",
    "item_ids = [x[1] for x in item_ids]\n",
    "item_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType() \\\n",
    "      .add(\"user_id\", IntegerType(), True) \\\n",
    "      .add(\"item_id\", IntegerType(), True) \n",
    "      \n",
    "      \n",
    "df_user_test = spark.read.format(\"csv\") \\\n",
    "      .option(\"header\", True) \\\n",
    "      .schema(schema) \\\n",
    "      .load(\"/labs/slaba03/laba03_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_user_test.select(df_user_test.user_id).distinct().count()\n",
    "df_user_test.select(df_user_test.item_id).distinct().count()\n",
    "# df_user_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_user_test.where(df_user_test.item_id.isin(ids)).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# views_programmes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_users_schema = StructType(fields=[\n",
    "    StructField('user_id', IntegerType()), \n",
    "    StructField('item_id', IntegerType()),\n",
    "    StructField('ts_start', IntegerType()),\n",
    "    StructField('ts_end', IntegerType()),\n",
    "    StructField('item_type', StringType()),\n",
    "]) \n",
    "\n",
    "df_views_programmes = spark.read.format(\"csv\") \\\n",
    "      .option(\"header\", True) \\\n",
    "      .schema(read_users_schema) \\\n",
    "      .load(\"/labs/slaba03/laba03_views_programmes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_views_programmes.show(5, False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# items.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_items_schema = StructType(fields=[\n",
    "    StructField('item_id', IntegerType()), \n",
    "    StructField('channel_id', FloatType()),\n",
    "    StructField('datetime_availability_start', StringType()),\n",
    "    StructField('datetime_availability_stop', StringType()),\n",
    "    StructField('datetime_show_start', StringType()),\n",
    "    StructField('datetime_show_stop', StringType()),\n",
    "    StructField('content_type', IntegerType()),\n",
    "    StructField('title', StringType(), nullable=True),\n",
    "    StructField('year', FloatType(), nullable=True),\n",
    "    StructField('genres', StringType()),\n",
    "    StructField('region_id', IntegerType()),\n",
    "]) \n",
    "\n",
    "df_items = (spark.read.format(\"csv\")\n",
    "            .option(\"header\", True)\n",
    "            .option(\"sep\", \"\\t\")\n",
    "            .schema(read_items_schema)\n",
    "            .load(\"/labs/slaba03/laba03_items.csv\")\n",
    "#             .filter(\"content_type == 1 and datetime_availability_stop == '2099-12-31T21:00:00Z'\")\n",
    "#             .drop('channel_id', 'content_type', 'region_id', \n",
    "#                   'datetime_availability_start', \n",
    "#                   'datetime_show_start', 'datetime_show_stop')\n",
    "           )\n",
    "            \n",
    "    \n",
    "df_items = (df_items\n",
    "            .withColumn(\"year\", \n",
    "                        when(df_items.item_id == 103377, 2008.0)\n",
    "                        .when(df_items.item_id == 95141, 2014.0)\n",
    "                        .when(df_items.item_id == 72544, 2009.0)\n",
    "                        .when(df_items.item_id == 8544, 1994.0)\n",
    "                        .otherwise(df_items.year))\n",
    "            .withColumn(\"genres\", \n",
    "                        when(df_items.item_id == 103377, 'Анимация, Короткометражные')\n",
    "                        .otherwise(df_items.genres))\n",
    "           )\n",
    "\n",
    "#.withColumn(df_items.genres, 'Анимация,Короткометражные'))\n",
    "    \n",
    "df_items = (df_items\n",
    "#             .filter(~df_items.genres.contains('Прочие'))\n",
    "            .filter(~df_items.item_id.isNull())\n",
    "            .repartition(16)\n",
    "            .cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Расчет категории для года выпуска материала\n",
    "df_items = df_items.withColumn(\"year_cat\", array((((df_items.year - 1910.0)/10) + 1).cast(IntegerType()).cast(StringType())))\\\n",
    "        .withColumn(\"year_cat_str\", (((df_items.year - 1910.0)/10) + 1).cast(IntegerType()).cast(StringType()))\n",
    "\n",
    "count_vectorizer_year = CountVectorizer(inputCol='year_cat', outputCol=\"year_cat_vector\", binary=False)\n",
    "count_vectorizer_year_model = count_vectorizer_year.fit(df_items)\n",
    "df_items = count_vectorizer_year_model.transform(df_items)\n",
    "\n",
    "\n",
    "normalizer_year = Normalizer(inputCol='year_cat_vector', outputCol=\"year_cat_norm\")\n",
    "df_items = normalizer_year.transform(df_items)\n",
    "df_items = df_items.drop(\"year_cat_vector\", \"region_id\", \"channel_id\", \"datetime_show_start\", \"datetime_show_stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# расчет фичи жанра для контента\n",
    "\n",
    "def replace_genres(s):\n",
    "    replace_map = {\n",
    "        'Арт-хаус': 'Артхаус',\n",
    "        'Боевики': 'Боевик',\n",
    "        'Военные': 'Военный',\n",
    "        'Военные': 'Военный',\n",
    "        'Детские': 'Детский',\n",
    "        'Для детей': 'Детский',\n",
    "        'Для самых маленьких': 'Детский',\n",
    "        'Для всей семьи': 'Семейные',\n",
    "        'Для взрослых': 'Эротика',\n",
    "        'Документальные': 'Документальный',\n",
    "        'Драмы': 'Драма',\n",
    "        'Западные мультфильмы': 'Зарубежные,Анимация',\n",
    "        'Исторические': 'Исторический',\n",
    "        'Короткометражки': 'Короткометражные',\n",
    "        'Детский песни': 'Детский,Музыкальные',\n",
    "        'Мультфильмы в 3D': 'Анимация',\n",
    "        'Мультфильмы': 'Анимация',\n",
    "        'Мультсериалы': 'Анимация,Сериалы',\n",
    "        'Мюзиклы': 'Музыкальные',\n",
    "        'Русские мультфильмы': 'Анимация,Русские',\n",
    "        'Аниме': 'Анимация',\n",
    "        'Спорт': 'Спортивные',\n",
    "        'Спортивныеивные': 'Спортивные',\n",
    "        'Наши': 'Русские',\n",
    "        'Фильмы в 3D': 'Фильмы',\n",
    "        'Юмористические': 'Юмористические,Передачи',\n",
    "        'Кулинария': 'Передачи',\n",
    "        'Игры': 'Передачи',\n",
    "        'О здоровье': 'Передачи',\n",
    "        'Охота и рыбалка': 'Передачи',\n",
    "        'Реалити-шоу': 'Передачи',\n",
    "        'Видеоигры': 'Видеоигры,Передачи',\n",
    "        'Фильмы-спектакли': 'Музыкальные,Фильмы',\n",
    "        'Познавательные': 'Развивающие,Передачи',\n",
    "        'Хочу всё знать': 'Развивающие,Передачи',\n",
    "        'Фантастические': 'Фантастика',\n",
    "        'Фэнтези': 'Фантастика',\n",
    "        'Союзмультфильм': 'Союзмультфильм,Анимация',\n",
    "        'Юмористические': 'Комедии',\n",
    "        'Развлекательные': 'Комедии',\n",
    "        'Вестерн': 'Фильмы,Зарубежные,Боевик',\n",
    "        'Советское кино': 'Советские,Фильмы',        \n",
    "    }\n",
    "    for key in replace_map:\n",
    "        s = str(s).replace(key, replace_map[key])\n",
    "        \n",
    "    return s.lower().split(',')\n",
    "\n",
    "replace_genres_udf = udf(replace_genres, ArrayType(StringType()))\n",
    "\n",
    "#df_items_m2 = df_items_m.select(replace_genres_udf(\"genres\").alias(\"genres\"))\n",
    "df_items = df_items.withColumn(\"genres2\", replace_genres_udf(\"genres\"))\n",
    "\n",
    "#df_items_m2.limit(100).toPandas()\n",
    "\n",
    "#df_items_m2 = df_items_m2.select(split(\"genres\", ',').alias('genres'))\n",
    "#df_items_m3 = df_items_m2.select(explode('genres').alias('genres'))\n",
    "#df_items_m3.groupby('genres').count().sort('genres').show(100)\n",
    "\n",
    "count_vectorizer = CountVectorizer(inputCol='genres2', outputCol=\"genres_vector\", binary=False)\n",
    "count_vectorizer_model = count_vectorizer.fit(df_items)\n",
    "df_items = count_vectorizer_model.transform(df_items)\n",
    "\n",
    "\n",
    "normalizer = Normalizer(inputCol='genres_vector', outputCol=\"genres_norm\")\n",
    "df_items = normalizer.transform(df_items)\n",
    "df_items = df_items.drop('genres2')\n",
    "\n",
    "df_items.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"year_cat_norm\", \"genres_norm\"], outputCol=\"features\")\n",
    "df_items = assembler.transform(df_items)\n",
    "df_items.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items.select(\"item_id\", \"genres\").take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user.select(\"user_id\", \"item_id\", \"purchase\").take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_genres = df_user.filter(df_user.purchase == 1)\\\n",
    "        .join(df_items, df_user.item_id == df_items.item_id, 'inner')\\\n",
    "        .select(df_user.user_id, df_items.genres.alias(\"genres\"))\n",
    "\n",
    "df_user_genres = df_user_genres.withColumn(\"genres2\", replace_genres_udf(\"genres\"))\n",
    "df_user_genres = df_user_genres.select(df_user_genres.user_id, explode(df_user_genres.genres2))\n",
    "df_user_genres = df_user_genres.groupBy('user_id').agg(collect_set('col').alias('genres2'))\n",
    "\n",
    "df_user_genres = count_vectorizer_model.transform(df_user_genres)\n",
    "df_user_genres = normalizer.transform(df_user_genres)\n",
    "df_user_genres = df_user_genres.drop(\"genres2\", \"genres_vector\")\n",
    "\n",
    "# , df_items.year_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_genres.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items.select(\"item_id\", \"year_cat_str\").take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_year = df_user.filter(df_user.purchase == 1)\\\n",
    "        .join(df_items, df_user.item_id == df_items.item_id, 'inner')\\\n",
    "        .select(df_user.user_id, df_items.year_cat_str.alias(\"year_cat_str\"))\n",
    "df_user_year = df_user_year.groupBy('user_id').agg(collect_set('year_cat_str').alias('year_cat'))\n",
    "df_user_year = count_vectorizer_year_model.transform(df_user_year)\n",
    "df_user_year = normalizer_year.transform(df_user_year)\n",
    "df_user_year = df_user_year.drop(\"year_cat\", \"year_cat_vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_year.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_norm = df_user_genres\\\n",
    "        .join(df_user_year, df_user_genres.user_id == df_user_year.user_id, 'inner')\\\n",
    "        .select(df_user_genres.user_id, df_user_year.year_cat_norm, df_user_genres.genres_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_norm.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"year_cat_norm\", \"genres_norm\"], outputCol=\"features\")\n",
    "df_users_features = assembler.transform(df_users_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_features.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# расчет фичи жанра для клиента\n",
    "\n",
    "#df_items_m2.schema\n",
    "# df_items_m2.groupby().sum().limit(10).show()\n",
    "#d = df_items_m2.limit(2).collect()\n",
    "#df_items_m2.groupby().agg()\n",
    "#SparseVector(DenseVector(d[0]['genres_vector']) + DenseVector(d[1]['genres_vector']))\n",
    "\n",
    "@udf(ArrayType(FloatType()))\n",
    "def toDense(v):\n",
    "    return DenseVector(v)\n",
    "    #new_array = list([float(x) for x in v])\n",
    "    #return new_array \n",
    "\n",
    "df_items_m2.withColumn(\"genres_vector2\", toDense(df_items_m2.genres_vector)).groupby().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# собираем набор для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_user\\\n",
    "        .join(df_users_features, df_user.user_id == df_users_features.user_id, 'inner')\\\n",
    "        .join(df_items, df_user.item_id == df_items.item_id, 'inner')\\\n",
    "        .select(df_users_features.features.alias('user_features'),\\\n",
    "               df_items.features.alias('item_features'),\\\n",
    "               df_user.purchase.alias('target'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_user.limit(5).toPandas()\n",
    "#df_items.limit(5).toPandas()\n",
    "df_train.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Считаем косинусы близости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity\n",
    "dot_udf = udf(lambda x,y: float(x.dot(y)), DoubleType())\n",
    "\n",
    "df_train = df_train.select(\\\n",
    "                dot_udf(df_train.user_year_cat_norm, df_train.item_year_cat_norm).alias(\"similarity_year_cat\"),\n",
    "                dot_udf(df_train.user_genres_norm, df_train.item_genres_norm).alias(\"similarity_genres\"),\\\n",
    "               df_train.target)\n",
    "\n",
    "df_train.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assembler = VectorAssembler(inputCols=[\"similarity_year_cat\", \"similarity_genres\"], outputCol=\"features\")\n",
    "assembler = VectorAssembler(inputCols=[\"user_features\", \"item_features\"], outputCol=\"features\")\n",
    "df_train = assembler.transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train.groupby(df_train.similarity_year_cat).count().limit(20).toPandas()\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features', labelCol=\"target\", maxIter=15)\n",
    "lr_model = lr.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Строим набор для тестирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_user_test.count())\n",
    "df_user_test.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_test = df_user_test\\\n",
    "        .join(df_users_features, df_user_test.user_id == df_users_features.user_id, 'inner')\\\n",
    "        .join(df_items, df_user_test.item_id == df_items.item_id, 'inner')\\\n",
    "        .select(df_users_features.features.alias('user_features'),\\\n",
    "               df_items.features.alias('item_features'),\n",
    "               df_user_test.user_id, df_user_test.item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_user_test.count())\n",
    "df_user_test.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_test = df_user_test.select(\\\n",
    "                dot_udf(df_user_test.user_year_cat_norm, df_user_test.item_year_cat_norm).alias(\"similarity_year_cat\"),\n",
    "                dot_udf(df_user_test.user_genres_norm, df_user_test.item_genres_norm).alias(\"similarity_genres\"),\n",
    "                df_user_test.user_id, df_user_test.item_id)\n",
    "\n",
    "df_user_test.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"user_features\", \"item_features\"], outputCol=\"features\")\n",
    "df_user_test = assembler.transform(df_user_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_test.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lr_model.transform(df_user_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.groupby('prediction').count().limit(50).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_test.join(df_users_norm, df_user_test.user_id == df_users_norm.user_id, 'inner')\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = predictions\\\n",
    "        .sort(\"user_id\", \"item_id\")\\\n",
    "        .select(\"user_id\", \"item_id\", predictions.prediction.alias(\"purchase\"))\\\n",
    "        .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_user_test\\\n",
    "        .sort(\"user_id\", \"item_id\")\\\n",
    "        .select(\"user_id\", \"item_id\", lit(0.5).alias('purchase'))\\\n",
    "        .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('lab03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ids = df_items.filter(df_items.year.isNull()).collect()\n",
    "#ids = [x[0] for x in ids]\n",
    "#ids # [103377, 8544, 95141, 72544]\n",
    "#df_items.filter(df_items.item_id.isin(ids)).toPandas()\n",
    "\n",
    "\n",
    "#df_items.filter(df_items.item_id.isin(item_ids)).toPandas()\n",
    "\n",
    "#df_items.filter(df_items.year.isNull()).toPandas()\n",
    "\n",
    "#df_items.groupby(df_items.year).count().toPandas()\n",
    "#df_items.filter(df_items.item_id == 103377).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_items.limit(100).toPandas()\n",
    "# df_items.sort(df_items.item_id.desc()).limit(100).toPandas()\n",
    "# df_items.filter((df_items.item_id.isNull()) | (~df_items.title.isNull())).limit(100).toPandas()\n",
    "# df_items.filter(df_items.title.isNull()).limit(100).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_genres_udf = udf(replace_genres, StringType())\n",
    "\n",
    "#df_items_m = df_items.select('genres')\n",
    "\n",
    "df_items_m2 = df_items_m.select(replace_genres_udf(\"genres\").alias(\"genres\"))\n",
    "\n",
    "df_items_m2 = df_items_m2.select(split(\"genres\", ',').alias('genres'))\n",
    "df_items_m3 = df_items_m2.select(explode('genres').alias('genres'))\n",
    "df_items_m3.groupby('genres').count().sort('genres').show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items.filter(\"datetime_availability_stop == '2018-12-31T00:00:00Z'\").show(5, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for col in ['channel_id', 'content_type', 'genres', 'region_id']:\n",
    "# for col in ['datetime_availability_start','datetime_availability_stop','datetime_show_start','datetime_show_stop']:\n",
    "for col in ['year']:\n",
    "    t = df_items.select(col).distinct()\n",
    "    print(t.count())\n",
    "    t.show(5, False, False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
