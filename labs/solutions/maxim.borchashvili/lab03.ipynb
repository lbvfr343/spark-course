{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В вашем распоряжении имеется уже предобработанный и очищенный датасет с фактами покупок абонентами телепередач от компании E-Contenta. \n",
    "\n",
    "По доступным вам данным, нужно предсказать вероятность покупки других передач этими, а, возможно, и другими абонентами. При решении задачи запрещено использовать библиотеки pandas, sklearn (кроме sklearn.metrics), xgboost и другие. Если scikit-learn (например, но и другие тоже) обернут в классы Transformer и Estimator, то их можно использовать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--conf spark.sql.catalogImplementation=in-memory pyspark-shell'\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 --executor-memory 4g --driver-memory 3g pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType \n",
    "from pyspark.sql.types import ArrayType, DoubleType, BooleanType, FloatType\n",
    "from pyspark.sql.functions import col,array_contains\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "-rw-r--r--   3 hdfs hdfs   91066524 2022-01-06 18:46 /labs/slaba03/laba03_items.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs   29965581 2022-01-06 18:46 /labs/slaba03/laba03_test.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs   74949368 2022-01-06 18:46 /labs/slaba03/laba03_train.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs  871302535 2022-01-06 18:46 /labs/slaba03/laba03_views_programmes.csv\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /labs/slaba03/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  laba03_train.csv \n",
    "\n",
    "\n",
    "* purchase - факты покупки \n",
    "* user_id - id пользователя\n",
    "* item_id - id телепередачи\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType() \\\n",
    "      .add(\"user_id\", IntegerType(), True) \\\n",
    "      .add(\"item_id\", IntegerType(), True) \\\n",
    "      .add(\"purchase\", IntegerType(), True)\n",
    "      \n",
    "df_user = spark.read.format(\"csv\") \\\n",
    "      .option(\"header\", True) \\\n",
    "      .schema(schema) \\\n",
    "      .load(\"/labs/slaba03/laba03_train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+\n",
      "|user_id|item_id|purchase|\n",
      "+-------+-------+--------+\n",
      "|   1654|  74107|       0|\n",
      "|   1654|  89249|       0|\n",
      "|   1654|  99982|       0|\n",
      "|   1654|  89901|       0|\n",
      "|   1654| 100504|       0|\n",
      "+-------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_user.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  laba03_test.csv \n",
    "\n",
    "\n",
    "* purchase - факты покупки \n",
    "* user_id - id пользователя\n",
    "* item_id - id телепередачи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType() \\\n",
    "      .add(\"user_id\", IntegerType(), True) \\\n",
    "      .add(\"item_id\", IntegerType(), True) \n",
    "      \n",
    "      \n",
    "df_user_test = spark.read.format(\"csv\") \\\n",
    "      .option(\"header\", True) \\\n",
    "      .schema(schema) \\\n",
    "      .load(\"/labs/slaba03/laba03_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|user_id|item_id|\n",
      "+-------+-------+\n",
      "|   1654|  94814|\n",
      "|   1654|  93629|\n",
      "|   1654|   9980|\n",
      "|   1654|  95099|\n",
      "|   1654|  11265|\n",
      "+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_user_test.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  laba03_items.csv\n",
    "\n",
    "\n",
    "* item_id - Соответствует item_id в предыдущем файле.\n",
    "* content_type - тип контента\n",
    "* title - название передачи, текстовое поле.\n",
    "* year - год выпуска передачи, число.\n",
    "* genres - поле с жанрами передачи, разделёнными через запятую.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_items_schema = StructType(fields=[StructField('item_id', IntegerType()), \n",
    "StructField('channel_id', IntegerType()),\n",
    "StructField('datetime_availability_start', StringType()),\n",
    "StructField('datetime_availability_stop', StringType()),\n",
    "StructField('datetime_show_start', StringType()),\n",
    "StructField('datetime_show_stop', StringType()),\n",
    "StructField('content_type', IntegerType()),\n",
    "StructField('title', StringType(), nullable=True),\n",
    "StructField('year', FloatType(), nullable=True),\n",
    "StructField('genres', StringType()),\n",
    "StructField('region_id', IntegerType()),\n",
    "]) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_items = spark.read.format(\"csv\") \\\n",
    "      .option(\"header\", True) \\\n",
    "      .option(\"sep\", \"\\t\")\\\n",
    "      .schema(read_items_schema) \\\n",
    "      .load(\"/labs/slaba03/laba03_items.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------------------------+--------------------------+-------------------+------------------+------------+--------------------------------------------------------------------------------------+------+-------+---------+\n",
      "|item_id|channel_id|datetime_availability_start|datetime_availability_stop|datetime_show_start|datetime_show_stop|content_type|title                                                                                 |year  |genres |region_id|\n",
      "+-------+----------+---------------------------+--------------------------+-------------------+------------------+------------+--------------------------------------------------------------------------------------+------+-------+---------+\n",
      "|65667  |null      |1970-01-01T00:00:00Z       |2018-01-01T00:00:00Z      |null               |null              |1           |на пробах только девушки (all girl auditions)                                         |2013.0|Эротика|null     |\n",
      "|65669  |null      |1970-01-01T00:00:00Z       |2018-01-01T00:00:00Z      |null               |null              |1           |скуби ду: эротическая пародия (scooby doo: a xxx parody)                              |2011.0|Эротика|null     |\n",
      "|65668  |null      |1970-01-01T00:00:00Z       |2018-01-01T00:00:00Z      |null               |null              |1           |горячие девочки для горячих девочек (hot babes 4 hot babes)                           |2011.0|Эротика|null     |\n",
      "|65671  |null      |1970-01-01T00:00:00Z       |2018-01-01T00:00:00Z      |null               |null              |1           |соблазнительницы женатых мужчин (top heavy homewreckers)                              |2011.0|Эротика|null     |\n",
      "|65670  |null      |1970-01-01T00:00:00Z       |2018-01-01T00:00:00Z      |null               |null              |1           |секретные секс-материалы ii: темная секс пародия (the sex files ii: a dark xxx parody)|2010.0|Эротика|null     |\n",
      "+-------+----------+---------------------------+--------------------------+-------------------+------------------+------------+--------------------------------------------------------------------------------------+------+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_items.show(5, False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## laba03_views_programmes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_users_schema = StructType(fields=[StructField('user_id', IntegerType()), \n",
    "StructField('item_id', IntegerType()),\n",
    "StructField('ts_start', IntegerType()),\n",
    "StructField('ts_end', IntegerType()),\n",
    "StructField('item_type', StringType()),\n",
    "]) \n",
    "\n",
    "\n",
    "\n",
    "df_views_programmes = spark.read.format(\"csv\") \\\n",
    "      .option(\"header\", True) \\\n",
    "      .schema(read_users_schema) \\\n",
    "      .load(\"/labs/slaba03/laba03_views_programmes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+----------+---------+\n",
      "|user_id|item_id|ts_start  |ts_end    |item_type|\n",
      "+-------+-------+----------+----------+---------+\n",
      "|0      |7101053|1491409931|1491411600|live     |\n",
      "|0      |7101054|1491412481|1491451571|live     |\n",
      "|0      |7101054|1491411640|1491412481|live     |\n",
      "|0      |6184414|1486191290|1486191640|live     |\n",
      "|257    |4436877|1490628499|1490630256|live     |\n",
      "+-------+-------+----------+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_views_programmes.show(5, False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"purchase\", metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.3 ms, sys: 0 ns, total: 10.3 ms\n",
      "Wall time: 18.3 s\n"
     ]
    }
   ],
   "source": [
    "# Fit ALS on the training data\n",
    "als = ALS(maxIter=20, regParam=2.2, rank=6, coldStartStrategy=\"nan\", \\\n",
    "          userCol='user_id', itemCol='item_id', ratingCol='purchase', \\\n",
    "          nonnegative=False, implicitPrefs=True, alpha=5.0, seed=87)\n",
    "%time als_model = als.fit(df_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-------------+\n",
      "|user_id|item_id|purchase|   prediction|\n",
      "+-------+-------+--------+-------------+\n",
      "| 754230|   8389|       0|  0.064516366|\n",
      "| 780033|   8389|       0| 0.0013112826|\n",
      "| 798454|   8389|       0|-2.7597987E-4|\n",
      "| 825061|   8389|       0| -0.007014811|\n",
      "| 833685|   8389|       0|   0.13137318|\n",
      "+-------+-------+--------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 0 ns, sys: 4.61 ms, total: 4.61 ms\n",
      "Wall time: 31.7 s\n"
     ]
    }
   ],
   "source": [
    "predict_train = als_model.transform(df_user)\n",
    "%time predict_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_train.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_train = predict_train.coalesce(4).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- purchase: integer (nullable = true)\n",
      " |-- prediction: float (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- purchase: integer (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n",
      "CPU times: user 298 µs, sys: 71 µs, total: 369 µs\n",
      "Wall time: 419 µs\n"
     ]
    }
   ],
   "source": [
    "predict_train = predict_train.withColumn(\"prediction\", predict_train.prediction.cast(DoubleType()))\n",
    "%time predict_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+--------------------+--------------------+\n",
      "|summary|          user_id|           item_id|            purchase|          prediction|\n",
      "+-------+-----------------+------------------+--------------------+--------------------+\n",
      "|  count|          5032624|           5032624|             5032624|             5032624|\n",
      "|   mean|869680.9464782189| 66869.30485865823|0.002166662957534...|0.005121428529973152|\n",
      "| stddev|60601.09821562932|35242.282055382544|0.046496977952915616|0.019777955581597964|\n",
      "|    min|             1654|               326|                   0|-0.21212543547153473|\n",
      "|    25%|           846231|             60351|                   0|-2.30404548346996...|\n",
      "|    50%|           885247|             79853|                   0|                 0.0|\n",
      "|    75%|           908726|             93602|                   0|0.003308809362351...|\n",
      "|    max|           941450|            104165|                   1| 0.45947134494781494|\n",
      "+-------+-----------------+------------------+--------------------+--------------------+\n",
      "\n",
      "CPU times: user 4.38 ms, sys: 3.92 ms, total: 8.31 ms\n",
      "Wall time: 43.9 s\n"
     ]
    }
   ],
   "source": [
    "%time predict_train.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.38 ms, sys: 4.03 ms, total: 5.41 ms\n",
      "Wall time: 12.8 s\n",
      "ROC AUC for train data: 0.9685476387336736\n"
     ]
    }
   ],
   "source": [
    "# check roc_auc on the train set\n",
    "%time rocauc_train = evaluator.evaluate(predict_train)\n",
    "print(f'ROC AUC for train data: {rocauc_train}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------+\n",
      "|user_id|item_id|   prediction|\n",
      "+-------+-------+-------------+\n",
      "| 822709|   8389|3.1188553E-19|\n",
      "| 824008|   8389|-0.0017191223|\n",
      "| 890476|   8389|          0.0|\n",
      "| 899993|   8389|  8.513293E-4|\n",
      "| 937345|   8389|  0.032060243|\n",
      "+-------+-------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 1.66 ms, sys: 395 µs, total: 2.06 ms\n",
      "Wall time: 13.4 s\n"
     ]
    }
   ],
   "source": [
    "# predict test data\n",
    "predict_test = als_model.transform(df_user_test)\n",
    "%time predict_test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_test.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test = predict_test.coalesce(4).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------+--------------------+\n",
      "|summary|          user_id|          item_id|          prediction|\n",
      "+-------+-----------------+-----------------+--------------------+\n",
      "|  count|          2156840|          2156840|             2156840|\n",
      "|   mean|869652.3733920922|66896.00283609354|0.005018660489555...|\n",
      "| stddev|60706.51616333836|35227.83130704636| 0.01914327488526151|\n",
      "|    min|             1654|              326|         -0.20062923|\n",
      "|    25%|           846231|            65667|       -2.3222984E-4|\n",
      "|    50%|           885247|            79856|                 0.0|\n",
      "|    75%|           908588|            93606|        0.0033056643|\n",
      "|    max|           941450|           104165|          0.44166976|\n",
      "+-------+-----------------+-----------------+--------------------+\n",
      "\n",
      "CPU times: user 4.1 ms, sys: 0 ns, total: 4.1 ms\n",
      "Wall time: 17.1 s\n"
     ]
    }
   ],
   "source": [
    "%time predict_test.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------+\n",
      "|user_id|item_id|    purchase|\n",
      "+-------+-------+------------+\n",
      "|   1654|    336|         0.0|\n",
      "|   1654|    678|         0.0|\n",
      "|   1654|    691|         0.0|\n",
      "|   1654|    696|1.7609971E-4|\n",
      "|   1654|    763|0.0017800244|\n",
      "+-------+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = predict_test.select('user_id', 'item_id', col('prediction').alias('purchase')) \\\n",
    "                     .orderBy(['user_id', 'item_id'])\n",
    "output.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.toPandas().to_csv('lab03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
