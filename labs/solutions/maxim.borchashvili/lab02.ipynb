{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 --executor-memory 3g --driver-memory 2g pyspark-shell'\n",
    " \n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"Lab02 tf-idf\") \n",
    "conf.set(\"spark.sql.crossJoin.enabled\", \"True\") \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).appName(\"Lab02 tf-idf\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master-5.newprolab.com:4049\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Lab02 tf-idf</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f76d7519438>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Читаем данные\n",
    "data = spark.read.json('/labs/slaba02/DO_record_per_line.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проведем токенизацию текста\n",
    "def tokenization(string):\n",
    "    regex = re.compile(u'[\\w\\d]{2,}', re.U)\n",
    "    return regex.findall(string.lower())\n",
    "tokenization = f.udf(tokenization, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn(\"desc_tokenization\", tokenization(data.desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сдедалаем TF-IDF по поулченным векторам слов\n",
    "ht = HashingTF(numFeatures=10000, inputCol=\"desc_tokenization\", outputCol=\"tf_features\")\n",
    "tf = ht.transform(data)\n",
    "idf = IDF(inputCol=\"tf_features\", outputCol=\"tfidf_features\").fit(tf)\n",
    "tfidf = idf.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Здесь указан мой вариант строчек\n",
    "courses_to_make_recommendations = \\\n",
    "[[23126, u'en', u'Compass - powerful SASS library that makes your life easier'], \n",
    " [21617, u'en', u'Preparing for the AP* Computer Science A Exam \\u2014 Part 2'], \n",
    " [16627, u'es', u'Aprende Excel: Nivel Intermedio by Alfonso Rinsche'], \n",
    " [11556, u'es', u'Aprendizaje Colaborativo by UNID Universidad Interamericana para el Desarrollo'], \n",
    " [16704, u'ru', u'\\u041f\\u0440\\u043e\\u0433\\u0440\\u0430\\u043c\\u043c\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u043d\\u0430 Lazarus'], [13702, u'ru', u'\\u041c\\u0430\\u0442\\u0435\\u043c\\u0430\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u0430\\u044f \\u044d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u043a\\u0430']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция косинусного расстояния\n",
    "@f.udf(returnType=DoubleType())\n",
    "def sim_cos(v1,v2):\n",
    "    try:\n",
    "        p = 2\n",
    "        return float(v1.dot(v2))/float(v1.norm(p)*v2.norm(p))\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{23126: [14760, 13665, 13782, 20638, 24419, 15909, 2724, 25782, 17499, 13348], 21617: [21609, 21616, 21608, 22298, 21630, 21628, 21623, 21508, 21081, 19417], 16627: [11431, 11575, 12247, 17964, 5687, 17961, 16694, 12660, 25010, 5558], 11556: [16488, 468, 13461, 23357, 19330, 7833, 9289, 10447, 22710, 11340], 16704: [1236, 1247, 1365, 1273, 20288, 1164, 8186, 1233, 8203, 875], 13702: [864, 21079, 8313, 1041, 28074, 8300, 1033, 13057, 21025, 1111]}\n"
     ]
    }
   ],
   "source": [
    "# Здесь расчитываем топ10 по косинусному расстоянию с каждой из строчек выше и записываем в словарь\n",
    "result = {}\n",
    "for j in courses_to_make_recommendations:\n",
    "    tfidf2 = tfidf[(tfidf.id != j[0])&(tfidf.lang == j[1])]\n",
    "    tfidf_j = tfidf[tfidf.id == j[0]][['tfidf_features']]\n",
    "    \n",
    "    tfidf_final = tfidf2.join(tfidf_j.withColumnRenamed('tfidf_features', 'tfidf_features_2'))\n",
    "    tfidf_final = tfidf_final.withColumn('sim_cos', sim_cos(tfidf_final.tfidf_features, tfidf_final.tfidf_features_2))\n",
    "    tfidf_final = tfidf_final.orderBy(tfidf_final.sim_cos.desc(), tfidf_final.name, tfidf_final.id).limit(10)[['id']].toPandas()\n",
    "    result[j[0]] = list(tfidf_final['id'])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('lab02.json', 'w') as f:\n",
    "    json.dump(result, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
