{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 2 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import Row\n",
    "import json\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .config(conf=conf)\n",
    "         .appName(\"test\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master-5.newprolab.com:4049\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fefc826b630>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '/labs/laba01/ml-100k'\n",
    "estimations_file_name = 'u.data'\n",
    "films_file_name = 'u.item'\n",
    "path_to_estimation_file = os.path.join(data_folder, estimations_file_name)\n",
    "path_to_films_file = os.path.join(data_folder, films_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23 items\r\n",
      "-rw-r--r--   3 hdfs hdfs       6750 2022-01-06 18:46 /labs/laba01/ml-100k/README\r\n",
      "-rw-r--r--   3 hdfs hdfs        716 2022-01-06 18:46 /labs/laba01/ml-100k/allbut.pl\r\n",
      "-rw-r--r--   3 hdfs hdfs        643 2022-01-06 18:46 /labs/laba01/ml-100k/mku.sh\r\n",
      "-rw-r--r--   3 hdfs hdfs    1979173 2022-01-06 18:46 /labs/laba01/ml-100k/u.data\r\n",
      "-rw-r--r--   3 hdfs hdfs        202 2022-01-06 18:46 /labs/laba01/ml-100k/u.genre\r\n",
      "-rw-r--r--   3 hdfs hdfs         36 2022-01-06 18:46 /labs/laba01/ml-100k/u.info\r\n",
      "-rw-r--r--   3 hdfs hdfs     236344 2022-01-06 18:46 /labs/laba01/ml-100k/u.item\r\n",
      "-rw-r--r--   3 hdfs hdfs        193 2022-01-06 18:46 /labs/laba01/ml-100k/u.occupation\r\n",
      "-rw-r--r--   3 hdfs hdfs      22628 2022-01-06 18:46 /labs/laba01/ml-100k/u.user\r\n",
      "-rw-r--r--   3 hdfs hdfs    1586544 2022-01-06 18:46 /labs/laba01/ml-100k/u1.base\r\n",
      "-rw-r--r--   3 hdfs hdfs     392629 2022-01-06 18:46 /labs/laba01/ml-100k/u1.test\r\n",
      "-rw-r--r--   3 hdfs hdfs    1583948 2022-01-06 18:46 /labs/laba01/ml-100k/u2.base\r\n",
      "-rw-r--r--   3 hdfs hdfs     395225 2022-01-06 18:46 /labs/laba01/ml-100k/u2.test\r\n",
      "-rw-r--r--   3 hdfs hdfs    1582546 2022-01-06 18:46 /labs/laba01/ml-100k/u3.base\r\n",
      "-rw-r--r--   3 hdfs hdfs     396627 2022-01-06 18:46 /labs/laba01/ml-100k/u3.test\r\n",
      "-rw-r--r--   3 hdfs hdfs    1581878 2022-01-06 18:46 /labs/laba01/ml-100k/u4.base\r\n",
      "-rw-r--r--   3 hdfs hdfs     397295 2022-01-06 18:46 /labs/laba01/ml-100k/u4.test\r\n",
      "-rw-r--r--   3 hdfs hdfs    1581776 2022-01-06 18:46 /labs/laba01/ml-100k/u5.base\r\n",
      "-rw-r--r--   3 hdfs hdfs     397397 2022-01-06 18:46 /labs/laba01/ml-100k/u5.test\r\n",
      "-rw-r--r--   3 hdfs hdfs    1792501 2022-01-06 18:46 /labs/laba01/ml-100k/ua.base\r\n",
      "-rw-r--r--   3 hdfs hdfs     186672 2022-01-06 18:46 /labs/laba01/ml-100k/ua.test\r\n",
      "-rw-r--r--   3 hdfs hdfs    1792476 2022-01-06 18:46 /labs/laba01/ml-100k/ub.base\r\n",
      "-rw-r--r--   3 hdfs hdfs     186697 2022-01-06 18:46 /labs/laba01/ml-100k/ub.test\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /labs/laba01/ml-100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimations = spark.sparkContext.textFile(path_to_estimation_file)\n",
    "films = spark.sparkContext.textFile(path_to_films_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import methodcaller\n",
    "from typing import List\n",
    "\n",
    "def split_raw_data(sentences: List[str], delimeter: str):\n",
    "    return list(map(methodcaller(\"split\", delimeter), sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вид образца датасета: \n",
      " 196\t242\t3\t881250949 \n",
      "\n",
      "Обработанный датасет: \n",
      " [['196', '242', '3', '881250949'], ['186', '302', '3', '891717742'], ['22', '377', '1', '878887116'], ['244', '51', '2', '880606923'], ['166', '346', '1', '886397596'], ['298', '474', '4', '884182806'], ['115', '265', '2', '881171488'], ['253', '465', '5', '891628467'], ['305', '451', '3', '886324817'], ['6', '86', '3', '883603013']] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Структура датасета: user id | item id | rating | timestamp\n",
    "print(\"Вид образца датасета: \\n\", estimations.first(), \"\\n\")\n",
    "print(\"Обработанный датасет: \\n\", split_raw_data(estimations.take(10), \"\\t\"), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вид образца датасета: \n",
      " 1|Toy Story (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0 \n",
      "\n",
      "Обработанный датасет: \n",
      " [['1', 'Toy Story (1995)', '01-Jan-1995', '', 'http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)', '0', '0', '0', '1', '1', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['2', 'GoldenEye (1995)', '01-Jan-1995', '', 'http://us.imdb.com/M/title-exact?GoldenEye%20(1995)', '0', '1', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0'], ['3', 'Four Rooms (1995)', '01-Jan-1995', '', 'http://us.imdb.com/M/title-exact?Four%20Rooms%20(1995)', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0'], ['4', 'Get Shorty (1995)', '01-Jan-1995', '', 'http://us.imdb.com/M/title-exact?Get%20Shorty%20(1995)', '0', '1', '0', '0', '0', '1', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['5', 'Copycat (1995)', '01-Jan-1995', '', 'http://us.imdb.com/M/title-exact?Copycat%20(1995)', '0', '0', '0', '0', '0', '0', '1', '0', '1', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0'], ['6', 'Shanghai Triad (Yao a yao yao dao waipo qiao) (1995)', '01-Jan-1995', '', 'http://us.imdb.com/Title?Yao+a+yao+yao+dao+waipo+qiao+(1995)', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['7', 'Twelve Monkeys (1995)', '01-Jan-1995', '', 'http://us.imdb.com/M/title-exact?Twelve%20Monkeys%20(1995)', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0'], ['8', 'Babe (1995)', '01-Jan-1995', '', 'http://us.imdb.com/M/title-exact?Babe%20(1995)', '0', '0', '0', '0', '1', '1', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['9', 'Dead Man Walking (1995)', '01-Jan-1995', '', 'http://us.imdb.com/M/title-exact?Dead%20Man%20Walking%20(1995)', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['10', 'Richard III (1995)', '22-Jan-1996', '', 'http://us.imdb.com/M/title-exact?Richard%20III%20(1995)', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0']] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Структура датасета: movie id | movie title | release date | video release date |\n",
    "#               IMDb URL | unknown | Action | Adventure | Animation |\n",
    "#               Children's | Comedy | Crime | Documentary | Drama | Fantasy |\n",
    "#               Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi |\n",
    "#               Thriller | War | Western |\n",
    "print(\"Вид образца датасета: \\n\", films.first(), \"\\n\")\n",
    "print(\"Обработанный датасет: \\n\", split_raw_data(films.take(10), \"|\"), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Персонализация\n",
    "test_film_id = '328'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_film = estimations.map(\n",
    "    methodcaller(\"split\", \"\\t\")\n",
    ").map(\n",
    "    lambda n: ((n[1], n[2]), 1)  # [(('film_id', 'estim'), 1),]\n",
    ").reduceByKey(\n",
    "    lambda estim1, estim2: estim1 + estim2  # [(('film_id', 'estim'), count),]\n",
    ").map(\n",
    "    lambda n: (n[0][0], (n[0][1], n[1]))  # [(film_id', ('estim', count)),]\n",
    ").filter(\n",
    "    lambda n: n[0] == test_film_id  # [('film_id', ('estim', count)),] where film_id == test_film_id\n",
    ").map(\n",
    "    lambda n: n[1]  # [('estim', count),]\n",
    ").sortBy(\n",
    "    lambda n: n[0]  # by 'estim' asc\n",
    ").map(\n",
    "    lambda n: n[1]  # only count\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "hist_all = list(map(\n",
    "    itemgetter(1),\n",
    "    sorted(\n",
    "        estimations.map(\n",
    "            methodcaller(\"split\", \"\\t\")\n",
    "    ).map(\n",
    "        lambda n: (n[2], 1)  # [(estim', 1),]\n",
    "    ).countByKey().items(), key=lambda n: n[0], reverse=False)))\n",
    "\n",
    "# Или так\n",
    "hist_all = estimations.map(\n",
    "    methodcaller(\"split\", \"\\t\")\n",
    ").map(\n",
    "    lambda n: (n[2], 1)  # [(('film_id', 'estim'), 1),]\n",
    ").reduceByKey(\n",
    "    lambda estim1, estim2: estim1 + estim2  # [(estim', count),]\n",
    ").sortBy(\n",
    "    lambda n: n[0]  # by 'estim' asc\n",
    ").map(\n",
    "    lambda n: n[1]  # only count\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\"hist_film\": hist_film, \"hist_all\": hist_all}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"lab01.json\", \"w\") as outfile:\n",
    "    json.dump(result, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
