{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 --executor-memory 3g --driver-memory 2g pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DoubleType, StructType, StructField, IntegerType, StringType\n",
    "\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())\n",
    "\n",
    "\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(fields = [\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"lang\", StringType()),\n",
    "        StructField(\"desc\", StringType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read\\\n",
    "          .format(\"json\")\\\n",
    "          .option(\"sep\", \",\")\\\n",
    "          .load(\"/labs/slaba02/DO_record_per_line.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.show(2)\n",
    "#df.select(F.translate('name', 'Aer', '$!').alias('name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "df2=df.select(f.lower(f.translate(\"desc\",',.-!?:;№#@%&*=+$0123456789\"()','')).alias('desc'),\"id\",\"lang\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df2.select(f.translate(\"desc\",\"'\",\"\").alias('desc'),\"id\",\"lang\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.select(\"desc\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF\n",
    "#tokenizer = Tokenizer(inputCol=\"comment_text\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"desc\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#desc_prob.select(\"desc\",f.split(\"desc\", \" \").alias(\"word_list\")).show(5, False, True)\n",
    "#log.select(\"ua\", f.split(\"ua\", \" \").alias(\"word_list\")).show(5, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = tokenizer.transform(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4=dataset2.select( \"id\", \"words\",\"lang\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF\n",
    "ht = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "result = ht.transform(df4)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "idf=IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(result)\n",
    "rescaledData = idfModel.transform(result)\n",
    "rescaledData.select(\"id\", \"features\",\"lang\").show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaledData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5=rescaledData.select(\"id\",\"lang\",\"rawFeatures\",\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "normalizer = Normalizer(inputCol='features', outputCol='norm')\n",
    "df6 = normalizer.transform(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.where(\"id in (23126,21617, 16627, 16704)\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.where(f.col('lang') == 'ru').alias('j').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df6.where(\"id in (21617, 16627, 16704, 23126, 13702, 11556)\").select(f.col(\"id\").alias(\"id2\"),\"lang\",f.col(\"norm\").alias(\"norm2\")).cache() #,23126,21617, 16627, 16704)}, seed=42).cache()\n",
    "test = train.join(df6, on=\"lang\", how=\"inner\").coalesce(10).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.crossJoin.enabled', True)\n",
    "#21617, 16627, 16704, 23126, 13702, 11556"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_udf = f.udf(lambda x, y: float(x.dot(y)), DoubleType())\n",
    "itog=test.select(\"id\",\"id2\", \"norm\",\"norm2\",dot_udf('norm', 'norm2').alias('dot')).sort(f.desc('dot')).where(f\"id<>id2 and id2=11556\").limit(10)\n",
    "otv=itog.select(\"id2\",\"id\",\"dot\")\n",
    "otv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iddd in [13702, 16627, 16704, 21617, 23126]:\n",
    "    dot_udf = f.udf(lambda x, y: float(x.dot(y)), DoubleType())\n",
    "    itog=test.select(\"id\",\"id2\", \"norm\",\"norm2\",dot_udf('norm', 'norm2').alias('dot')).sort(f.desc('dot')).where(f\"id<>id2 and id2={iddd}\").limit(10)\n",
    "    otv=otv.union(itog.select(\"id2\",\"id\",\"dot\"))\n",
    "       # \\\n",
    "        #.select('id','id2' 'dot')\\\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "otv2=otv.select(\"id2\", \"id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_df = otv2.select(\"id\").where(\"id2=11556\").toPandas()\n",
    "end_11556=collected_df['id'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_df = otv2.select(\"id\").where(\"id2=13702\").toPandas()\n",
    "end_13702=collected_df['id'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_df = otv2.select(\"id\").where(\"id2=16627\").toPandas()\n",
    "end_16627=collected_df['id'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_df = otv2.select(\"id\").where(\"id2=16704\").toPandas()\n",
    "end_16704=collected_df['id'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_df = otv2.select(\"id\").where(\"id2=21617\").toPandas()\n",
    "end_21617=collected_df['id'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_df = otv2.select(\"id\").where(\"id2=23126\").toPandas()\n",
    "end_23126=collected_df['id'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end= dict({'21617' : end_21617 , '23126' : end_23126 , '16627' : end_16627 , '11556' : end_11556, '16704' : end_16704, '13702' : end_13702  }) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "collected_df = end\n",
    "with open('lab02.json', 'w') as outfile:\n",
    "    json.dump(collected_df , outfile , indent=1)\n",
    "# itog1.write.json(\"2лаб\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1= sc.parallelize(otv.map(lambda x: x[2]).countByKey().items()).collect()\n",
    "z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lab02.json', 'w') as f:\n",
    "    json.dump(otv2 , f, indent=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "otv.write.format('json').save('lab02.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df5.show(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
