{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import Row\n",
    "import json\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .config(conf=conf)\n",
    "         .appName(\"test\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master-4.newprolab.com:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f7a44827588>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, asc, desc, lower #lit, #array\n",
    "from pyspark.ml.linalg import DenseVector, SparseVector\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, Normalizer, StopWordsRemover\n",
    "import pyspark.sql.functions as psf\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #регулярные выражения\n",
    "import tqdm # прогресс-бар"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка датасета и его репартицирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_films = [[23126, u'en', u'Compass - powerful SASS library that makes your life easier'], [21617, u'en', u'Preparing for the AP* Computer Science A Exam \\u2014 Part 2'], [16627, u'es', u'Aprende Excel: Nivel Intermedio by Alfonso Rinsche'], [11556, u'es', u'Aprendizaje Colaborativo by UNID Universidad Interamericana para el Desarrollo'], [16704, u'ru', u'\\u041f\\u0440\\u043e\\u0433\\u0440\\u0430\\u043c\\u043c\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u043d\\u0430 Lazarus'], [13702, u'ru', u'\\u041c\\u0430\\u0442\\u0435\\u043c\\u0430\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u0430\\u044f \\u044d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u043a\\u0430']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"/labs/slaba02/DO_record_per_line.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.repartition(6)\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оставим только нужные столбцы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+----+--------------------+--------+\n",
      "|                 cat|                desc|   id|lang|                name|provider|\n",
      "+--------------------+--------------------+-----+----+--------------------+--------+\n",
      "|3/business_manage...| Unique video ske...|10209|  en|Learn How To Writ...|   Udemy|\n",
      "|   1/arts_music_film|Explores such ind...|15686|  en|Photoshop CS5 One...|   Lynda|\n",
      "|  5/computer_science|Learn the basic c...|11858|  en|Human Anatomy and...|   ed2go|\n",
      "|   1/arts_music_film|How to streamline...|15821|  en|Premiere Pro CS4 ...|   Lynda|\n",
      "|                    |\n",
      "Hola a Todos!\n",
      "En...|10037|  es|Introduccion Visu...|   Udemy|\n",
      "|   1/arts_music_film|\n",
      "En este curso ve...|11682|  es|                Nuke|   Udemy|\n",
      "|   1/arts_music_film|A tour of editing...|13931|  en|After Effects App...|   Lynda|\n",
      "|3/business_manage...|The basics: using...|15210|  en|Learning Mac OS X...|   Lynda|\n",
      "|                    | A Comprehensive ...| 9909|  en|Video Optimizatio...|   Udemy|\n",
      "|   1/arts_music_film|Introduces the co...|14800|  en|Foundations of Vi...|   Lynda|\n",
      "|3/business_manage...| Understand the P...|13655|  en|Project Managemen...|   Udemy|\n",
      "|                    | Discover a syste...|10356|  en|Making Money Onli...|   Udemy|\n",
      "|3/business_manage...|\n",
      "Do you have a gr...|13723|  en|Creating Your Onl...|   Udemy|\n",
      "|   1/arts_music_film|\n",
      "Nothing channels...|13236|  en|How to Build a Go...|   Udemy|\n",
      "|                    | Grow your busine...|19965|  en|Smart Marketing f...|   Udemy|\n",
      "|                    | Gain a solid und...|19387|  en|Beginning Scala P...|   Udemy|\n",
      "|                    | Learn how one un...|19034|  en|The Unschooling E...|   Udemy|\n",
      "|   1/arts_music_film|All the details o...|14670|  en|Final Cut Pro 6 E...|   Lynda|\n",
      "|                    |Learn the secrets...|17747|  en|Create iPhone and...|   Udemy|\n",
      "|  14/social_sciences|Learn how incorpo...|19737|  en|Gamification of L...|   Lynda|\n",
      "+--------------------+--------------------+-----+----+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_desc_lang = df.select([c for c in df.columns if c in ['id','desc', 'lang']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чистка текста с помощью pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_en = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "stop_words_ru = StopWordsRemover.loadDefaultStopWords(\"russian\")\n",
    "stop_words_es = StopWordsRemover.loadDefaultStopWords(\"spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(data, column):\n",
    "    data=data.withColumn(column, lower(col('desc')))\n",
    "    data = data.withColumn(column, F.regexp_replace('desc', '[!@\"“’«»#$%&\\'()*+,—/:;<=>?^_`{|}~\\[\\]]', ''))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_desc_lang_clean = clean_text(id_desc_lang, 'desc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разделим датасет по языкам фильмов, для которых нужно дать рекомендацию, то есть на en и ru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_data = id_desc_lang_clean.filter(id_desc_lang_clean.lang == 'ru').select('id', 'desc')\n",
    "en_data = id_desc_lang_clean.filter(id_desc_lang_clean.lang == 'en').select('id', 'desc')\n",
    "es_data = id_desc_lang_clean.filter(id_desc_lang_clean.lang == 'es').select('id', 'desc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1231"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24553"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1374"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расчет количества уникальных слов в каждом датасете(это пригодиться при выборе кол-ва фичей для TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#чтобы понять ноебходимо повторить bagofwords tf-idf \n",
    "list_ru = ru_data.select('desc').collect()\n",
    "list_en = en_data.select('desc').collect()\n",
    "list_es = es_data.select('desc').collect()\n",
    "lst_full_language = [list_ru, list_en, list_es]\n",
    "#mvv_array = [int(row.mvv) for row in mvv_list.collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20335\n",
      "347337\n",
      "57222\n"
     ]
    }
   ],
   "source": [
    "#расчет количество фичей в зависимости от уникальных слов в дата-сете\n",
    "for lst in lst_full_language:\n",
    "    set_lang = set()\n",
    "    for i in lst:\n",
    "        for j in i[0].split():\n",
    "            set_lang.add(j)\n",
    "    print(len(set_lang))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ru = 20335\n",
    "\n",
    "En = 347337\n",
    "\n",
    "Es = 57222"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построение Pipeline(выход предыдущего шага является входом для следующего)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline for english films"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#разбиваем текст на токены\n",
    "tokenizer = Tokenizer(inputCol=\"desc\", outputCol=\"words\")\n",
    "\n",
    "swr_en = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"words_filtered_en\", stopWords=stop_words_en)\n",
    "\n",
    "hasher = HashingTF(numFeatures=347337, binary=False, inputCol=swr_en.getOutputCol(), outputCol=\"word_vector\")\n",
    "\n",
    "idf = IDF(inputCol=\"word_vector\", outputCol=\"features\")\n",
    "\n",
    "pipeline_en = Pipeline(stages=[\n",
    "    tokenizer,\n",
    "    swr_en,\n",
    "    hasher,\n",
    "    idf\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model_en = pipeline_en.fit(en_data)\n",
    "tr_df_en = pipeline_model_en.transform(en_data).select('id', 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|   id|            features|\n",
      "+-----+--------------------+\n",
      "|22604|(347337,[914,2479...|\n",
      "|25900|(347337,[984,1659...|\n",
      "|25089|(347337,[914,2760...|\n",
      "|27393|(347337,[2626,334...|\n",
      "|24400|(347337,[83,2760,...|\n",
      "|22865|(347337,[1036,244...|\n",
      "|27686|(347337,[43543,52...|\n",
      "|23287|(347337,[914,2368...|\n",
      "|26899|(347337,[31072,62...|\n",
      "|20791|(347337,[10092,10...|\n",
      "|26985|(347337,[16327,27...|\n",
      "|23839|(347337,[1587,256...|\n",
      "|24369|(347337,[984,1637...|\n",
      "|27131|(347337,[1927,534...|\n",
      "|24432|(347337,[2479,276...|\n",
      "|22624|(347337,[2479,276...|\n",
      "|26787|(347337,[630,4291...|\n",
      "|22119|(347337,[43142,47...|\n",
      "|24041|(347337,[973,5066...|\n",
      "|27910|(347337,[9656,163...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tr_df_en.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline for russian films"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"desc\", outputCol=\"words\")\n",
    "\n",
    "swr_en = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"words_filtered_en\", stopWords=stop_words_ru)\n",
    "\n",
    "hasher = HashingTF(numFeatures=20335, binary=False, inputCol=swr_en.getOutputCol(), outputCol=\"word_vector\")\n",
    "\n",
    "idf = IDF(inputCol=\"word_vector\", outputCol=\"features\")\n",
    "\n",
    "pipeline_ru = Pipeline(stages=[\n",
    "    tokenizer,\n",
    "    swr_en,\n",
    "    hasher,\n",
    "    idf\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model_ru = pipeline_ru.fit(ru_data)\n",
    "tr_df_ru = pipeline_model_ru.transform(ru_data).select('id', 'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline for spanish films"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"desc\", outputCol=\"words\")\n",
    "\n",
    "swr_en = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"words_filtered_en\", stopWords=stop_words_es)\n",
    "\n",
    "hasher = HashingTF(numFeatures=57222, binary=False, inputCol=swr_en.getOutputCol(), outputCol=\"word_vector\")\n",
    "\n",
    "idf = IDF(inputCol=\"word_vector\", outputCol=\"features\")\n",
    "\n",
    "pipeline_es = Pipeline(stages=[\n",
    "    tokenizer,\n",
    "    swr_en,\n",
    "    hasher,\n",
    "    idf\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model_es = pipeline_es.fit(es_data)\n",
    "tr_df_es = pipeline_model_es.transform(es_data).select('id', 'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расчет косинусного расстояния"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(v,u):\n",
    "    return float(v.dot(u) / (v.norm(2) * u.norm(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обрабатывается рекомендаций: 1\n",
      "Обрабатывается рекомендаций: 2\n",
      "Обрабатывается рекомендаций: 3\n",
      "Обрабатывается рекомендаций: 4\n",
      "Обрабатывается рекомендаций: 5\n",
      "Обрабатывается рекомендаций: 6\n"
     ]
    }
   ],
   "source": [
    "film_recomendation = {}\n",
    "count = 0\n",
    "for i in my_films:\n",
    "    count += 1\n",
    "    print(\"Обрабатывается рекомендаций:\", count)\n",
    "    if i[1] == 'ru':\n",
    "        df_lang = tr_df_ru\n",
    "    elif i[1] == 'en':\n",
    "        df_lang = tr_df_en\n",
    "    elif i[1] == 'es':\n",
    "        df_lang = tr_df_es\n",
    "    lst_id = []\n",
    "    lst_cosine = []\n",
    "    f_vec = df_lang.filter(df_lang.id == i[0]).collect()[0]['features']\n",
    "    for itertator in df_lang.filter(df_lang.id != i[0]).collect():\n",
    "        lst_id.append(itertator['id'])\n",
    "        lst_cosine.append(cos_sim(itertator['features'], f_vec))\n",
    "    res = sqlContext.createDataFrame(zip(lst_id, lst_cosine), schema=['id', 'cos'])\n",
    "    res = res.repartition(6)\n",
    "    res = res.dropna()\n",
    "    res = res.sort(\"cos\", ascending=False).collect()[0:10]\n",
    "    lst = []\n",
    "    for j in res:\n",
    "        lst.append(j[0])\n",
    "    film_recomendation[i[0]] = lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[23126, 'en', 'Compass - powerful SASS library that makes your life easier'],\n",
       " [21617, 'en', 'Preparing for the AP* Computer Science A Exam — Part 2'],\n",
       " [16627, 'es', 'Aprende Excel: Nivel Intermedio by Alfonso Rinsche'],\n",
       " [11556,\n",
       "  'es',\n",
       "  'Aprendizaje Colaborativo by UNID Universidad Interamericana para el Desarrollo'],\n",
       " [16704, 'ru', 'Программирование на Lazarus'],\n",
       " [13702, 'ru', 'Математическая экономика']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_films"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{23126: [13665, 13782, 15909, 25782, 14760, 13348, 19270, 17499, 25071, 7153],\n",
       " 21617: [21609, 21608, 21616, 21492, 21703, 21675, 21506, 21624, 21623, 21630],\n",
       " 16627: [11431, 12247, 5687, 17964, 12660, 16694, 5558, 9563, 10738, 13529],\n",
       " 11556: [16488, 13461, 22710, 468, 10447, 23357, 11523, 19330, 12679, 9289],\n",
       " 16704: [1228, 1327, 20362, 13696, 1215, 1365, 26980, 1236, 8186, 875],\n",
       " 13702: [864, 21079, 1052, 8123, 1396, 1041, 1033, 13057, 1217, 8313]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "film_recomendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сохранение результата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lab02.json', 'w') as outfile:\n",
    "    json.dump(film_recomendation, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
