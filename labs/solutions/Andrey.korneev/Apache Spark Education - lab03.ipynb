{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 9 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"korneev\") \n",
    "\n",
    "conf.set('spark.sql.autoBroadcastJoinThreshold', -1)\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StringType, StructField, IntegerType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_fields = [StructField(\"user_id\", IntegerType()),\n",
    "               StructField(\"item_id\", IntegerType()),\n",
    "               StructField(\"purchase\", DoubleType())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(list_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = spark.read.csv('/labs/slaba03/laba03_train.csv', \n",
    "                          header=True, \n",
    "                          schema=schema)\n",
    "test = spark.read.csv('/labs/slaba03/laba03_test.csv', \n",
    "                          header=True, \n",
    "                          schema=schema)\n",
    "df_views = spark.read.csv('/labs/slaba03/laba03_views_programmes.csv', \n",
    "                          header=True)\n",
    "df_items = spark.read.csv('/labs/slaba03/laba03_items.csv', \n",
    "                          header=True, \n",
    "                          sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+\n",
      "|user_id|item_id|purchase|\n",
      "+-------+-------+--------+\n",
      "|   1654|  74107|     0.0|\n",
      "|   1654|  89249|     0.0|\n",
      "|   1654|  99982|     0.0|\n",
      "|   1654|  89901|     0.0|\n",
      "|   1654| 100504|     0.0|\n",
      "+-------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+\n",
      "|user_id|item_id|purchase|\n",
      "+-------+-------+--------+\n",
      "|   1654|  94814|    null|\n",
      "|   1654|  93629|    null|\n",
      "|   1654|   9980|    null|\n",
      "|   1654|  95099|    null|\n",
      "|   1654|  11265|    null|\n",
      "+-------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+----------+---------+\n",
      "|user_id|item_id|  ts_start|    ts_end|item_type|\n",
      "+-------+-------+----------+----------+---------+\n",
      "|      0|7101053|1491409931|1491411600|     live|\n",
      "|      0|7101054|1491412481|1491451571|     live|\n",
      "|      0|7101054|1491411640|1491412481|     live|\n",
      "|      0|6184414|1486191290|1486191640|     live|\n",
      "|    257|4436877|1490628499|1490630256|     live|\n",
      "+-------+-------+----------+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_views.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------------------------------------\n",
      " item_id                     | 65667                                         \n",
      " channel_id                  | null                                          \n",
      " datetime_availability_start | 1970-01-01T00:00:00Z                          \n",
      " datetime_availability_stop  | 2018-01-01T00:00:00Z                          \n",
      " datetime_show_start         | null                                          \n",
      " datetime_show_stop          | null                                          \n",
      " content_type                | 1                                             \n",
      " title                       | на пробах только девушки (all girl auditions) \n",
      " year                        | 2013.0                                        \n",
      " genres                      | Эротика                                       \n",
      " region_id                   | null                                          \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_items.show(1, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spplit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_train.sampleBy(\"purchase\",\n",
    "                          fractions={0: 0.8, 1: 0.8},\n",
    "                          seed=42).cache()\n",
    "\n",
    "valid = df_train.join(train, \n",
    "                      on=[\"user_id\", \"item_id\"],\n",
    "                      how=\"leftanti\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(purchase=0.0, count=5021720), Row(purchase=1.0, count=10904)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target\n",
    "df_train.groupBy(\"purchase\").count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+\n",
      "|user_purchase|user_id|\n",
      "+-------------+-------+\n",
      "|         53.0| 754230|\n",
      "|          1.0| 761341|\n",
      "+-------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#покупки каждого юзера\n",
    "user_purchase = train.groupBy('user_id')\\\n",
    "                        .sum()\\\n",
    "                        .select(f.col(\"sum(purchase)\").alias(\"user_purchase\"),\n",
    "                                f.col(\"user_id\")).cache()\n",
    "user_purchase.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+--------------------+\n",
      "|user_purchase|user_id|   user_purchase_per|\n",
      "+-------------+-------+--------------------+\n",
      "|         53.0| 754230|0.006083562901744...|\n",
      "|          1.0| 761341|1.147842056932966E-4|\n",
      "+-------------+-------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#доля от всех\n",
    "all_purchase = user_purchase.agg(f.sum(\"user_purchase\")).collect()[0][0]\n",
    "user_purchase = user_purchase.withColumn(\"user_purchase_per\",\n",
    "                                         user_purchase[\"user_purchase\"] / all_purchase)\n",
    "user_purchase.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|user_count|user_id|\n",
      "+----------+-------+\n",
      "|      2053| 825061|\n",
      "|      2041| 833685|\n",
      "+----------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#просмотры юзера\n",
    "user_purchase_v = train.groupBy('user_id')\\\n",
    "                        .count()\\\n",
    "                        .select(f.col(\"count\").alias(\"user_count\"),\n",
    "                                f.col(\"user_id\")).cache()\n",
    "user_purchase_v.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+\n",
      "|item_purchase|item_id|\n",
      "+-------------+-------+\n",
      "|          3.0|  90019|\n",
      "|          1.0|  72820|\n",
      "+-------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#покупаемость каждого фильма\n",
    "item_purchase = train.groupBy('item_id')\\\n",
    "                        .sum()\\\n",
    "                        .select(f.col(\"sum(purchase)\").alias(\"item_purchase\"),\n",
    "                                f.col(\"item_id\")).cache()\n",
    "item_purchase.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+--------------------+\n",
      "|item_purchase|item_id|   item_purchase_per|\n",
      "+-------------+-------+--------------------+\n",
      "|          3.0|  90019|3.443526170798898E-4|\n",
      "|          1.0|  72820|1.147842056932966E-4|\n",
      "+-------------+-------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#доля ото всех\n",
    "all_purchase_i = item_purchase.agg(f.sum(\"item_purchase\")).collect()[0][0]\n",
    "item_purchase = item_purchase.withColumn(\"item_purchase_per\",\n",
    "                                         item_purchase[\"item_purchase\"] / all_purchase_i)\n",
    "item_purchase.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[67036, 90762, 8661, 10585, 77442, 8658, 93666, 9919, 9911, 89637]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#topN самых покупаемых\n",
    "#если item_id = top1-topN, ставим True в столбец\n",
    "\n",
    "N = 10\n",
    "list_topN = [row.asDict()['item_id'] for row in \n",
    "             item_purchase.sort(f.desc(\"item_purchase\"))\\\n",
    "             .select('item_id')\\\n",
    "             .head(N)]\n",
    "list_topN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_topN(item, list_topN):\n",
    "    if item in list_topN:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "udf_check_topN = f.udf(lambda x: check_topN(x, list_topN), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_purchase = item_purchase.withColumn(\"item_id_topN\",\n",
    "                                         udf_check_topN(item_purchase[\"item_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+--------------------+------------+\n",
      "|item_purchase|item_id|   item_purchase_per|item_id_topN|\n",
      "+-------------+-------+--------------------+------------+\n",
      "|          3.0|  90019|3.443526170798898E-4|           0|\n",
      "|          1.0|  72820|1.147842056932966E-4|           0|\n",
      "+-------------+-------+--------------------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "item_purchase.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|item_count|item_id|\n",
      "+----------+-------+\n",
      "|      1075|   8638|\n",
      "|      1096|  74757|\n",
      "+----------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#просмотры фильма\n",
    "item_purchase_v = train.groupBy('item_id')\\\n",
    "                        .count()\\\n",
    "                        .select(f.col(\"count\").alias(\"item_count\"),\n",
    "                                f.col(\"item_id\")).cache()\n",
    "item_purchase_v.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#жанры\n",
    "items_genres_years = df_items.select('item_id',\n",
    "                                     'title',\n",
    "                                     'genres',\n",
    "                                     'year')\n",
    "items_genres_years = items_genres_years.na.fill({'title': u\"_\"})\n",
    "items_genres_years = items_genres_years.na.fill({'genres': u\"_\"})\n",
    "items_genres_years = items_genres_years.na.fill({'year': u\"0000\"})\n",
    "items_genres_years = items_genres_years.withColumn('year', \n",
    "                                                   items_genres_years.year.cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, HashingTF, IDF\n",
    "import re\n",
    "def text_regexp_filter(string):\n",
    "    regex = re.compile(u'[\\w\\d]{2,}', re.U)\n",
    "    return \" \".join(regex.findall(string.lower()))\n",
    "udf_text_regexp_filter = f.udf(lambda x: text_regexp_filter(x), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text prep\n",
    "items_genres_years = items_genres_years.withColumn(\"genres_filter\", udf_text_regexp_filter(items_genres_years[\"genres\"]))\n",
    "\n",
    "#tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"genres_filter\", outputCol=\"words\")\n",
    "items_genres_years = tokenizer.transform(items_genres_years)\n",
    "\n",
    "#считаем tf\n",
    "ht = HashingTF(inputCol=\"words\", outputCol=\"features\", numFeatures=300)\n",
    "items_genres_years = ht.transform(items_genres_years)\n",
    "\n",
    "#считаем tfidf\n",
    "idf = IDF(inputCol=\"features\", outputCol=\"genres_tfidf\").fit(items_genres_years)\n",
    "items_genres_years = idf.transform(items_genres_years)\n",
    "\n",
    "\n",
    "\n",
    "#title\n",
    "#text prep\n",
    "items_genres_years = items_genres_years.withColumn(\"title_filter\",\n",
    "                                                   udf_text_regexp_filter(items_genres_years[\"title\"]))\n",
    "\n",
    "#tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"title_filter\", outputCol=\"words_title\")\n",
    "items_genres_years = tokenizer.transform(items_genres_years)\n",
    "\n",
    "#считаем tf\n",
    "ht = HashingTF(inputCol=\"words_title\", outputCol=\"features_title\", numFeatures=300)\n",
    "items_genres_years = ht.transform(items_genres_years)\n",
    "\n",
    "#считаем tfidf\n",
    "idf = IDF(inputCol=\"features_title\", outputCol=\"title_tfidf\").fit(items_genres_years)\n",
    "items_genres_years = idf.transform(items_genres_years)\n",
    "\n",
    "#need cols\n",
    "items_genres_years = items_genres_years.select('item_id', 'year', 'genres_tfidf', 'title_tfidf')\n",
    "\n",
    "\n",
    "\n",
    "#user\n",
    "train = train.join(user_purchase, on='user_id', how='left')\n",
    "valid = valid.join(user_purchase, on='user_id', how='left')\n",
    "test = test.join(user_purchase, on='user_id', how='left')\n",
    "\n",
    "train = train.join(user_purchase_v, on='user_id', how='left')\n",
    "valid = valid.join(user_purchase_v, on='user_id', how='left')\n",
    "test = test.join(user_purchase_v, on='user_id', how='left')\n",
    "\n",
    "#item\n",
    "train = train.join(item_purchase, on='item_id', how='left')\n",
    "valid = valid.join(item_purchase, on='item_id', how='left')\n",
    "test = test.join(item_purchase, on='item_id', how='left')\n",
    "\n",
    "train = train.join(item_purchase_v, on='item_id', how='left')\n",
    "valid = valid.join(item_purchase_v, on='item_id', how='left')\n",
    "test = test.join(item_purchase_v, on='item_id', how='left')\n",
    "\n",
    "#pers\n",
    "train = train.withColumn('user_addict', f.col('user_purchase') / f.col('user_count'))\n",
    "valid = valid.withColumn('user_addict', f.col('user_purchase') / f.col('user_count'))\n",
    "test = test.withColumn('user_addict', f.col('user_purchase') / f.col('user_count'))\n",
    "\n",
    "train = train.withColumn('item_addict', f.col('item_purchase') / f.col('item_count'))\n",
    "valid = valid.withColumn('item_addict', f.col('item_purchase') / f.col('item_count'))\n",
    "test = test.withColumn('item_addict', f.col('item_purchase') / f.col('item_count'))\n",
    "\n",
    "#item genres, years\n",
    "train = train.join(items_genres_years, on='item_id', how='left')\n",
    "valid = valid.join(items_genres_years, on='item_id', how='left')\n",
    "test = test.join(items_genres_years, on='item_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna\n",
    "train = train.na.fill(0)\n",
    "valid = valid.na.fill(0)\n",
    "test = test.na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------+\n",
      "|user_id|item_row_id|purchase|\n",
      "+-------+-----------+--------+\n",
      "| 761341|        124|     0.0|\n",
      "| 780033|        124|     0.0|\n",
      "+-------+-----------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Copy paste\n",
    "# Добавим вектор пользовательской истории\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "items_count = train.groupBy('item_id').count().withColumnRenamed('count', 'item_count')\n",
    "items_desc_count = items_count.orderBy(items_count.item_count.desc()).limit(250) \n",
    "items_desc_count = items_desc_count.coalesce(1)\n",
    "items_desc_count = items_desc_count.withColumn(\"item_row_id\", monotonically_increasing_id())\n",
    "\n",
    "items_desc_count.cache()\n",
    "\n",
    "train_truncated = train.join(items_desc_count, on='item_id', how='inner')\\\n",
    "                    .select('user_id', 'item_row_id', 'purchase').cache()\n",
    "\n",
    "train_truncated.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import CoordinateMatrix, MatrixEntry\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "as_ml = udf(lambda v: v.asML(), VectorUDT())\n",
    "\n",
    "train_matrix = train_truncated.rdd.map(lambda r: MatrixEntry(r[0], r[1], r[2]))\n",
    "train_matrix = CoordinateMatrix(train_matrix)\n",
    "\n",
    "train_row_mat_i = train_matrix.toIndexedRowMatrix()\n",
    "\n",
    "train_mat_df = train_row_mat_i.rows.toDF().withColumnRenamed('index', 'user_id') \\\n",
    "                                    .withColumn(\"history_vec\", as_ml(\"vector\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.join(train_mat_df, 'user_id', 'left')\n",
    "valid = valid.join(train_mat_df, 'user_id', 'left')\n",
    "test = test.join(train_mat_df, 'user_id', 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-------------+-----------------+----------+-------------+--------------------+------------+----------+-----------+--------------------+----+--------------------+--------------------+--------------------+--------------------+\n",
      "|user_id|item_id|purchase|user_purchase|user_purchase_per|user_count|item_purchase|   item_purchase_per|item_id_topN|item_count|user_addict|         item_addict|year|        genres_tfidf|         title_tfidf|              vector|         history_vec|\n",
      "+-------+-------+--------+-------------+-----------------+----------+-------------+--------------------+------------+----------+-----------+--------------------+----+--------------------+--------------------+--------------------+--------------------+\n",
      "| 728960|   8389|     0.0|          0.0|              0.0|      2081|          6.0|6.887052341597796E-4|           0|      1063|        0.0|0.005644402634054563|1981|(300,[222,246,266...|(300,[25,258,284]...|(250,[1,2,3,5,9,1...|(250,[1,2,3,5,9,1...|\n",
      "| 728960|  10817|     0.0|          0.0|              0.0|      2081|          1.0|1.147842056932966E-4|           0|      1074|        0.0| 9.31098696461825E-4|2013|(300,[3,10],[8.71...|(300,[150,179,261...|(250,[1,2,3,5,9,1...|(250,[1,2,3,5,9,1...|\n",
      "| 728960|  72820|     0.0|          0.0|              0.0|      2081|          1.0|1.147842056932966E-4|           0|      1084|        0.0|9.225092250922509E-4|2016|(300,[10,92,141],...|(300,[247],[6.576...|(250,[1,2,3,5,9,1...|(250,[1,2,3,5,9,1...|\n",
      "+-------+-------+--------+-------------+-----------------+----------+-------------+--------------------+------------+----------+-----------+--------------------+----+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features_col = [\"user_purchase\", \"user_purchase_per\", \"user_count\", \n",
    "                \"item_purchase\", \"item_purchase_per\", \"item_count\",\n",
    "                \"year\", \"genres_tfidf\", \"title_tfidf\", \"item_id_topN\",\n",
    "                'user_addict', 'item_addict', 'history_vec']\n",
    "target_col = \"purchase\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=features_col,\n",
    "                            outputCol=\"features\")\n",
    "\n",
    "train_data = assembler.transform(train).cache()\n",
    "valid_data = assembler.transform(valid)\n",
    "test_data  = assembler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(maxDepth=5,\n",
    "                    minInstancesPerNode=3,\n",
    "                    maxBins=50,\n",
    "                    labelCol=target_col)\n",
    "gbt_m = gbt.fit(train_data)\n",
    "pred_valid = gbt_m.transform(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-------------+-----------------+----------+-------------+--------------------+------------+----------+-----------+--------------------+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|user_id|item_id|purchase|user_purchase|user_purchase_per|user_count|item_purchase|   item_purchase_per|item_id_topN|item_count|user_addict|         item_addict|year|        genres_tfidf|         title_tfidf|              vector|         history_vec|            features|       rawPrediction|         probability|prediction|\n",
      "+-------+-------+--------+-------------+-----------------+----------+-------------+--------------------+------------+----------+-----------+--------------------+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "| 728960|  93486|     0.0|          0.0|              0.0|      2081|          3.0|3.443526170798898E-4|           0|      1094|        0.0|0.002742230347349177|2014|(300,[73,241],[0....|(300,[133],[3.330...|(250,[1,2,3,5,9,1...|(250,[1,2,3,5,9,1...|(860,[2,3,4,5,6,8...|[1.54165154015252...|[0.95619873522209...|       0.0|\n",
      "| 728960|  94851|     0.0|          0.0|              0.0|      2081|          1.0|1.147842056932966E-4|           0|      1105|        0.0|9.049773755656109E-4|2010|(300,[10,97],[5.9...|(300,[12,90],[5.0...|(250,[1,2,3,5,9,1...|(250,[1,2,3,5,9,1...|(860,[2,3,4,5,6,1...|[1.54272675604104...|[0.95628871283625...|       0.0|\n",
      "| 728960|  68609|     0.0|          0.0|              0.0|      2081|          1.0|1.147842056932966E-4|           0|      1087|        0.0|9.199632014719411E-4|2013|(300,[10,92,141],...|(300,[86,279],[5....|(250,[1,2,3,5,9,1...|(250,[1,2,3,5,9,1...|(860,[2,3,4,5,6,1...|[1.54272675604104...|[0.95628871283625...|       0.0|\n",
      "+-------+-------+--------+-------------+-----------------+----------+-------------+--------------------+------------+----------+-----------+--------------------+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_valid.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(860, {0: 0.2849, 1: 0.0032, 2: 0.014, 3: 0.2939, 4: 0.0138, 5: 0.0046, 6: 0.0028, 11: 0.0, 17: 0.0066, 66: 0.0035, 79: 0.0174, 80: 0.0007, 99: 0.0025, 104: 0.0011, 138: 0.0118, 144: 0.0003, 148: 0.0082, 188: 0.0023, 229: 0.0005, 248: 0.0001, 253: 0.0047, 273: 0.0025, 307: 0.0018, 311: 0.0014, 331: 0.0009, 341: 0.0, 350: 0.0009, 357: 0.0004, 359: 0.0059, 399: 0.0011, 439: 0.0009, 474: 0.0006, 537: 0.0, 545: 0.0011, 589: 0.0002, 608: 0.1226, 609: 0.0934, 617: 0.0013, 619: 0.0027, 625: 0.0214, 628: 0.0004, 644: 0.0074, 648: 0.0088, 649: 0.0061, 658: 0.0006, 662: 0.0017, 678: 0.0014, 687: 0.0006, 695: 0.001, 696: 0.0034, 697: 0.0131, 747: 0.0015, 753: 0.013, 769: 0.0004, 808: 0.0003, 816: 0.0003, 831: 0.001, 835: 0.0024, 859: 0.0006})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbt_m.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation \n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=target_col, \n",
    "                                          metricName='areaUnderROC')\n",
    "score = evaluator.evaluate(pred_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test \n",
    "pred = gbt_m.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit \n",
    "pred_pd = pred.select(\"user_id\", \n",
    "                      \"item_id\",\n",
    "                      f.col(\"probability\").alias(target_col)).toPandas()\n",
    "pred_pd = pred_pd.sort_values(by=[\"user_id\", \"item_id\"])\n",
    "pred_pd[target_col] = pred_pd[target_col].apply(lambda x: x[1])\n",
    "pred_pd.to_csv(\"lab03.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
