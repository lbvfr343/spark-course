{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 2 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_COURSES = [[23126, u'en', u'Compass - powerful SASS library that makes your life easier'], [21617, u'en', u'Preparing for the AP* Computer Science A Exam \\u2014 Part 2'], [16627, u'es', u'Aprende Excel: Nivel Intermedio by Alfonso Rinsche'], [11556, u'es', u'Aprendizaje Colaborativo by UNID Universidad Interamericana para el Desarrollo'], [16704, u'ru', u'\\u041f\\u0440\\u043e\\u0433\\u0440\\u0430\\u043c\\u043c\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u043d\\u0430 Lazarus'], [13702, u'ru', u'\\u041c\\u0430\\u0442\\u0435\\u043c\\u0430\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u0430\\u044f \\u044d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u043a\\u0430']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[23126, 'en', 'Compass - powerful SASS library that makes your life easier'],\n",
       " [21617, 'en', 'Preparing for the AP* Computer Science A Exam — Part 2'],\n",
       " [16627, 'es', 'Aprende Excel: Nivel Intermedio by Alfonso Rinsche'],\n",
       " [11556,\n",
       "  'es',\n",
       "  'Aprendizaje Colaborativo by UNID Universidad Interamericana para el Desarrollo'],\n",
       " [16704, 'ru', 'Программирование на Lazarus'],\n",
       " [13702, 'ru', 'Математическая экономика']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MY_COURSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"Temirlan Spark Dataframe app\") \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).appName(\"Recommend system app\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -get /labs/slaba02/DO_record_per_line.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master-5.newprolab.com:4060\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Recommend system app</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5ea803c8d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.read.json(\"/labs/slaba02/DO_record_per_line.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col, lower, regexp_replace\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_with_regions.groupBy(\"region\")\\\n",
    "                .agg(f.count(\"ip\").alias(\"count\"), f.count(f.lit(\"1\")).alias(\"cnt\"))\\\n",
    "                .orderBy(\"count\", ascending=False)\\\n",
    "                .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.withColumn('desc', lower(col('desc'))) \\\n",
    "                 .withColumn('desc', regexp_replace('desc', \"[^a-zA-Z\\\\s]\", \"\")) \\\n",
    "                 .select(['id', 'lang', 'desc']).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "dataset = dataset[(dataset.lang=='en') | (dataset.lang=='es') | (dataset.lang=='ru')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stop_words = nltk.corpus.stopwords.words('english')\n",
    "spanish_stop_words = nltk.corpus.stopwords.words('spanish')\n",
    "russian_stop_words = nltk.corpus.stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_remover = StopWordsRemover(inputCol='desc', outputCol='desc_clean', stopWords=english_stop_words)\n",
    "spanish_remover = StopWordsRemover(inputCol='desc', outputCol='desc_clean', stopWords=spanish_stop_words)\n",
    "russian_remover = StopWordsRemover(inputCol='desc', outputCol='desc_clean', stopWords=russian_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_english = dataset[dataset.lang == 'en']\n",
    "df_spanigh = dataset[dataset.lang == 'es']\n",
    "df_russian = dataset[dataset.lang == 'ru']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_english = english_remover.transform(df_english.withColumn('desc', f.split('desc', ' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spanigh = spanish_remover.transform(df_spanigh.withColumn('desc', f.split('desc', ' ')))\n",
    "df_russian = russian_remover.transform(df_russian.withColumn('desc', f.split('desc', ' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_english.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = {'eng': english_stop_words,\n",
    "             'esp': spanish_stop_words,\n",
    "             'rus': russian_stop_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, max as max_, col, rank, lit\n",
    "from pyspark.sql.types import IntegerType, DoubleType, ArrayType, StringType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "import re\n",
    "import numpy as np \n",
    "\n",
    "def cos_(x, y):\n",
    "    if(x == None or y == None):\n",
    "        return np.nan\n",
    "    else:\n",
    "        if float(x.norm(2)*y.norm(2))!=0:\n",
    "            return float(x.dot(y)/(x.norm(2)*y.norm(2)))\n",
    "        else:\n",
    "            return float(-1)\n",
    "cosinus_ = udf(lambda x,y: cos_(x, y), DoubleType())\n",
    "\n",
    "def get_tokens(s_):\n",
    "    regex = re.compile(u'[\\w\\d]{2,}', re.U)\n",
    "    return regex.findall(s_.lower())\n",
    "\n",
    "get_tokens_ = udf(lambda x: get_tokens(x), ArrayType(StringType()))\n",
    "\n",
    "result_dict = {}\n",
    "len_ = len(MY_COURSES)\n",
    "for i in range(0, len_):\n",
    "    cur_course = MY_COURSES[i]\n",
    "    df_ = rdd \\\n",
    "    .filter(col('lang')==cur_course[1])\n",
    "\n",
    "    t_ = df_ \\\n",
    "    .withColumn('desc', get_tokens_(col('desc')))\n",
    "    \n",
    "    ht = HashingTF(inputCol='desc', outputCol='vector_tf', numFeatures=10000)\n",
    "    tf_ = ht.transform(t_)\n",
    "\n",
    "    idf_m = IDF(inputCol='vector_tf', outputCol='vector_')  \n",
    "    idf_r = idf_m.fit(tf_) \n",
    "    idf_ = idf_r.transform(tf_)\n",
    "    \n",
    "    v_ = idf_\\\n",
    "    .filter(col('id')==cur_course[0])\\\n",
    "    .withColumnRenamed('id','id_base')\\\n",
    "    .withColumnRenamed('vector_','vector_base')\\\n",
    "    .select('id_base','vector_base')\n",
    "\n",
    "    tv_ = idf_\\\n",
    "    .join(v_,col('id')!=col('id_base'))\\\n",
    "    .withColumn('cos', cosinus_(col('vector_base'), col('vector_')))\\\n",
    "    .select('id', 'id_base', 'name', 'cos')\n",
    "\n",
    "    window = Window.orderBy(tv_['cos'].desc(), tv_['name'])\n",
    "    t_ = tv_.select('id', 'cos', rank().over(window).alias('rank')).filter(col('rank') <= 10).collect()\n",
    "    l_ = [int(row[0]) for row in t_]\n",
    "    result_dict[str(cur_course[0])] = l_\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'23126': [14760, 13665, 13782, 20638, 24419, 15909, 2724, 25782, 17499, 13348], '21617': [21609, 21616, 22298, 21608, 21630, 21628, 21508, 21623, 21081, 19417], '16627': [11431, 17961, 17964, 5687, 12247, 16694, 5558, 12660, 11575, 9563], '11556': [10384, 16488, 468, 22710, 13461, 21707, 19330, 23357, 10447, 9465], '16704': [1219, 1327, 20362, 1228, 26980, 55, 1236, 1247, 1365, 913, 20095], '13702': [864, 21079, 1111, 792, 1410, 8123, 1041, 1033, 8313, 1396]}\n"
     ]
    }
   ],
   "source": [
    "print(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('lab02.json', 'w') as fp:\n",
    "    json.dump(result_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
