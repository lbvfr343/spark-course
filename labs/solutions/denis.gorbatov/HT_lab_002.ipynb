{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 2 pyspark-shell'\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import Row\n",
    "import json\n",
    "from pyspark import SparkContext\n",
    "conf = SparkConf()\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .config(conf=conf)\n",
    "         .appName(\"test\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(fields=[\n",
    "    StructField(\"lang\", StringType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"cat\", StringType()),\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"desc\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"/labs/slaba02/DO_record_per_line.json\", schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_chars(str_1):\n",
    "    regex = re.compile(u'[\\w\\d]{2,}', re.U)\n",
    "    \n",
    "    return regex.findall(str_1.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType,IntegerType,StringType\n",
    "change_charsUDF = udf(lambda z:change_chars(z),StringType())\n",
    "# df = df.withColumn(\"desc_new\", change_charsUDF(\"desc\")) \n",
    "df = df.select(\"id\", \\\n",
    "    change_charsUDF(\"desc\").alias(\"desc\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, Tokenizer\n",
    "# retokenizer = RegexTokenizer(inputCol=\"desc\", outputCol=\"words\", pattern=\"[0-9]{5,}\")\n",
    "# df = retokenizer.transform(df)\n",
    "tokenizer = Tokenizer(inputCol=\"desc\",outputCol=\"words\")\n",
    "# df = tokenizer.transform(df)\n",
    "# df.select('words').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "preprocessing = Pipeline(stages=[\n",
    "    tokenizer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_model = preprocessing.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dataset = preprocessing_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dataset.select('id','words').where('id = 23126').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dataset.select('words').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF\n",
    "ht = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_tf = ht.transform(preprocessed_dataset)\n",
    "# result_tf.select('rawFeatures').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(result_tf)\n",
    "rescaledData = idfModel.transform(result_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaledData.select('id','features').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_id_list = [23126, 21617, 16627, 11556, 16704, 13702]\n",
    "# rescaledData.select('id','features').where('id = 23126').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_ids = rescaledData.select('id')\n",
    "# only_ids.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids=only_ids.rdd.map(lambda x: x[0]).collect()\n",
    "print(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_tfidf = rescaledData.select('features')\n",
    "# only_tfidf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_tfidf = only_tfidf.rdd.map(lambda x: x[0]).collect()\n",
    "all_tfidf = only_tfidf.collect()\n",
    "all_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tfidf = only_tfidf.rdd.map(lambda x: x[0]).collect()\n",
    "all_tfidf[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(all_tfidf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_tfidf[1].dot(all_tfidf[2]) / (all_tfidf[1].norm(2) * all_tfidf[2].norm(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_cos_sim = {}\n",
    "for l in my_id_list:\n",
    "    dict_cos_sim[l] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_get_cos_sim(my_id, list_of_all_ids,all_tfidf):\n",
    "    dict_output = {}\n",
    "    my_id_index = list_of_all_ids.index(my_id)\n",
    "    for i in list_of_all_ids:\n",
    "        if my_id != i:\n",
    "            index_of_el = list_of_all_ids.index(i)\n",
    "            dict_output[i] = all_tfidf[my_id_index].dot(all_tfidf[index_of_el]) / (all_tfidf[my_id_index].norm(2) * all_tfidf[index_of_el].norm(2))\n",
    "    return dict_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_sorted = {}\n",
    "for i,v in enumerate(my_id_list):\n",
    "    dict_1 = func_get_cos_sim(v,all_ids, all_tfidf)\n",
    "    sorted_tuple = sorted(dict_1.items(), key=lambda x: x[1], reverse = True)\n",
    "    top_10 = sorted_tuple[0:10]\n",
    "    anwr = []\n",
    "    for k in top_10:\n",
    "        anwr.append(k[0])\n",
    "    dict_sorted[str(v)] = anwr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('desc').where('id = 16627').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asd = [25010, 387, 386, 166, 196, 59, 160, 198, 468, 2474]\n",
    "df.select('desc').where('id = 166').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('desc').where('id = 196').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dataset.select('words').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('lab02.json', \"w\") as file:\n",
    "    json.dump(dict_sorted, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
