{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "spark_home = '/usr/hdp/current/spark2-client'\n",
    "os.environ[\"SPARK_HOME\"]=spark_home\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 --executor-cores 1 --executor-memory 5g --driver-memory 2g pyspark-shell'\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'application_1667306389915_1653'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, HiveContext\n",
    "\n",
    "conf = SparkConf().set(\"spark.app.name\", \"lab04\") \n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "spark = HiveContext(sc)\n",
    "\n",
    "sc.applicationId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, StringIndexer, IndexToString\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Работа с тестовыми данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------------------------------------+----------------------------------------------------------------------+\n",
      "|gender|  age|                                 uid|                                                             user_json|\n",
      "+------+-----+------------------------------------+----------------------------------------------------------------------+\n",
      "|     F|18-24|d50192e5-c44e-4ae8-ae7a-7cfe67c8b777|{\"visits\": [{\"url\": \"http://zebra-zoya.ru/200028-chehol-organayzer-...|\n",
      "|     M|25-34|d502331d-621e-4721-ada2-5d30b2c3801f|{\"visits\": [{\"url\": \"http://sweetrading.ru/?p=900\", \"timestamp\": 14...|\n",
      "|     F|25-34|d50237ea-747e-48a2-ba46-d08e71dddfdb|{\"visits\": [{\"url\": \"http://ru.oriflame.com/products/product?code=3...|\n",
      "|     F|25-34|d502f29f-d57a-46bf-8703-1cb5f8dcdf03|{\"visits\": [{\"url\": \"http://translate-tattoo.ru/font-selection/?has...|\n",
      "|     M| >=55|d503c3b2-a0c2-4f47-bb27-065058c73008|{\"visits\": [{\"url\": \"https://mail.rambler.ru/#/folder/\", \"timestamp...|\n",
      "+------+-----+------------------------------------+----------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "event_schema = T.StructType([\n",
    "    T.StructField('gender', T.StringType()),\n",
    "    T.StructField('age', T.StringType()),\n",
    "    T.StructField('uid', T.StringType()),\n",
    "    T.StructField('user_json', T.StringType()),\n",
    "])\n",
    "train_data_raw = spark.read.csv('/labs/slaba04/gender_age_dataset.txt', header=True, schema=event_schema, sep='\\t')\n",
    "train_data_raw.show(5, 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_json='{\"visits\": [{\"url\": \"http://zebra-zoya.ru/200028-chehol-organayzer-dlja-macbook-11-grid-it.html?utm_campaign=397720794&utm_content=397729344&utm_medium=cpc&utm_source=begun\", \"timestamp\": 1419688144068}, {\"url\": \"http://news.yandex.ru/yandsearch?cl4url=chezasite.com/htc/htc-one-m9-delay-86327.html&lr=213&rpt=story\", \"timestamp\": 1426666298001}, {\"url\": \"http://www.sotovik.ru/news/240283-htc-one-m9-zaderzhivaetsja.html\", \"timestamp\": 1426666298000}, {\"url\": \"http://news.yandex.ru/yandsearch?cl4url=chezasite.com/htc/htc-one-m9-delay-86327.html&lr=213&rpt=story\", \"timestamp\": 1426661722001}, {\"url\": \"http://www.sotovik.ru/news/240283-htc-one-m9-zaderzhivaetsja.html\", \"timestamp\": 1426661722000}]}')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visit_schema = T.StructType([\n",
    "    T.StructField('visits', T.ArrayType(T.StructType([\n",
    "        T.StructField('url', T.StringType()),\n",
    "        T.StructField('timestamp', T.LongType()),\n",
    "    ])))\n",
    "])\n",
    "train_data_raw.select(\"user_json\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------------------------------------+----------------------------------------------------------------------+\n",
      "|gender|  age|                                 uid|                                                                   url|\n",
      "+------+-----+------------------------------------+----------------------------------------------------------------------+\n",
      "|     F|18-24|d50192e5-c44e-4ae8-ae7a-7cfe67c8b777|[http://zebra-zoya.ru/200028-chehol-organayzer-dlja-macbook-11-grid...|\n",
      "|     M|25-34|d502331d-621e-4721-ada2-5d30b2c3801f|[http://sweetrading.ru/?p=900, http://sweetrading.ru/?p=884, http:/...|\n",
      "|     F|25-34|d50237ea-747e-48a2-ba46-d08e71dddfdb|[http://ru.oriflame.com/products/product?code=30569, http://ru.orif...|\n",
      "|     F|25-34|d502f29f-d57a-46bf-8703-1cb5f8dcdf03|[http://translate-tattoo.ru/font-selection/?hash=1199c573a5f4da47ed...|\n",
      "|     M| >=55|d503c3b2-a0c2-4f47-bb27-065058c73008|[https://mail.rambler.ru/#/folder/, http://news.rambler.ru/29728405...|\n",
      "+------+-----+------------------------------------+----------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = (train_data_raw\n",
    "              .filter(F.length(\"age\") > 1)\n",
    "              .filter(F.col(\"gender\").isin([\"M\", \"F\"]))\n",
    "              .withColumn('visits_json', F.from_json(F.col('user_json'), visit_schema))\n",
    "              .withColumn('url', F.col('visits_json.visits.url'))\n",
    "              .drop('visits_json', 'user_json'))\n",
    "train_data.show(5, 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexGender = StringIndexer(inputCol='gender', outputCol='gender_cat')\n",
    "indexAge = StringIndexer(inputCol='age', outputCol='age_cat')\n",
    "indexModelGender = indexGender.fit(train_data)\n",
    "indexModelAge = indexAge.fit(train_data)\n",
    "df_visits = indexModelGender.transform(train_data)\n",
    "df_visits = indexModelAge.transform(df_visits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "\n",
    "class ParseURLTransformer(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(ParseURLTransformer, self).__init__()\n",
    "        if inputCol is not None:\n",
    "            self.setInputCol(inputCol)\n",
    "        if outputCol is not None:\n",
    "            self.setOutputCol(outputCol)\n",
    "        \n",
    "    def _transform(self, dataset):\n",
    "        res = (dataset.withColumn('url_inter', F.explode(self.getInputCol()))\n",
    "               .withColumn('url_inter', F.expr('parse_url(url_inter, \"HOST\")'))\n",
    "               .withColumn('url_inter', F.lower(F.col('url_inter')))\n",
    "               .drop(self.getInputCol())\n",
    "        )\n",
    "        res = res.groupBy(dataset.drop(self.getInputCol()).columns).agg(F.collect_list('url_inter').alias(self.getOutputCol()))\n",
    "        return res\n",
    "    \n",
    "parseURL = ParseURLTransformer(inputCol='url', outputCol='url_parsed')\n",
    "hashingTF = HashingTF(numFeatures=100000, binary=False, inputCol=\"url_parsed\", outputCol=\"url_freq\")\n",
    "forestG = RandomForestClassifier(featuresCol='url_freq', labelCol='gender_cat', predictionCol='predictionG',\n",
    "                                 probabilityCol='probabilityG', rawPredictionCol='rawPredictionG')\n",
    "forestA = RandomForestClassifier(featuresCol='url_freq', labelCol='age_cat', predictionCol='predictionA',\n",
    "                                 probabilityCol='probabilityA', rawPredictionCol='rawPredictionA')\n",
    "strindG = IndexToString(inputCol='predictionG', outputCol='gender_str', labels=indexModelGender.labels)\n",
    "strindA = IndexToString(inputCol='predictionA', outputCol='age_str', labels=indexModelAge.labels)\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    parseURL,\n",
    "    hashingTF,\n",
    "    forestG,\n",
    "    forestA,\n",
    "    strindG,\n",
    "    strindA\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 124 ms, sys: 58.9 ms, total: 183 ms\n",
      "Wall time: 13min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipeline_model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_model.write().overwrite().save(\"tmp/lab04/pipeline_model\")\n",
    "# pipeline_model = PipelineModel.load(\"tmp/lab04/pipeline_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Работа с Kafka Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kill_all():\n",
    "    streams = SparkSession.builder.getOrCreate().streams.active\n",
    "    if streams:\n",
    "        for s in streams:\n",
    "            desc = \"\"\n",
    "            try:\n",
    "                desc = s.lastProgress[\"sources\"][0][\"description\"]\n",
    "            except:\n",
    "                print(f\"{s} data not available\")\n",
    "            s.stop()\n",
    "            print(f\"Stopped {desc} at {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_BOOTSTRAP_SERVER = 'spark-master-1.newprolab.com:6667'\n",
    "KAFKA_INPUT_TOPIC = 'input_anton.gladkiy'\n",
    "KAFKA_OUTPUT_TOPIC = 'anton.gladkiy'\n",
    "\n",
    "event_schema = T.StructType([\n",
    "    T.StructField('uid', T.StringType(), True),\n",
    "    T.StructField('visits', T.StringType(), True),\n",
    "])\n",
    "\n",
    "visit_schema = T.ArrayType(\n",
    "    T.StructType([\n",
    "        T.StructField('url', T.StringType(), True),\n",
    "        T.StructField('timestamp', T.LongType(), True)\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/06 23:51:14 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-master-1.newprolab.com:8020/user/anton.gladkiy/tmp/lab04/checkpointLocation' to trash at: hdfs://spark-master-1.newprolab.com:8020/user/anton.gladkiy/.Trash/Current/user/anton.gladkiy/tmp/lab04/checkpointLocation1667767874505\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f0179271e10>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! hdfs dfs -rm -R /user/anton.gladkiy/tmp/lab04/checkpointLocation\n",
    "\n",
    "kafka_read_df = (spark\n",
    "    .readStream\n",
    "    .format('kafka')\n",
    "    .option('kafka.bootstrap.servers', KAFKA_BOOTSTRAP_SERVER)\n",
    "    .option('subscribe', KAFKA_INPUT_TOPIC)\n",
    "    .option('startingOffsets', 'earliest')\n",
    "    .option('failOnDataLoss', 'False')\n",
    "    .load()\n",
    ")\n",
    "\n",
    "clean_df = (kafka_read_df\n",
    "    .select(F.col('value').cast('string').alias('value'))\n",
    "    .select(F.from_json(F.col('value'), event_schema).alias('event'))\n",
    "    .select('evenT.uid', F.from_json(F.col('evenT.visits'), visit_schema).alias('visits'))\n",
    "    .withColumn('url', F.col('visits.url'))\n",
    "    .drop('visits')\n",
    ")\n",
    "\n",
    "predictions_df = pipeline_model.transform(clean_df) \\\n",
    ".select('uid', F.col('gender_str').alias('gender'), F.col('age_str').alias('age'))\n",
    "\n",
    "kafka_out_df = predictions_df.select(F.to_json(F.struct(*predictions_df.columns)).alias('value'))\n",
    "\n",
    "kafka_write_stream = (\n",
    "    kafka_out_df\n",
    "    .writeStream\n",
    "    .format(\"kafka\")\n",
    "    .outputMode(\"complete\")\n",
    "    .option(\"checkpointLocation\", \"tmp/lab04/checkpointLocation\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVER)\n",
    "    .option(\"topic\", KAFKA_OUTPUT_TOPIC)\n",
    ")\n",
    "kafka_write_stream.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! /usr/hdp/current/kafka-broker/bin/kafka-console-consumer.sh --bootstrap-server spark-master-1:6667 --topic anton.gladkiy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped KafkaV2[Subscribe[input_anton.gladkiy]] at <pyspark.sql.streaming.StreamingQuery object at 0x7f01792545f8>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kill_all()\n",
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
