{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "home = os.environ[\"HOME\"]\n",
    "spark_home = '/usr/hdp/current/spark2-client'\n",
    "\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='pyspark-shell --num-executors 2'\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'application_1665753715559_2711'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext, HiveContext\n",
    "from pyspark.sql.types import DoubleType, StringType, ArrayType\n",
    "from pyspark.sql.functions import udf, lit, col, row_number, collect_list, concat, concat_ws\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, Normalizer\n",
    "\n",
    "conf = SparkConf().setAppName(\"lab02\").setMaster(\"yarn\").set(\"spark.executor.instances\", 2) \\\n",
    ".set(\"spark.sql.crossJoin.enabled\", True)\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "spark = HiveContext(sc)\n",
    "\n",
    "sc.getConf().get(\"spark.app.id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable \n",
    "\n",
    "class CustomTokenizer(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(CustomTokenizer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "        \n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        def f(s):\n",
    "            regex = re.compile(u'[\\w\\d]{2,}', re.U)\n",
    "            return regex.findall(s.lower())\n",
    "        \n",
    "        t = ArrayType(StringType())\n",
    "        out_col = self.getOutputCol()\n",
    "        in_col = dataset[self.getInputCol()]\n",
    "        return dataset.withColumn(out_col, udf(f, t)(in_col))\n",
    "\n",
    "cosine_udf = udf(lambda i, j: float(i.dot(j) / (i.norm(2) * j.norm(2))), DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23126, 21617, 16627, 11556, 16704, 13702]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses_variant = [[23126, u'en', u'Compass - powerful SASS library that makes your life easier'], [21617, u'en', u'Preparing for the AP* Computer Science A Exam \\u2014 Part 2'], [16627, u'es', u'Aprende Excel: Nivel Intermedio by Alfonso Rinsche'], [11556, u'es', u'Aprendizaje Colaborativo by UNID Universidad Interamericana para el Desarrollo'], [16704, u'ru', u'\\u041f\\u0440\\u043e\\u0433\\u0440\\u0430\\u043c\\u043c\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u043d\\u0430 Lazarus'], [13702, u'ru', u'\\u041c\\u0430\\u0442\\u0435\\u043c\\u0430\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u0430\\u044f \\u044d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u043a\\u0430']]\n",
    "courses_ids = [i[0] for i in courses_variant]\n",
    "courses_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23126, 21617, 16627, 11556, 16704, 13702]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# courses_variant_df = pd.DataFrame(courses_variant, columns=['id', 'lang','name'])\n",
    "# courses_variant_sdf = spark.createDataFrame(data=courses_variant_df)\n",
    "# courses_ids = courses_variant_sdf.select('id').rdd.flatMap(lambda x: x).collect()\n",
    "# courses_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "|                 cat|                desc| id|lang|                name|      provider|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "|3/business_manage...|This course intro...|  4|  en|Accounting Cycle:...|Canvas Network|\n",
      "|              11/law|This online cours...|  5|  en|American Counter ...|Canvas Network|\n",
      "|5/computer_scienc...|This course is ta...|  6|  fr|Arithmétique: en ...|Canvas Network|\n",
      "|  14/social_sciences|We live in a digi...|  7|  en|Becoming a Dynami...|Canvas Network|\n",
      "|2/biology_life_sc...|This self-paced c...|  8|  en|           Bioethics|Canvas Network|\n",
      "|9/humanities|15/m...|This game-based c...|  9|  en|College Foundatio...|Canvas Network|\n",
      "|  14/social_sciences|What’s in your di...| 10|  en|Digital Literacies I|Canvas Network|\n",
      "|  14/social_sciences|The goal of the D...| 11|  en|Digital Literacie...|Canvas Network|\n",
      "|  14/social_sciences|Ready to explore ...| 12|  en|Digital Tools for...|Canvas Network|\n",
      "|  14/social_sciences|This self-paced c...| 13|  en|Discover Your Val...|Canvas Network|\n",
      "|  12/medicine_health|What is “interpro...| 14|  en|Enhancing Patient...|Canvas Network|\n",
      "|        16/languages|This course prese...| 15|  en|Ethics and Values...|Canvas Network|\n",
      "|         4/chemistry|Chemistry is an i...| 16|  en| Exploring Chemistry|Canvas Network|\n",
      "|8/engineering_tec...|Are you consideri...| 17|  en|Exploring Enginee...|Canvas Network|\n",
      "|   1/arts_music_film|Princess stories ...| 18|  en|Fairy Tales: Orig...|Canvas Network|\n",
      "|        9/humanities|This first instal...| 19|  en|First Peoples to ...|Canvas Network|\n",
      "|  14/social_sciences|This course exami...| 20|  en| Forums for a Future|Canvas Network|\n",
      "|        9/humanities|This course will ...| 21|  en|From the Gilded A...|Canvas Network|\n",
      "|8/engineering_tec...|The field of tech...| 22|  en|Fundamentals of S...|Canvas Network|\n",
      "|  14/social_sciences|Are you a Higher ...| 23|  en|Hybrid Courses: B...|Canvas Network|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "courses_raw = spark.read.json(\"/labs/slaba02/DO_record_per_line.json\")\n",
    "courses_raw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_ru = courses_raw.filter(col(\"lang\") == \"ru\")\n",
    "courses_en = courses_raw.filter(col(\"lang\") == \"en\")\n",
    "courses_other = courses_raw.filter(~col(\"lang\").isin([\"ru\", \"en\"]))\n",
    "courses_langs = [courses_ru, courses_en, courses_other]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+\n",
      "| id|               words|        words_filter|            features|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "|  4|[this, course, in...|[course, introduc...|(262144,[7532,891...|\n",
      "|  5|[this, online, co...|[online, course, ...|(262144,[1598,172...|\n",
      "|  6|[this, course, is...|[course, taught, ...|(262144,[1244,106...|\n",
      "|  7|[we, live, in, a,...|[live, digitally,...|(262144,[836,4525...|\n",
      "|  8|[this, self-paced...|[self-paced, cour...|(262144,[619,3535...|\n",
      "|  9|[this, game-based...|[game-based, cour...|(262144,[1889,286...|\n",
      "| 10|[what’s, in, your...|[what’s, digital,...|(262144,[12250,27...|\n",
      "| 11|[the, goal, of, t...|[goal, digital, l...|(262144,[7416,183...|\n",
      "| 12|[ready, to, explo...|[ready, explore, ...|(262144,[20457,20...|\n",
      "| 13|[this, self-paced...|[self-paced, cour...|(262144,[37750,43...|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = Tokenizer(inputCol=\"desc\", outputCol=\"words\")\n",
    "# words = tokenizer.transform(courses_raw)\n",
    "# stopWordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"words_filter\")\n",
    "# words_filter = stopWordsRemover.transform(words)\n",
    "# hashingTF = HashingTF(inputCol=\"words_filter\", outputCol=\"tf\")\n",
    "# tf = hashingTF.transform(words_filter)\n",
    "# idf = IDF(inputCol=\"tf\", outputCol=\"features\").fit(tf)\n",
    "# tfidf = idf.transform(tf)\n",
    "# normalizer = Normalizer(inputCol=\"features\", outputCol=\"norm\")\n",
    "# data = normalizer.transform(tfidf)\n",
    "# data.select(\"id\", \"words\", \"words_filter\", \"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|            features|                norm|\n",
      "+---+--------------------+--------------------+\n",
      "|  4|(262144,[4211,753...|(262144,[4211,753...|\n",
      "|  5|(262144,[1598,172...|(262144,[1598,172...|\n",
      "|  6|(262144,[7601,106...|(262144,[7601,106...|\n",
      "|  7|(262144,[836,4525...|(262144,[836,4525...|\n",
      "|  8|(262144,[619,2042...|(262144,[619,2042...|\n",
      "|  9|(262144,[1889,402...|(262144,[1889,402...|\n",
      "| 10|(262144,[12250,12...|(262144,[12250,12...|\n",
      "| 11|(262144,[7416,183...|(262144,[7416,183...|\n",
      "| 12|(262144,[16121,17...|(262144,[16121,17...|\n",
      "| 13|(262144,[43996,48...|(262144,[43996,48...|\n",
      "+---+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = CustomTokenizer(inputCol=\"desc\", outputCol=\"words\")\n",
    "stopWordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"words_filter\")\n",
    "hashingTF = HashingTF(inputCol=\"words_filter\", outputCol=\"tf\")\n",
    "idf = IDF(minDocFreq=1, inputCol=\"tf\", outputCol=\"features\")\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"norm\")\n",
    "\n",
    "preProcStages = [tokenizer, stopWordsRemover, hashingTF, idf, normalizer]\n",
    "pipeline = Pipeline(stages=preProcStages)\n",
    "\n",
    "model = pipeline.fit(courses_raw)\n",
    "data = model.transform(courses_raw)\n",
    "data.select(\"id\", \"words\", \"words_filter\", \"features\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "tokenizer = CustomTokenizer(inputCol=\"desc\", outputCol=\"words\")\n",
    "stopWordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"words_filter\")\n",
    "hashingTF = HashingTF(inputCol=\"words_filter\", outputCol=\"tf\")\n",
    "idf = IDF(minDocFreq=1, inputCol=\"tf\", outputCol=\"features\")\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"norm\")\n",
    "\n",
    "preProcStages = [tokenizer, stopWordsRemover, hashingTF, idf, normalizer]\n",
    "pipeline = Pipeline(stages=preProcStages)\n",
    "\n",
    "courses_list = []\n",
    "for courses_lang in courses_langs:\n",
    "    model = pipeline.fit(courses_lang)\n",
    "    data = model.transform(courses_lang)\n",
    "    courses_list.append(data)\n",
    "\n",
    "courses = reduce(DataFrame.unionAll, courses_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------------------+\n",
      "|i    |j  |cosine               |\n",
      "+-----+---+---------------------+\n",
      "|11556|4  |1.1588395991968858E-4|\n",
      "|11556|5  |1.2107104601292938E-4|\n",
      "|11556|6  |0.0056017904338261735|\n",
      "|11556|7  |0.006421368255308077 |\n",
      "|11556|8  |2.165499882280121E-4 |\n",
      "|11556|9  |0.004655588580904408 |\n",
      "|11556|10 |1.4134647607303173E-4|\n",
      "|11556|11 |1.5108864886008257E-4|\n",
      "|11556|12 |0.001955294352430839 |\n",
      "|11556|13 |0.00665510819623961  |\n",
      "|11556|14 |7.251027573754172E-4 |\n",
      "|11556|15 |1.8650100219093103E-4|\n",
      "|11556|16 |1.4047445252391848E-4|\n",
      "|11556|17 |1.2472397625644356E-4|\n",
      "|11556|18 |0.0                  |\n",
      "|11556|19 |1.1835813111225394E-4|\n",
      "|11556|20 |0.001938116219564554 |\n",
      "|11556|21 |5.644162541220802E-5 |\n",
      "|11556|22 |5.186429992949809E-4 |\n",
      "|11556|23 |0.007832670387017144 |\n",
      "+-----+---+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "courses_cosine = courses.filter(col(\"id\").isin(courses_ids)).alias(\"i\") \\\n",
    ".join(courses.filter(~col(\"id\").isin(courses_ids)).alias(\"j\")) \\\n",
    ".select(col(\"i.id\").alias(\"i\"), \n",
    "        col(\"j.id\").alias(\"j\"), \n",
    "        cosine_udf(\"i.norm\", \"j.norm\").alias(\"cosine\")) \\\n",
    ".na.drop(\"any\").sort(\"i\", \"j\").cache()\n",
    "courses_cosine.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|    i|                   j|\n",
      "+-----+--------------------+\n",
      "|11556|[16488, 468, 1346...|\n",
      "|13702|[864, 21079, 1594...|\n",
      "|16627|[11431, 12247, 17...|\n",
      "|16704|[4592, 1327, 2036...|\n",
      "|21617|[21609, 21616, 22...|\n",
      "|23126|[14760, 13665, 13...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = courses_cosine.withColumn(\"row_number\", row_number().over(Window.partitionBy(\"i\").orderBy(col(\"cosine\").desc()))) \\\n",
    ".filter(col(\"row_number\") <= 10).groupBy(\"i\").agg(collect_list(col(\"j\")).alias(\"j\")).orderBy(col(\"i\")).cache()\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{11556: [16488, 468, 13461, 11523, 22710, 23357, 19330, 10447, 21707, 9465],\n",
       " 13702: [864, 21079, 15946, 8617, 8123, 1396, 1041, 1033, 22053, 8313],\n",
       " 16627: [11431, 12247, 17964, 17961, 5687, 5558, 16694, 12660, 27487, 27879],\n",
       " 16704: [4592, 1327, 20362, 1228, 1236, 1247, 1365, 26980, 8186, 875],\n",
       " 21617: [21609, 21616, 22298, 21608, 21628, 21630, 21081, 21623, 19417, 21624],\n",
       " 23126: [14760, 13665, 13782, 15909, 17864, 25782, 17499, 13348, 19270, 25071]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "result_list = result_df.toJSON().map(lambda j: json.loads(j)).collect()\n",
    "json_dict = {x[\"i\"]: x[\"j\"] for x in result_list}\n",
    "json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 11556: [16488, 468, 13461, 11523, 22710, 23357, 19330, 10447, 21707, 9465],\n",
      "  13702: [864, 21079, 15946, 8617, 8123, 1396, 1041, 1033, 22053, 8313],\n",
      "  16627: [11431, 12247, 17964, 17961, 5687, 5558, 16694, 12660, 27487, 27879],\n",
      "  16704: [4592, 1327, 20362, 1228, 1236, 1247, 1365, 26980, 8186, 875],\n",
      "  21617: [21609, 21616, 22298, 21608, 21628, 21630, 21081, 21623, 19417, 21624],\n",
      "  23126: [14760, 13665, 13782, 15909, 17864, 25782, 17499, 13348, 19270, 25071]}\n"
     ]
    }
   ],
   "source": [
    "# import pprint\n",
    "# json_pformat = pprint.pformat(json_dict, indent=2).replace(\"'\", '\"')\n",
    "# print(json_pformat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(home, \"lab02.json\"), \"w\") as f:\n",
    "    f.write(json.dumps(json_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|    i|                   j|\n",
      "+-----+--------------------+\n",
      "|11556|[16488,468,13461,...|\n",
      "|13702|[864,21079,15946,...|\n",
      "|16627|[11431,12247,1796...|\n",
      "|16704|[4592,1327,20362,...|\n",
      "|21617|[21609,21616,2229...|\n",
      "|23126|[14760,13665,1378...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# result_str = result_df.select(\"i\", concat(lit(\"[\"), concat_ws(\",\", col(\"j\")), lit(\"]\")).alias(\"j\"))\n",
    "# result_str.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_str.coalesce(1).orderBy(\"i\").write.mode(\"overwrite\").option(\"header\", False).csv(\"lab02.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -f lab02.json\n",
    "# ! hdfs dfs -get lab02.json/part-* lab02.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
