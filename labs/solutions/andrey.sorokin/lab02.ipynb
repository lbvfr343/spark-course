{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 2 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master-4.newprolab.com:4051\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f95d8b5f5f8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import Row\n",
    "import json\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .config(conf=conf)\n",
    "         .appName(\"Lab2_by_sand\")\n",
    "         .getOrCreate())\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, IDF\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "COURCES_FILE = \"/labs/slaba02/DO_record_per_line.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "|                 cat|                desc| id|lang|                name|      provider|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "|3/business_manage...|This course intro...|  4|  en|Accounting Cycle:...|Canvas Network|\n",
      "|              11/law|This online cours...|  5|  en|American Counter ...|Canvas Network|\n",
      "|5/computer_scienc...|This course is ta...|  6|  fr|Arithmétique: en ...|Canvas Network|\n",
      "|  14/social_sciences|We live in a digi...|  7|  en|Becoming a Dynami...|Canvas Network|\n",
      "|2/biology_life_sc...|This self-paced c...|  8|  en|           Bioethics|Canvas Network|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Source dataset\n",
    "cources = spark.read.json(COURCES_FILE)\n",
    "cources.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------------------+\n",
      "| id|lang|                desc|\n",
      "+---+----+--------------------+\n",
      "|  4|  en|This course intro...|\n",
      "|  5|  en|This online cours...|\n",
      "|  6|  fr|This course is ta...|\n",
      "|  7|  en|We live in a digi...|\n",
      "|  8|  en|This self-paced c...|\n",
      "+---+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cources = cources.select('id', 'lang', 'desc')\n",
    "cources.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA = [\n",
    "    [23126, u'en', u'Compass - powerful SASS library that makes your life easier'], \n",
    "    [21617, u'en', u'Preparing for the AP* Computer Science A Exam \\u2014 Part 2'], \n",
    "    [16627, u'es', u'Aprende Excel: Nivel Intermedio by Alfonso Rinsche'], \n",
    "    [11556, u'es', u'Aprendizaje Colaborativo by UNID Universidad Interamericana para el Desarrollo'], \n",
    "    [16704, u'ru', u'\\u041f\\u0440\\u043e\\u0433\\u0440\\u0430\\u043c\\u043c\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u043d\\u0430 Lazarus'], \n",
    "    [13702, u'ru', u'\\u041c\\u0430\\u0442\\u0435\\u043c\\u0430\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u0430\\u044f \\u044d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u043a\\u0430']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(fields=[\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"lang\", StringType()),\n",
    "    StructField(\"desc\", StringType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- desc: string (nullable = true)\n",
      "\n",
      "+-----+----+--------------------+\n",
      "|   id|lang|                desc|\n",
      "+-----+----+--------------------+\n",
      "|23126|  en|Compass - powerfu...|\n",
      "|21617|  en|Preparing for the...|\n",
      "|16627|  es|Aprende Excel: Ni...|\n",
      "|11556|  es|Aprendizaje Colab...|\n",
      "|16704|  ru|Программирование ...|\n",
      "|13702|  ru|Математическая эк...|\n",
      "+-----+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Dtaset for raiting\n",
    "test = spark.createDataFrame(data=TEST_DATA, schema=schema)\n",
    "test.printSchema()\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cources.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline_9df0fb73adba"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure an ML pipeline, which consists of three stages: tokenizer, tf, and idf.\n",
    "tokenizer = Tokenizer(inputCol=\"desc\", outputCol=\"words\")\n",
    "tf = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"tf\", ).setNumFeatures(10000)\n",
    "idf = IDF(inputCol=tf.getOutputCol(), outputCol=\"idf\", )\n",
    "pipeline = Pipeline(stages=[tokenizer, tf, idf])\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_db876fa9046b"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc = pipeline.fit(cources)\n",
    "preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform source\n",
    "c1 = preproc.transform(cources).select('id', 'lang', 'desc', 'idf') \\\n",
    "        .withColumnRenamed('idf', 'idf_src') \\\n",
    "        .withColumnRenamed('desc', 'desc_src') \\\n",
    "        .withColumnRenamed('id', 'id_src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform tests\n",
    "c2 = preproc.transform(test).select('id', 'lang', 'desc', 'idf') \\\n",
    "        .withColumnRenamed('idf', 'idf_dst') \\\n",
    "        .withColumnRenamed('desc', 'desc_dst') \\\n",
    "        .withColumnRenamed('id', 'id_dst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join source and test for compare tfidf vectors (only if lang equals)\n",
    "c = c1.join(c2, how='inner', on='lang')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_executors = spark.sparkContext.getConf().get(\"spark.executor.instances\")\n",
    "int(num_executors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coalesce dataframe partitions from 200 to 3 * num_executors\n",
    "c = c.coalesce(int(num_executors) * 3).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Udf function for calculate cosine distance between vectors\n",
    "@f.udf(FloatType())\n",
    "def cosine_distance(v1, v2):\n",
    "#    norm = Vectors.norm(v1, p=2) * Vectors.norm(v2, p=2)\n",
    "    norm = v1.norm(p=2) * v2.norm(p=2)\n",
    "    if norm != 0:\n",
    "        return float(v1.dot(v2) / norm)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id_dst: int, id_src: bigint, desc_src: string, cos_dist: float]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calc distances\n",
    "c = c.withColumn('cos_dist', cosine_distance(f.col('idf_src'), f.col('idf_dst'))) \\\n",
    "        .select(f.col('id_dst'), f.col('id_src'), f.col('desc_src'), f.col('cos_dist')) \\\n",
    "        .sort(f.col('id_dst'), f.col('cos_dist').desc(), f.col('desc_src').asc(), f.col('id_src').asc())\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_dst: integer (nullable = true)\n",
      " |-- id_src: long (nullable = true)\n",
      " |-- desc_src: string (nullable = true)\n",
      " |-- cos_dist: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|   id|                recs|\n",
      "+-----+--------------------+\n",
      "|21617|[21703, 21700, 21...|\n",
      "|13702|[864, 13702, 1266...|\n",
      "|23126|[13665, 23126, 13...|\n",
      "|16704|[1252, 1427, 2010...|\n",
      "|16627|[9471, 19161, 388...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Make result dataframe\n",
    "res = c.select(f.col('id_dst').cast('string').alias('id'), f.col('id_src')) \\\n",
    "        .groupBy('id').agg(f.collect_list('id_src')) \\\n",
    "        .withColumnRenamed('collect_list(id_src)', 'recs') \n",
    "res.cache()\n",
    "res.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write to local file\n",
    "home_dir = os.environ['HOME']\n",
    "with open(f\"/{home_dir}/lab02.json\", 'wt') as file:\n",
    "    json.dump({k: v for (k, v) in res.collect()}, fp=file, indent=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "   \"21617\": [\r\n",
      "      21703,\r\n",
      "      21700,\r\n",
      "      21675,\r\n",
      "      21506,\r\n",
      "      21676,\r\n",
      "      21706,\r\n",
      "      21508,\r\n",
      "      21854,\r\n"
     ]
    }
   ],
   "source": [
    "#Check result file\n",
    "!head ~/lab02.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
