{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 --executor-memory 2g --executor-cores 1 --driver-memory 2g pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml import Transformer, Pipeline\n",
    "from pyspark.ml.param.shared import HasOutputCol, HasInputCol\n",
    "from pyspark import keyword_only\n",
    "\n",
    "from pyspark.sql.types import LongType, StringType, StructType, StructField, IntegerType, FloatType, ArrayType, LongType\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"lab4\") \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/labs/slaba03/laba03_train.csv\"\n",
    "test_path = \"/labs/slaba03/laba03_test.csv\"\n",
    "\n",
    "schema = StructType(fields=[\n",
    "    StructField(\"user_id\", IntegerType()),\n",
    "    StructField(\"item_id\", IntegerType()),\n",
    "    StructField(\"purchase\", IntegerType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_path = \"/labs/slaba03/laba03_items.csv\"\n",
    "\n",
    "items_schema = StructType(fields=[\n",
    "    StructField(\"item_id\", IntegerType()),\n",
    "    StructField(\"channel_id\", FloatType()),\n",
    "    StructField(\"datetime_availability_start\", StringType()),\n",
    "    StructField(\"datetime_availability_stop\", StringType()),    \n",
    "    StructField(\"datetime_show_start\", StringType()),\n",
    "    StructField(\"datetime_show_stop\", StringType()),\n",
    "    StructField(\"content_type\", IntegerType()),\n",
    "    StructField(\"title\", StringType()),\n",
    "    StructField(\"year\", FloatType()),\n",
    "    StructField(\"genres\", StringType()),   \n",
    "    StructField(\"region_id\", FloatType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "views_path = \"/labs/slaba03/laba03_views_programmes.csv\"\n",
    "\n",
    "views_schema = StructType(fields=[\n",
    "    StructField(\"user_id\", IntegerType()),\n",
    "    StructField(\"item_id\", IntegerType()),\n",
    "    StructField(\"ts_start\", IntegerType()),\n",
    "    StructField(\"ts_end\", IntegerType()),\n",
    "    StructField(\"item_type\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = spark.read.csv(train_path, schema=schema, header=True)\n",
    "df_test = spark.read.csv(test_path, schema=schema, header=True)\n",
    "df_items = spark.read.csv(items_path, schema=items_schema, header=True, sep=\"\\t\")\n",
    "df_views = spark.read.csv(views_path, schema=views_schema, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items = df_items.withColumn(\"duration\", f.col(\"datetime_show_stop\") - f.col(\"datetime_show_start\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.join(df_items, on=\"item_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "films_info = df_train.groupby(\"item_id\").agg(f.sum(\"purchase\").alias(\"buy_cnt\"),\n",
    "                               f.count(\"purchase\").alias(\"total\")) \\\n",
    "            .withColumn(\"conversion\", f.col(\"buy_cnt\") / f.col(\"total\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.withColumn(\"genres_array\", f.when(f.col(\"genres\").isNotNull(), f.split(\"genres\", \",\")).otherwise(f.array([]))) \\\n",
    "    .withColumn(\"firsts_genres\", f.array([f.col(\"genres_array\")[0], f.col(\"genres_array\")[1], f.col(\"genres_array\")[2]])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_info = df_views \\\n",
    "    .withColumn(\"is_live\", f.when(f.col(\"item_type\") == \"live\", 1).otherwise(0)) \\\n",
    "    .withColumn(\"watch_time\", f.col(\"ts_end\") - f.col(\"ts_start\")) \\\n",
    "    .groupby(\"user_id\").agg(f.count(\"user_id\").alias(\"total_views\"),\n",
    "                            f.max(\"watch_time\").alias(\"max_watch_time\"),\n",
    "                            f.min(\"watch_time\").alias(\"min_watch_time\"),\n",
    "                            f.mean(\"watch_time\").alias(\"mean_time\"),\n",
    "                            f.sum(\"is_live\").alias(\"cnt_is_live\")) \\\n",
    "    .withColumn(\"live_ratio\", f.col(\"cnt_is_live\") / f.col(\"total_views\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_buy_rate = df_train.select(\"user_id\", \"purchase\").groupby(\"user_id\").agg(f.sum(\"purchase\").alias(\"total_purchases\"),\n",
    "                                                              f.count(\"user_id\").alias(\"total_offers\")) \\\n",
    "        .withColumn(\"buy_ratio\", f.col(\"total_purchases\") / f.col(\"total_offers\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.join(user_buy_rate, on=\"user_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.join(user_info, on=\"user_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.join(films_info, on=\"item_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.fillna({\"year\": 2005.0,\n",
    "                            \"region_id\": 0,\n",
    "                            \"content_type\": 0,\n",
    "                            \"total_views\": 0,\n",
    "                            \"max_watch_time\": 0,\n",
    "                            \"min_watch_time\": 0,\n",
    "                            \"mean_time\": 0,\n",
    "                            \"cnt_is_live\": 0,\n",
    "                            \"live_ratio\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"firsts_genres\", outputCol=\"cv_genres\")\n",
    "model = cv.fit(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = model.transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = df_train.fillna({\"primary_genre\": \"Не известно\",\n",
    "#                             \"secondary_genre\": \"Не известно\",\n",
    "#                             \"duration\": 0, \n",
    "#                             \"year\": 0,\n",
    "#                             \"region_id\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primary = StringIndexer(inputCol=\"primary_genre\", outputCol=\"primary_genre_enc\", handleInvalid=\"skip\")\n",
    "# secondary = StringIndexer(inputCol=\"secondary_genre\", outputCol=\"secondary_genre_enc\", handleInvalid=\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_train = primary.fit(df_train).transform(df_train)\n",
    "# df_train = secondary.fit(df_train).transform(df_train), \"cv_genres\" , handleInvalid=\"skip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assembler = VectorAssembler(inputCols=[\"buy_ratio\", \"total_purchases\", \"total_offers\", \"content_type\",\n",
    "#                                        \"total_views\", \"max_watch_time\", \"min_watch_time\", \"mean_time\", \"cnt_is_live\", \"live_ratio\", \n",
    "#                                        \"year\", \"region_id\", \"conversion\",\n",
    "#                                        \"total\", \"buy_cnt\", \"cv_genres\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"buy_ratio\", \"total_purchases\", \"total_offers\", \"content_type\",\n",
    "                                       \"total_views\", \"max_watch_time\", \"min_watch_time\", \"mean_time\", \"cnt_is_live\", \"live_ratio\",\n",
    "                                       ], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = assembler.transform(df_train)\n",
    "df_train1 = assembler.transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = df_train.sampleBy(\"purchase\", fractions={0: 0.8, 1: 0.8}, seed=5757)\n",
    "# test = df_train.join(train, on=[\"user_id\", \"item_id\"], how=\"leftanti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_train1.sampleBy(\"purchase\", fractions={0: 0.8, 1: 0.8}, seed=5757)\n",
    "test = df_train1.join(train, on=[\"user_id\", \"item_id\"], how=\"leftanti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"probability\", labelCol=\"purchase\", metricName='areaUnderROC')\n",
    "\n",
    "gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"purchase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "    gbt\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8511177007880439"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_udf = f.udf(lambda x: float(x[1]), FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------+\n",
      "|user_id|item_id|   purchase|\n",
      "+-------+-------+-----------+\n",
      "|   1654|    540|0.044613488|\n",
      "|   1654|    546|0.044613488|\n",
      "|   1654|   1125|0.044613488|\n",
      "|   1654|   1131|0.044613488|\n",
      "|   1654|   1320|0.044613488|\n",
      "+-------+-------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"user_id\", \"item_id\", v_udf(\"probability\").alias(\"purchase\")).orderBy(\"user_id\", \"item_id\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "# gbparamGrid = ParamGridBuilder() \\\n",
    "#              .addGrid(gbt.maxDepth, [5, 10]) \\\n",
    "#              .build()\n",
    "\n",
    "# crossval = CrossValidator(estimator=pipeline, estimatorParamMaps=gbparamGrid,\n",
    "#                               evaluator=evaluator, numFolds=3, parallelism=5)\n",
    "\n",
    "# cv_model = crossval.fit(train)\n",
    "\n",
    "# cv_model.bestModel\n",
    "\n",
    "\n",
    "# predictions = cv_model.transform(test)\n",
    "# evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = spark.read.csv(test_path, schema=schema, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2156840"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.join(df_items, on=\"item_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.join(user_buy_rate, on=\"user_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.join(user_info, on=\"user_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.join(films_info, on=\"item_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.withColumn(\"genres_array\", f.when(f.col(\"genres\").isNotNull(), f.split(\"genres\", \",\")).otherwise(f.array([]))) \\\n",
    "    .withColumn(\"firsts_genres\", f.array([f.col(\"genres_array\")[0], f.col(\"genres_array\")[1], f.col(\"genres_array\")[2]])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.fillna({\"year\": 2005.0,\n",
    "                            \"region_id\": 0,\n",
    "                            \"content_type\": 0,\n",
    "                            \"total_views\": 0,\n",
    "                            \"max_watch_time\": 0,\n",
    "                            \"min_watch_time\": 0,\n",
    "                            \"mean_time\": 0,\n",
    "                            \"cnt_is_live\": 0,\n",
    "                            \"live_ratio\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = model.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = assembler.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipeline_model.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = result.select(\"user_id\", \"item_id\", v_udf(\"probability\").alias(\"purchase\")).orderBy(\"user_id\", \"item_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2156840"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.toPandas().to_csv(\"lab03.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/31 14:06:22 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-master-1.newprolab.com:8020/user/alexander.zhukov/lab03.csv' to trash at: hdfs://spark-master-1.newprolab.com:8020/user/alexander.zhukov/.Trash/Current/user/alexander.zhukov/lab03.csv1667214382534\r\n",
      "rm: `/user/alexander.zhukov': Is a directory\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm lab03.csv /user/alexander.zhukov/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -put lab03.csv /user/alexander.zhukov/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
