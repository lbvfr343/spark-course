{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 --executor-memory 2g --executor-cores 1 --driver-memory 2g pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.feature import HashingTF, StringIndexer, VectorAssembler, IndexToString\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Transformer, Pipeline\n",
    "from pyspark.ml.param.shared import HasOutputCol, HasInputCol\n",
    "from pyspark import keyword_only\n",
    "\n",
    "from pyspark.sql.types import LongType, StringType, StructType, StructField, IntegerType, FloatType, ArrayType, LongType\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"lab4_5\") \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/labs/slaba04/gender_age_dataset.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(fields=[\n",
    "    StructField(\"gender\", StringType()),\n",
    "    StructField(\"age\", StringType()),\n",
    "    StructField(\"uid\", StringType()),\n",
    "    StructField(\"user_json\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(data_path, schema=schema, sep='\\t', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------------+--------------------+\n",
      "|gender|  age|                 uid|           user_json|\n",
      "+------+-----+--------------------+--------------------+\n",
      "|     F|18-24|d50192e5-c44e-4ae...|{\"visits\": [{\"url...|\n",
      "|     M|25-34|d502331d-621e-472...|{\"visits\": [{\"url...|\n",
      "+------+-----+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = StructType(fields=[\n",
    "    StructField(\"visits\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"url\", StringType(), True),\n",
    "            StructField(\"timestamp\", LongType(), True)        \n",
    "        ])\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df \\\n",
    "    .filter(f.col(\"gender\").isin([\"F\", \"M\"])) \\\n",
    "    .withColumn(\"concat_target\", f.concat(f.col(\"age\"), f.lit(\":\"), f.col(\"gender\"))) \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36138"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_urls = df \\\n",
    "    .withColumn(\"url_array\", f.from_json(f.col(\"user_json\"), json_schema)) \\\n",
    "    .withColumn(\"url\", f.explode(f.col(\"url_array.visits.url\"))) \\\n",
    "    .withColumn(\"parsed_url\", f.expr(\"parse_url(url, 'HOST')\")) \\\n",
    "    .groupBy(\"uid\") \\\n",
    "    .agg(f.collect_list(f.col(\"parsed_url\")).alias(\"clear_urls\"))\\\n",
    "    .select(\"uid\", \"clear_urls\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(clear_urls, on=\"uid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = {\"18-24:F\": 0,\n",
    "          \"25-34:F\": 1,\n",
    "          \"35-44:F\": 2,\n",
    "          \"45-54:F\": 3,\n",
    "          \">=55:F\": 4,\n",
    "          \"18-24:M\": 5,\n",
    "          \"25-34:M\": 6,\n",
    "          \"35-44:M\": 7,\n",
    "          \"45-54:M\": 8,\n",
    "          \">=55:M\": 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_target(s):\n",
    "      return encode[s]\n",
    "\n",
    "encode_target_udf = udf(encode_target, LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"target\", encode_target_udf(\"concat_target\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"clear_urls\", outputCol=\"features\", numFeatures=15000, binary=False)\n",
    "rf = RandomForestClassifier(labelCol=\"target\", featuresCol=hashingTF.getOutputCol(), numTrees=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "    hashingTF,\n",
    "    rf\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_kafka_params = {\n",
    "    \"kafka.bootstrap.servers\": 'spark-master-1.newprolab.com:6667',\n",
    "    \"subscribe\": \"input_alexander.zhukov\",\n",
    "    \"startingOffsets\": \"earliest\",\n",
    "    \"failOnDataLoss\": \"False\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_sdf = spark.read.format(\"kafka\").options(**read_kafka_params).load().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_schema = StructType([\n",
    "    StructField(\"uid\", StringType(), True),\n",
    "    StructField(\"visits\", StringType(), True)\n",
    "])\n",
    "\n",
    "visit_schema = ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"url\", StringType(), True),\n",
    "            StructField(\"timestamp\", LongType(), True)   \n",
    "        ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = kafka_sdf.select(f.col(\"value\").cast(\"string\").alias(\"value\"))\\\n",
    "         .select(f.from_json(f.col(\"value\"), event_schema).alias(\"event\"))\\\n",
    "         .select(f.col(\"event.uid\").alias(\"uid\"),\n",
    "                 f.from_json(f.col(\"event.visits\"), visit_schema).alias(\"url_array\")) \\\n",
    "         .select(\"uid\",\n",
    "                 f.col(\"url_array.url\").alias(\"url\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = clean_df.select(\"uid\",\n",
    "                           f.expr(\"\"\"transform(url, url -> parse_url(url, 'HOST'))\"\"\").alias(\"clear_urls\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = rf_model.transform(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode = {0: \"18-24:F\",\n",
    "          1: \"25-34:F\",\n",
    "          2: \"35-44:F\",\n",
    "          3: \"45-54:F\",\n",
    "          4: \">=55:F\",\n",
    "          5: \"18-24:M\",\n",
    "          6: \"25-34:M\",\n",
    "          7: \"35-44:M\",\n",
    "          8: \"45-54:M\",\n",
    "          9: \">=55:M\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_target(s):\n",
    "      return decode[s]\n",
    "\n",
    "decode_target_udf = udf(decode_target, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = prediction.withColumn(\"test\", f.split(decode_target_udf(\"prediction\"), \":\")) \\\n",
    "      .withColumn(\"gender\", f.col(\"test\")[1]) \\\n",
    "      .withColumn(\"age\", f.col(\"test\")[0]) \\\n",
    "      .select(\"uid\", \"gender\", \"age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----+\n",
      "|                 uid|gender|  age|\n",
      "+--------------------+------+-----+\n",
      "|bd7a30e1-a25d-4cb...|     M|25-34|\n",
      "|bd7a6f52-45db-49b...|     M|25-34|\n",
      "|bd7a7fd9-ab06-42f...|     M|25-34|\n",
      "|bd7c5d7a-0def-41d...|     M|25-34|\n",
      "|bd7e54a2-0215-45c...|     M|25-34|\n",
      "+--------------------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kill_all():\n",
    "    streams = SparkSession.builder.getOrCreate().streams.active\n",
    "    if streams:\n",
    "        for s in streams:\n",
    "            desc = s.lastProgress[\"sources\"][0][\"description\"]\n",
    "            s.stop()\n",
    "            print(\"Stopped {s}\".format(s=desc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_kafka_params = {\n",
    "    \"kafka.bootstrap.servers\": 'spark-master-1.newprolab.com:6667',\n",
    "    \"subscribe\": \"input_alexander.zhukov\",\n",
    "    \"startingOffsets\": \"latest\",\n",
    "    \"failOnDataLoss\": \"False\"\n",
    "}\n",
    "\n",
    "write_kafka_params = {\n",
    "   \"kafka.bootstrap.servers\": 'spark-master-1.newprolab.com:6667',\n",
    "   \"topic\": \"alexander.zhukov\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_schema = StructType([\n",
    "    StructField(\"uid\", StringType(), True),\n",
    "    StructField(\"visits\", StringType(), True)\n",
    "])\n",
    "\n",
    "visit_schema = ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"url\", StringType(), True),\n",
    "            StructField(\"timestamp\", LongType(), True)   \n",
    "        ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.readStream.format(\"kafka\").options(**read_kafka_params).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = sdf.select(f.col(\"value\").cast(\"string\").alias(\"value\"))\\\n",
    "         .select(f.from_json(f.col(\"value\"), event_schema).alias(\"event\"))\\\n",
    "         .select(f.col(\"event.uid\").alias(\"uid\"),\n",
    "                 f.from_json(f.col(\"event.visits\"), visit_schema).alias(\"url_array\")) \\\n",
    "         .select(\"uid\",\n",
    "                 f.col(\"url_array.url\").alias(\"url\")) \\\n",
    "         .select(\"uid\",\n",
    "                 f.expr(\"\"\"transform(url, url -> parse_url(url, 'HOST'))\"\"\").alias(\"clear_urls\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = rf_model.transform(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode = {0: \"18-24:F\",\n",
    "          1: \"25-34:F\",\n",
    "          2: \"35-44:F\",\n",
    "          3: \"45-54:F\",\n",
    "          4: \">=55:F\",\n",
    "          5: \"18-24:M\",\n",
    "          6: \"25-34:M\",\n",
    "          7: \"35-44:M\",\n",
    "          8: \"45-54:M\",\n",
    "          9: \">=55:M\"}\n",
    "\n",
    "def decode_target(s):\n",
    "      return decode[s]\n",
    "\n",
    "decode_target_udf = udf(decode_target, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = prediction.withColumn(\"test\", f.split(decode_target_udf(\"prediction\"), \":\")) \\\n",
    "      .withColumn(\"gender\", f.col(\"test\")[1]) \\\n",
    "      .withColumn(\"age\", f.col(\"test\")[0]) \\\n",
    "      .select(\"uid\", \"gender\", \"age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = f.to_json(f.struct(f.col(\"*\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = prediction.select(doc.alias(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f8c12fe16a0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.writeStream.format(\"kafka\").options(**write_kafka_params)\\\n",
    "    .option(\"checkpointLocation\", \"streaming/chk/chk_kafka\")\\\n",
    "    .outputMode(\"append\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
