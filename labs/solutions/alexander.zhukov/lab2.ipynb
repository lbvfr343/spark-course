{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запускаем Spark\n",
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, ArrayType, StringType, IntegerType, DoubleType\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, Normalizer, StopWordsRemover\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "import json\n",
    "import re\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"lab2\") \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_data_path = \"/labs/slaba02/DO_record_per_line.json\"\n",
    "submission_example_path = \"/share/submission-files/slaba02/lab02.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_recomendation = [[23126, u'en', u'Compass - powerful SASS library that makes your life easier'],\n",
    "                          [21617, u'en', u'Preparing for the AP* Computer Science A Exam \\u2014 Part 2'],\n",
    "                          [16627, u'es', u'Aprende Excel: Nivel Intermedio by Alfonso Rinsche'],\n",
    "                          [11556, u'es', u'Aprendizaje Colaborativo by UNID Universidad Interamericana para el Desarrollo'],\n",
    "                          [16704, u'ru', u'\\u041f\\u0440\\u043e\\u0433\\u0440\\u0430\\u043c\\u043c\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u043d\\u0430 Lazarus'],\n",
    "                          [13702, u'ru', u'\\u041c\\u0430\\u0442\\u0435\\u043c\\u0430\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u0430\\u044f \\u044d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u043a\\u0430']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_schema = StructType(fields=[\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"lang\", StringType()),\n",
    "    StructField(\"desc\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = spark.createDataFrame(target_recomendation, schema=target_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.json(courses_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_string(series):\n",
    "    series = series.str.lower()\n",
    "    regex = re.compile(u'[\\w\\d]{2,}', re.U)\n",
    "    words = series.str.findall(regex)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = {}\n",
    "for loc in [\"en\", \"es\", \"ru\"]:\n",
    "    df = data.filter(data.lang == loc)\n",
    "    \n",
    "    sim_cos = udf(lambda v, u: float(v.dot(u) / (v.norm(2) * u.norm(2))), DoubleType())\n",
    "    tokenizer_udf = pandas_udf(clear_string, ArrayType(StringType()))\n",
    "    remover = StopWordsRemover(inputCol=\"tokenized\", outputCol=\"filtered\", locale=loc)\n",
    "    ht = HashingTF(inputCol=remover.getOutputCol(), outputCol=\"tf\", numFeatures=10000, binary=True)\n",
    "    idf = IDF(inputCol=ht.getOutputCol(), outputCol=\"features\")\n",
    "    \n",
    "    df = df.withColumn(\"tokenized\", tokenizer_udf(\"desc\"))\n",
    "    \n",
    "    pipeline = Pipeline(stages=[\n",
    "    remover,\n",
    "    ht,\n",
    "    idf\n",
    "    ])\n",
    "    \n",
    "    pipe = pipeline.fit(df)\n",
    "    df = pipe.transform(df)\n",
    "    \n",
    "    ids = [row[0] for row in target_df.filter(f.col(\"lang\") == loc).collect()]\n",
    "    \n",
    "    df_target = df.filter(df.id.isin(ids))\n",
    "    \n",
    "    \n",
    "    merged_df = df.alias(\"i\").join(f.broadcast(df_target.alias(\"t\")), \n",
    "                       (df.lang == df_target.lang)\n",
    "                    &  (df.id != df_target.id)                       \n",
    "                       )\n",
    "    \n",
    "    merged_df = merged_df \\\n",
    "            .withColumn(\"cos\", sim_cos(f.col(\"t.features\"), f.col(\"i.features\"))) \\\n",
    "            .select(f.col(\"t.id\").alias(\"target\"),\n",
    "                    f.col(\"i.id\").alias(\"recommend\"),\n",
    "                    f.col(\"cos\")) \n",
    "    \n",
    "    merged_df = merged_df.fillna(value=0) \n",
    "    \n",
    "    windowSpec = Window.partitionBy(\"target\").orderBy(f.col(\"cos\").desc_nulls_last())\n",
    "    \n",
    "    result = merged_df.withColumn(\"rnk\", row_number().over(windowSpec)) \\\n",
    "                  .filter(f.col(\"rnk\") <= 10).collect()\n",
    "    \n",
    "    for row in result:\n",
    "        json_data.setdefault(str(row[0]), []).append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lab02.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json_file.write(json.dumps(json_data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lab02.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    res = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -put lab02.json /user/alexander.zhukov/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
