{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 6 --executor-memory 5g --executor-cores 4 --driver-memory 6g --conf spark.memory.storageFraction=0.3 --conf spark.executor.memoryOverhead=1g pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import Row\n",
    "import json\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .config(conf=conf)\n",
    "         .appName(\"lab 3\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "-rw-r--r--   3 hdfs hdfs   91066524 2022-01-06 18:46 /labs/slaba03/laba03_items.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs   29965581 2022-01-06 18:46 /labs/slaba03/laba03_test.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs   74949368 2022-01-06 18:46 /labs/slaba03/laba03_train.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs  871302535 2022-01-06 18:46 /labs/slaba03/laba03_views_programmes.csv\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /labs/slaba03/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = T.StructType([\n",
    "    StructField(\"user_id\", T.IntegerType(), True),\n",
    "    StructField(\"item_id\", T.IntegerType(), True),\n",
    "    StructField(\"purchase\", T.IntegerType(), True),\n",
    "])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, item_id: int, purchase: int]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = spark.read.format(\"csv\"). \\\n",
    "                      schema(schema). \\\n",
    "                      option(\"header\", True). \\\n",
    "                      load(\"/labs/slaba03/laba03_train.csv\")\n",
    "df_train.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- purchase: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+\n",
      "|user_id|item_id|purchase|\n",
      "+-------+-------+--------+\n",
      "|   1654|  74107|       0|\n",
      "|   1654|  89249|       0|\n",
      "|   1654|  99982|       0|\n",
      "|   1654|  89901|       0|\n",
      "|   1654| 100504|       0|\n",
      "+-------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_purchase_prob = df_train.groupBy(\"item_id\").agg(F.mean(\"purchase\").alias(\"item_purchase_prob\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_purchase_prob = item_purchase_prob.withColumn(\"item_purchase_prob\", F.col(\"item_purchase_prob\") * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+\n",
      "|summary|           item_id| item_purchase_prob|\n",
      "+-------+------------------+-------------------+\n",
      "|  count|              3704|               3704|\n",
      "|   mean| 66877.31425485961| 0.2165993075229777|\n",
      "| stddev|35242.702380266725|0.44829320161235064|\n",
      "|    min|               326|                0.0|\n",
      "|    25%|             60351|0.07267441860465117|\n",
      "|    50%|             79853|0.07429420505200594|\n",
      "|    75%|             93602| 0.1516300227445034|\n",
      "|    max|            104165|  7.153284671532846|\n",
      "+-------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "item_purchase_prob.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_features = df_train.groupBy(\"user_id\").agg(F.mean(\"purchase\").alias(\"user_purchase_prob\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_features = df_user_features.withColumn(\"user_purchase_prob\", F.col(\"user_purchase_prob\") * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------------+\n",
      "|summary|          user_id|  user_purchase_prob|\n",
      "+-------+-----------------+--------------------+\n",
      "|  count|             1941|                1941|\n",
      "|   mean|869672.3745492015|  0.2164949883315816|\n",
      "| stddev|60648.36081128855|  0.5807849230483886|\n",
      "|    min|             1654|                 0.0|\n",
      "|    25%|           846231|0.038387715930902115|\n",
      "|    50%|           885247|  0.0761904761904762|\n",
      "|    75%|           908588| 0.19282684149633628|\n",
      "|    max|           941450|  18.617021276595743|\n",
      "+-------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_user_features.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, user_purchase_prob: double]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user_features.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_features = F.broadcast(df_user_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.join(F.broadcast(item_purchase_prob), \"item_id\", \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = T.StructType([\n",
    "    StructField(\"user_id\", T.IntegerType(), True),\n",
    "    StructField(\"item_id\", T.IntegerType(), True),\n",
    "])\n",
    "\n",
    "df_test = spark.read.format(\"csv\"). \\\n",
    "                      schema(schema). \\\n",
    "                      option(\"header\", True). \\\n",
    "                      load(\"/labs/slaba03/laba03_test.csv\")\n",
    "df_test.cache()\n",
    "df_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|user_id|item_id|\n",
      "+-------+-------+\n",
      "|   1654|  94814|\n",
      "|   1654|  93629|\n",
      "|   1654|   9980|\n",
      "|   1654|  95099|\n",
      "|   1654|  11265|\n",
      "+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2156840"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.join(F.broadcast(item_purchase_prob), \"item_id\", \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2156840"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1941"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.select(\"user_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1941"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user_features.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- ts_start: integer (nullable = true)\n",
      " |-- ts_end: integer (nullable = true)\n",
      " |-- item_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = T.StructType([\n",
    "    StructField(\"user_id\", T.IntegerType(), True),\n",
    "    StructField(\"item_id\", T.IntegerType(), True),\n",
    "    StructField(\"ts_start\", T.IntegerType(), True),\n",
    "    StructField(\"ts_end\", T.IntegerType(), True),\n",
    "    StructField(\"item_type\", T.StringType(), True)\n",
    "])\n",
    "\n",
    "df_views = spark.read.format(\"csv\"). \\\n",
    "                      schema(schema). \\\n",
    "                      option(\"header\", True). \\\n",
    "                      load(\"/labs/slaba03/laba03_views_programmes.csv\")\n",
    "df_views.cache()\n",
    "df_views.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+----------+---------+\n",
      "|user_id|item_id|  ts_start|    ts_end|item_type|\n",
      "+-------+-------+----------+----------+---------+\n",
      "|      0|7101053|1491409931|1491411600|     live|\n",
      "|      0|7101054|1491412481|1491451571|     live|\n",
      "|      0|7101054|1491411640|1491412481|     live|\n",
      "|      0|6184414|1486191290|1486191640|     live|\n",
      "|    257|4436877|1490628499|1490630256|     live|\n",
      "+-------+-------+----------+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_views.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79385"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_views.select(\"user_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_views = df_views.withColumn(\"ts_duration\", F.col(\"ts_end\") - F.col(\"ts_start\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_views = df_views.withColumn(\"is_live\", (F.col(\"item_type\") == F.lit(\"live\")).cast(T.IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_views = df_views.drop(\"item_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_views = df_views.withColumn('ts_year', F.year(F.to_timestamp('ts_start')))\n",
    "df_views = df_views.withColumn('ts_month', F.month(F.to_timestamp('ts_start')))\n",
    "df_views = df_views.withColumn('ts_day', F.dayofmonth(F.to_timestamp('ts_start')))\n",
    "df_views = df_views.withColumn('ts_hour', F.hour(F.to_timestamp('ts_start')))\n",
    "df_views = df_views.withColumn('ts_minute', F.minute(F.to_timestamp('ts_start')))\n",
    "df_views = df_views.withColumn('ts_second', F.second(F.to_timestamp('ts_start')))\n",
    "df_views = df_views.withColumn('ts_week', F.weekofyear(F.to_timestamp('ts_start')))\n",
    "df_views = df_views.withColumn('ts_day_of_week', F.dayofweek(F.to_timestamp('ts_start')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_views = df_views.withColumn(\"ts_duration_min\", (F.col(\"ts_duration\") / 60).cast(T.IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_views = df_views.withColumn(\"ts_duration_hour\", (F.col(\"ts_duration\") / 3600).cast(T.IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+\n",
      "|ts_duration_hour|   count|\n",
      "+----------------+--------+\n",
      "|               0|15283245|\n",
      "|               1| 2918778|\n",
      "|               2|  987671|\n",
      "|               3|  629477|\n",
      "|               4|  655463|\n",
      "|               5|  101911|\n",
      "|               6|   62170|\n",
      "|               7|   42483|\n",
      "|               8|   36792|\n",
      "|               9|   23090|\n",
      "|              10|   18789|\n",
      "|              11|   15700|\n",
      "|              12|   13246|\n",
      "|              13|    9487|\n",
      "|              14|    7502|\n",
      "|              15|    6169|\n",
      "|              16|    5219|\n",
      "|              17|    4451|\n",
      "|              18|    4200|\n",
      "|              19|    4150|\n",
      "|              20|    3990|\n",
      "|              21|    4121|\n",
      "|              22|    3786|\n",
      "|              23|    3713|\n",
      "|              24|       3|\n",
      "|              26|       1|\n",
      "+----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_views.select(\"ts_duration_hour\").groupBy(\"ts_duration_hour\").count().sort(\"ts_duration_hour\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_views = df_views.drop(\"ts_start\", \"ts_end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|ts_year|   count|\n",
      "+-------+--------+\n",
      "|   2017|20845607|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_views.select(\"ts_year\").groupBy(\"ts_year\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|is_live|   count|\n",
      "+-------+--------+\n",
      "|      1|17704201|\n",
      "|      0| 3141406|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_views.select(\"is_live\").groupBy(\"is_live\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, item_id: int, ts_duration: int, is_live: int, ts_year: int, ts_month: int, ts_day: int, ts_hour: int, ts_minute: int, ts_second: int, ts_week: int, ts_day_of_week: int, ts_duration_min: int, ts_duration_hour: int]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_views.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------------+\n",
      "|summary|          user_id|views_user_live_prob|\n",
      "+-------+-----------------+--------------------+\n",
      "|  count|            79385|               79385|\n",
      "|   mean|856331.6499464634|  0.8907404781862069|\n",
      "| stddev|67407.01028043882| 0.20262261743898266|\n",
      "|    min|                0|                 0.0|\n",
      "|    25%|           817453|  0.8896551724137931|\n",
      "|    50%|           867425|  0.9919354838709677|\n",
      "|    75%|           909325|                 1.0|\n",
      "|    max|           941970|                 1.0|\n",
      "+-------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "views_user_live_prob = df_views.groupBy(\"user_id\").agg(F.mean(\"is_live\").alias(\"views_user_live_prob\"))\n",
    "views_user_live_prob.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_features = df_user_features.join(F.broadcast(views_user_live_prob), \"user_id\", \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+\n",
      "|summary|          user_id|views_user_mean_sec|\n",
      "+-------+-----------------+-------------------+\n",
      "|  count|            79385|              79385|\n",
      "|   mean|856331.6499464634|  4225.128783183427|\n",
      "| stddev|67407.01028043882| 3617.5646767763205|\n",
      "|    min|                0|              300.0|\n",
      "|    25%|           817462|           2169.252|\n",
      "|    50%|           867425|  3370.603550295858|\n",
      "|    75%|           909346|  5132.411764705882|\n",
      "|    max|           941970|            85916.0|\n",
      "+-------+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "views_user_mean_sec = df_views.groupBy(\"user_id\").agg(F.mean(\"ts_duration\").alias(\"views_user_mean_sec\"))\n",
    "df_user_features = df_user_features.join(F.broadcast(views_user_mean_sec), \"user_id\", \"left\")\n",
    "views_user_mean_sec.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------------+\n",
      "|summary|          user_id|views_user_mean_sec_live|\n",
      "+-------+-----------------+------------------------+\n",
      "|  count|            78943|                   78943|\n",
      "|   mean|856288.1839808469|       4487.200668822623|\n",
      "| stddev|67386.24789242803|      3895.9746899889105|\n",
      "|    min|                0|                   300.0|\n",
      "|    25%|           817432|       2179.160714285714|\n",
      "|    50%|           867371|       3551.238738738739|\n",
      "|    75%|           909275|       5567.416666666667|\n",
      "|    max|           941970|                 85916.0|\n",
      "+-------+-----------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "views_user_mean_sec_live = df_views.filter(F.col(\"is_live\") == 1).groupBy(\"user_id\").agg(F.mean(\"ts_duration\").alias(\"views_user_mean_sec_live\"))\n",
    "df_user_features = df_user_features.join(F.broadcast(views_user_mean_sec_live), \"user_id\", \"left\")\n",
    "views_user_mean_sec_live.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------------+\n",
      "|summary|          user_id|views_user_mean_sec_pvr|\n",
      "+-------+-----------------+-----------------------+\n",
      "|  count|            45777|                  45777|\n",
      "|   mean|852349.5113048038|       2574.39233133019|\n",
      "| stddev|66283.71685162785|     1997.4501158390106|\n",
      "|    min|             1654|                  300.0|\n",
      "|    25%|           812351|                 1614.0|\n",
      "|    50%|           861555|               2348.875|\n",
      "|    75%|           904010|                3121.75|\n",
      "|    max|           941872|                81744.0|\n",
      "+-------+-----------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "views_user_mean_sec_pvr = df_views.filter(F.col(\"is_live\") != 1).groupBy(\"user_id\").agg(F.mean(\"ts_duration\").alias(\"views_user_mean_sec_pvr\"))\n",
    "df_user_features = df_user_features.join(F.broadcast(views_user_mean_sec_pvr), \"user_id\", \"left\")\n",
    "views_user_mean_sec_pvr.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, user_purchase_prob: double, views_user_live_prob: double, views_user_mean_sec: double, views_user_mean_sec_live: double, views_user_mean_sec_pvr: double]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user_features.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(7):\n",
    "    df_views = df_views.withColumn(f\"ts_day_of_week_{d}\", (F.col(\"ts_day_of_week\") == d).cast(T.IntegerType()))\n",
    "    \n",
    "    df = df_views.filter(F.col(\"is_live\") == 1).groupBy(\"user_id\"). \\\n",
    "        agg(F.mean(f\"ts_day_of_week_{d}\").alias(f\"views_user_day_{d}_mean_live\"))\n",
    "    df_user_features = df_user_features.join(F.broadcast(df), \"user_id\", \"left\")\n",
    "    df = df_views.filter(F.col(\"is_live\") == 1).groupBy(\"user_id\"). \\\n",
    "        agg(F.mean(F.col(f\"ts_day_of_week_{d}\") * F.col(\"ts_duration\")).alias(f\"views_user_day_{d}_mean_sec_live\"))\n",
    "    df_user_features = df_user_features.join(F.broadcast(df), \"user_id\", \"left\")\n",
    "    \n",
    "    df = df_views.filter(F.col(\"is_live\") != 1).groupBy(\"user_id\"). \\\n",
    "        agg(F.mean(f\"ts_day_of_week_{d}\").alias(f\"views_user_day_{d}_mean_pvr\"))\n",
    "    df_user_features = df_user_features.join(F.broadcast(df), \"user_id\", \"left\")\n",
    "    df = df_views.filter(F.col(\"is_live\") != 1).groupBy(\"user_id\"). \\\n",
    "        agg(F.mean(F.col(f\"ts_day_of_week_{d}\") * F.col(\"ts_duration\")).alias(f\"views_user_day_{d}_mean_sec_pvr\"))\n",
    "    df_user_features = df_user_features.join(F.broadcast(df), \"user_id\", \"left\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h in range(24):\n",
    "    df_views = df_views.withColumn(f\"ts_hour_{h}\", (F.col(\"ts_hour\") == h).cast(T.IntegerType()))\n",
    "    \n",
    "    df = df_views.filter(F.col(\"is_live\") == 1).groupBy(\"user_id\"). \\\n",
    "        agg(F.mean(f\"ts_hour_{h}\").alias(f\"views_user_hour_{h}_mean_live\"))\n",
    "    df_user_features = df_user_features.join(F.broadcast(df), \"user_id\", \"left\")\n",
    "    df = df_views.filter(F.col(\"is_live\") == 1).groupBy(\"user_id\"). \\\n",
    "        agg(F.mean(F.col(f\"ts_hour_{h}\") * F.col(\"ts_duration\")).alias(f\"views_user_hour_{h}_mean_sec_live\"))\n",
    "    df_user_features = df_user_features.join(F.broadcast(df), \"user_id\", \"left\")\n",
    "    \n",
    "    df = df_views.filter(F.col(\"is_live\") != 1).groupBy(\"user_id\"). \\\n",
    "        agg(F.mean(f\"ts_hour_{h}\").alias(f\"views_user_hour_{h}_mean_pvr\"))\n",
    "    df_user_features = df_user_features.join(F.broadcast(df), \"user_id\", \"left\")\n",
    "    df = df_views.filter(F.col(\"is_live\") != 1).groupBy(\"user_id\"). \\\n",
    "        agg(F.mean(F.col(f\"ts_hour_{h}\") * F.col(\"ts_duration\")).alias(f\"views_user_hour_{h}_mean_sec_pvr\"))\n",
    "    df_user_features = df_user_features.join(F.broadcast(df), \"user_id\", \"left\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- channel_id: integer (nullable = true)\n",
      " |-- datetime_availability_start: timestamp (nullable = true)\n",
      " |-- datetime_availability_stop: timestamp (nullable = true)\n",
      " |-- datetime_show_start: timestamp (nullable = true)\n",
      " |-- datetime_show_stop: timestamp (nullable = true)\n",
      " |-- content_type: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      " |-- region_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = T.StructType([\n",
    "    StructField(\"item_id\", T.IntegerType(), True),\n",
    "    StructField(\"channel_id\", T.IntegerType(), True),\n",
    "    StructField(\"datetime_availability_start\", T.TimestampType(), True),\n",
    "    StructField(\"datetime_availability_stop\", T.TimestampType(), True),\n",
    "    StructField(\"datetime_show_start\", T.TimestampType(), True),\n",
    "    StructField(\"datetime_show_stop\", T.TimestampType(), True),\n",
    "    StructField(\"content_type\", T.IntegerType(), True),\n",
    "    StructField(\"title\", T.StringType(), True),\n",
    "    StructField(\"year\", T.FloatType(), True),\n",
    "    StructField(\"genres\", T.StringType(), True),\n",
    "    StructField(\"region_id\", T.IntegerType(), True)\n",
    "])\n",
    "\n",
    "df_items = spark.read.format(\"csv\"). \\\n",
    "                      schema(schema). \\\n",
    "                      option(\"header\", True). \\\n",
    "                      option(\"sep\", \"\\t\"). \\\n",
    "                      load(\"/labs/slaba03/laba03_items.csv\")\n",
    "df_items = df_items.withColumn(\"year\", F.col(\"year\").cast(T.IntegerType()))\n",
    "\n",
    "df_items.cache()\n",
    "df_items.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------------------------+--------------------------+-------------------+------------------+------------+--------------------+----+-------+---------+\n",
      "|item_id|channel_id|datetime_availability_start|datetime_availability_stop|datetime_show_start|datetime_show_stop|content_type|               title|year| genres|region_id|\n",
      "+-------+----------+---------------------------+--------------------------+-------------------+------------------+------------+--------------------+----+-------+---------+\n",
      "|  65667|      null|        1970-01-01 03:00:00|       2018-01-01 03:00:00|               null|              null|           1|на пробах только ...|2013|Эротика|     null|\n",
      "|  65669|      null|        1970-01-01 03:00:00|       2018-01-01 03:00:00|               null|              null|           1|скуби ду: эротиче...|2011|Эротика|     null|\n",
      "|  65668|      null|        1970-01-01 03:00:00|       2018-01-01 03:00:00|               null|              null|           1|горячие девочки д...|2011|Эротика|     null|\n",
      "|  65671|      null|        1970-01-01 03:00:00|       2018-01-01 03:00:00|               null|              null|           1|соблазнительницы ...|2011|Эротика|     null|\n",
      "|  65670|      null|        1970-01-01 03:00:00|       2018-01-01 03:00:00|               null|              null|           1|секретные секс-ма...|2010|Эротика|     null|\n",
      "+-------+----------+---------------------------+--------------------------+-------------------+------------------+------------+--------------------+----+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_items.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items = df_items.withColumn(\"genres_list\", F.split(F.col(\"genres\"), \",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = [x[0] for x in df_items.select(F.explode(\"genres_list\")).distinct().collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ужасы',\n",
       " 'Анимация',\n",
       " 'Мелодрама',\n",
       " 'Исторические',\n",
       " 'Мистические',\n",
       " 'Мультфильмы',\n",
       " 'Триллер',\n",
       " 'Документальный',\n",
       " 'Хочу всё знать',\n",
       " 'Зарубежные',\n",
       " 'Приключение',\n",
       " 'Семейный',\n",
       " 'Союзмультфильм',\n",
       " ' сказка',\n",
       " 'Западные мультфильмы',\n",
       " 'Фильмы',\n",
       " 'Юмористические',\n",
       " 'Детские песни',\n",
       " 'Развлекательные',\n",
       " 'Игры',\n",
       " 'Передачи',\n",
       " 'Короткометражки',\n",
       " 'Русские мультфильмы',\n",
       " 'Боевики',\n",
       " 'Мелодрамы',\n",
       " 'Мультфильм',\n",
       " 'Для детей',\n",
       " 'Эротика',\n",
       " 'Аниме',\n",
       " 'Фантастические',\n",
       " 'Короткометражные',\n",
       " 'Наши',\n",
       " 'Военные',\n",
       " 'Спортивные',\n",
       " 'Детские',\n",
       " 'Семейные',\n",
       " 'Советское кино',\n",
       " 'Драмы',\n",
       " 'Комедия',\n",
       " 'Криминал',\n",
       " 'О здоровье',\n",
       " 'Мюзиклы',\n",
       " 'Для взрослых',\n",
       " 'Приключения',\n",
       " 'Фильмы в 3D',\n",
       " 'Военный',\n",
       " 'Романтические',\n",
       " 'Познавательные',\n",
       " 'Спорт',\n",
       " 'Охота и рыбалка',\n",
       " 'Фильмы-спектакли',\n",
       " 'Комедии',\n",
       " 'Полнометражные',\n",
       " 'Сериалы',\n",
       " 'Для всей семьи',\n",
       " 'Мультфильмы в 3D',\n",
       " 'Экранизации',\n",
       " 'Документальные',\n",
       " 'Арт-хаус',\n",
       " 'Для самых маленьких',\n",
       " 'Боевик',\n",
       " 'Развивающие',\n",
       " 'Фантастика',\n",
       " 'Биография',\n",
       " 'Сказки',\n",
       " 'Мультсериалы',\n",
       " 'Фэнтези',\n",
       " 'Русские',\n",
       " 'Про животных',\n",
       " 'Кулинария',\n",
       " 'Музыкальные',\n",
       " 'Советские',\n",
       " 'Артхаус',\n",
       " 'Вестерн',\n",
       " 'Научная фантастика',\n",
       " 'Драма',\n",
       " 'Музыкальный',\n",
       " 'Детективы',\n",
       " 'Реалити-шоу',\n",
       " 'Триллеры',\n",
       " 'Видеоигры',\n",
       " 'Прочие',\n",
       " 'Исторический']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df_items.select(\"genres\").distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [x.genres for x in t if x.genres and \"сказка\" in x.genres]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Мелодрама,Фэнтези, сказка,Криминал,Триллер,Драма',\n",
       " 'Ужасы,Боевик,Фэнтези, сказка,Комедия',\n",
       " 'Ужасы,Фэнтези, сказка,Драма']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres.remove(\" сказка\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items = df_items.select(\"*\",\n",
    "    *[F.array_contains(\"genres_list\", x).alias(f\"genre_{x}\").cast(\"integer\") for x in genres]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items = df_items.drop(\"genres\", \"genres_list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items = df_items.withColumn(\"datetime_availability_start\", F.to_date(\"datetime_availability_start\"))\n",
    "df_items = df_items.withColumn(\"datetime_availability_stop\", F.to_date(\"datetime_availability_stop\"))\n",
    "df_items = df_items.withColumn(\"datetime_show_start\", F.to_date(\"datetime_show_start\"))\n",
    "df_items = df_items.withColumn(\"datetime_show_stop\", F.to_date(\"datetime_show_stop\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+--------------------------+-------------------+------------------+\n",
      "|datetime_availability_start|datetime_availability_stop|datetime_show_start|datetime_show_stop|\n",
      "+---------------------------+--------------------------+-------------------+------------------+\n",
      "+---------------------------+--------------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_items.select(\"datetime_availability_start\", \"datetime_availability_stop\", \"datetime_show_start\", \"datetime_show_stop\"). \\\n",
    "where(F.col(\"datetime_show_stop\").isNotNull() | F.col(\"datetime_show_start\").isNotNull()).show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items = df_items.drop(\"datetime_show_start\", \"datetime_show_stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|channel_id|\n",
      "+----------+\n",
      "|      null|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_items.select(\"channel_id\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|content_type| count|\n",
      "+------------+------+\n",
      "|        null|631864|\n",
      "|           1|  3704|\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_items.select(\"content_type\").groupBy(\"content_type\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|region_id|\n",
      "+---------+\n",
      "|     null|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_items.select(\"region_id\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+------+\n",
      "|datetime_availability_start| count|\n",
      "+---------------------------+------+\n",
      "|                       null|631864|\n",
      "|                 2017-01-01|     2|\n",
      "|                 1970-01-01|  3702|\n",
      "+---------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_items.select(\"datetime_availability_start\").groupBy(\"datetime_availability_start\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+------+\n",
      "|datetime_availability_stop| count|\n",
      "+--------------------------+------+\n",
      "|                      null|631864|\n",
      "|                2017-08-21|     1|\n",
      "|                2018-12-31|    10|\n",
      "|                2100-01-01|  3599|\n",
      "|                2018-01-01|    94|\n",
      "+--------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_items.select(\"datetime_availability_stop\").groupBy(\"datetime_availability_stop\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items = df_items.drop(\"channel_id\", \"region_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items = df_items.withColumn(\"is_limited\", (F.col(\"datetime_availability_stop\") < F.lit(\"2100-01-01\")).cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+--------------------------+------------+----------+------+\n",
      "|datetime_availability_start|datetime_availability_stop|content_type|is_limited| count|\n",
      "+---------------------------+--------------------------+------------+----------+------+\n",
      "|                 2017-01-01|                2018-01-01|           1|         1|     2|\n",
      "|                 1970-01-01|                2018-01-01|           1|         1|    92|\n",
      "|                       null|                      null|        null|      null|631864|\n",
      "|                 1970-01-01|                2018-12-31|           1|         1|    10|\n",
      "|                 1970-01-01|                2017-08-21|           1|         1|     1|\n",
      "|                 1970-01-01|                2100-01-01|           1|         0|  3599|\n",
      "+---------------------------+--------------------------+------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_items.select(\"datetime_availability_start\", \"datetime_availability_stop\", \"content_type\", \"is_limited\").groupBy(\"datetime_availability_start\", \"datetime_availability_stop\", \"content_type\", \"is_limited\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items = df_items.filter(F.col(\"content_type\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|genre_Эротика|count|\n",
      "+-------------+-----+\n",
      "|         null|   33|\n",
      "|            1|  124|\n",
      "|            0| 3547|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_items.select(\"genre_Эротика\").groupBy(\"genre_Эротика\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items = df_items.na.fill(value=0,subset=[col for col in df_items.columns if col.startswith(\"genre_\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------------+--------------------------+------------+--------------------+----+-----------+--------------+---------------+------------------+-----------------+-----------------+-------------+--------------------+--------------------+----------------+-----------------+--------------+--------------------+--------------------------+------------+--------------------+-------------------+---------------------+----------+--------------+---------------------+-------------------------+-------------+---------------+----------------+---------------+-------------+-----------+--------------------+----------------------+----------+-------------+----------------+-------------+--------------+--------------------+-----------+-------------+--------------+----------------+-------------+------------------+-----------------+-----------------+-------------+-------------------+--------------------+-----------+---------------------+----------------------+-------------+--------------------+-------------+--------------------+----------------------+-----------------+--------------------+--------------+-------------------------+------------+-----------------+----------------+---------------+------------+------------------+-------------+-------------+------------------+---------------+-----------------+---------------+-------------+-------------+------------------------+-----------+-----------------+---------------+-----------------+--------------+---------------+------------+------------------+----------+\n",
      "|item_id|datetime_availability_start|datetime_availability_stop|content_type|               title|year|genre_Ужасы|genre_Анимация|genre_Мелодрама|genre_Исторические|genre_Мистические|genre_Мультфильмы|genre_Триллер|genre_Документальный|genre_Хочу всё знать|genre_Зарубежные|genre_Приключение|genre_Семейный|genre_Союзмультфильм|genre_Западные мультфильмы|genre_Фильмы|genre_Юмористические|genre_Детские песни|genre_Развлекательные|genre_Игры|genre_Передачи|genre_Короткометражки|genre_Русские мультфильмы|genre_Боевики|genre_Мелодрамы|genre_Мультфильм|genre_Для детей|genre_Эротика|genre_Аниме|genre_Фантастические|genre_Короткометражные|genre_Наши|genre_Военные|genre_Спортивные|genre_Детские|genre_Семейные|genre_Советское кино|genre_Драмы|genre_Комедия|genre_Криминал|genre_О здоровье|genre_Мюзиклы|genre_Для взрослых|genre_Приключения|genre_Фильмы в 3D|genre_Военный|genre_Романтические|genre_Познавательные|genre_Спорт|genre_Охота и рыбалка|genre_Фильмы-спектакли|genre_Комедии|genre_Полнометражные|genre_Сериалы|genre_Для всей семьи|genre_Мультфильмы в 3D|genre_Экранизации|genre_Документальные|genre_Арт-хаус|genre_Для самых маленьких|genre_Боевик|genre_Развивающие|genre_Фантастика|genre_Биография|genre_Сказки|genre_Мультсериалы|genre_Фэнтези|genre_Русские|genre_Про животных|genre_Кулинария|genre_Музыкальные|genre_Советские|genre_Артхаус|genre_Вестерн|genre_Научная фантастика|genre_Драма|genre_Музыкальный|genre_Детективы|genre_Реалити-шоу|genre_Триллеры|genre_Видеоигры|genre_Прочие|genre_Исторический|is_limited|\n",
      "+-------+---------------------------+--------------------------+------------+--------------------+----+-----------+--------------+---------------+------------------+-----------------+-----------------+-------------+--------------------+--------------------+----------------+-----------------+--------------+--------------------+--------------------------+------------+--------------------+-------------------+---------------------+----------+--------------+---------------------+-------------------------+-------------+---------------+----------------+---------------+-------------+-----------+--------------------+----------------------+----------+-------------+----------------+-------------+--------------+--------------------+-----------+-------------+--------------+----------------+-------------+------------------+-----------------+-----------------+-------------+-------------------+--------------------+-----------+---------------------+----------------------+-------------+--------------------+-------------+--------------------+----------------------+-----------------+--------------------+--------------+-------------------------+------------+-----------------+----------------+---------------+------------+------------------+-------------+-------------+------------------+---------------+-----------------+---------------+-------------+-------------+------------------------+-----------+-----------------+---------------+-----------------+--------------+---------------+------------+------------------+----------+\n",
      "|   8544|                 1970-01-01|                2100-01-01|           1|                30-е|null|          0|             0|              0|                 0|                0|                0|            0|                   0|                   0|               0|                0|             0|                   0|                         0|           0|                   0|                  0|                    0|         0|             0|                    0|                        0|            0|              0|               0|              0|            0|          0|                   0|                     0|         0|            0|               0|            0|             0|                   0|          0|            0|             0|               0|            0|                 0|                0|                0|            0|                  0|                   0|          0|                    0|                     0|            0|                   0|            0|                   0|                     0|                0|                   0|             0|                        0|           0|                0|               0|              0|           0|                 0|            0|            0|                 0|              0|                0|              0|            0|            0|                       0|          0|                0|              0|                0|             0|              0|           1|                 0|         0|\n",
      "|  72544|                 1970-01-01|                2100-01-01|           1|     городские герои|null|          0|             0|              0|                 0|                0|                0|            0|                   0|                   0|               1|                0|             0|                   0|                         0|           0|                   0|                  0|                    0|         0|             0|                    0|                        0|            0|              0|               0|              1|            0|          0|                   0|                     0|         0|            0|               0|            0|             0|                   0|          0|            0|             0|               0|            0|                 0|                0|                0|            0|                  0|                   0|          0|                    0|                     0|            0|                   0|            0|                   1|                     0|                0|                   0|             0|                        0|           0|                0|               0|              0|           0|                 0|            0|            0|                 0|              0|                0|              0|            0|            0|                       0|          0|                0|              0|                0|             0|              0|           0|                 0|         0|\n",
      "|  95141|                 1970-01-01|                2100-01-01|           1|   современный роман|null|          0|             0|              0|                 0|                0|                0|            0|                   0|                   0|               0|                0|             0|                   0|                         0|           0|                   0|                  0|                    0|         0|             0|                    0|                        0|            0|              0|               0|              0|            1|          0|                   0|                     0|         0|            0|               0|            0|             0|                   0|          0|            0|             0|               0|            0|                 0|                0|                0|            0|                  0|                   0|          0|                    0|                     0|            0|                   0|            0|                   0|                     0|                0|                   0|             0|                        0|           0|                0|               0|              0|           0|                 0|            0|            0|                 0|              0|                0|              0|            0|            0|                       0|          0|                0|              0|                0|             0|              0|           0|                 0|         0|\n",
      "| 103377|                 1970-01-01|                2018-01-01|           1|big buck bunny 1080p|null|          0|             0|              0|                 0|                0|                0|            0|                   0|                   0|               0|                0|             0|                   0|                         0|           0|                   0|                  0|                    0|         0|             0|                    0|                        0|            0|              0|               0|              0|            0|          0|                   0|                     0|         0|            0|               0|            0|             0|                   0|          0|            0|             0|               0|            0|                 0|                0|                0|            0|                  0|                   0|          0|                    0|                     0|            0|                   0|            0|                   0|                     0|                0|                   0|             0|                        0|           0|                0|               0|              0|           0|                 0|            0|            0|                 0|              0|                0|              0|            0|            0|                       0|          0|                0|              0|                0|             0|              0|           0|                 0|         1|\n",
      "+-------+---------------------------+--------------------------+------------+--------------------+----+-----------+--------------+---------------+------------------+-----------------+-----------------+-------------+--------------------+--------------------+----------------+-----------------+--------------+--------------------+--------------------------+------------+--------------------+-------------------+---------------------+----------+--------------+---------------------+-------------------------+-------------+---------------+----------------+---------------+-------------+-----------+--------------------+----------------------+----------+-------------+----------------+-------------+--------------+--------------------+-----------+-------------+--------------+----------------+-------------+------------------+-----------------+-----------------+-------------+-------------------+--------------------+-----------+---------------------+----------------------+-------------+--------------------+-------------+--------------------+----------------------+-----------------+--------------------+--------------+-------------------------+------------+-----------------+----------------+---------------+------------+------------------+-------------+-------------+------------------+---------------+-----------------+---------------+-------------+-------------+------------------------+-----------+-----------------+---------------+-----------------+--------------+---------------+------------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_items.filter(F.col(\"year\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items = df_items.na.fill(value=2000, subset=[\"year\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items = df_items.drop(\"title\", \"datetime_availability_start\", \"datetime_availability_stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.join(F.broadcast(df_user_features), \"user_id\", \"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.join(F.broadcast(df_user_features), \"user_id\", \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2156840"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5032624 218\n"
     ]
    }
   ],
   "source": [
    "df_train_items = df_train.join(df_items, \"item_id\", \"left\")\n",
    "df_train_items.cache()\n",
    "print(df_train_items.count(), len(df_train_items.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[item_id: int, user_id: int, item_purchase_prob: double, user_purchase_prob: double, views_user_live_prob: double, views_user_mean_sec: double, views_user_mean_sec_live: double, views_user_mean_sec_pvr: double, views_user_day_0_mean_live: double, views_user_day_0_mean_sec_live: double, views_user_day_0_mean_pvr: double, views_user_day_0_mean_sec_pvr: double, views_user_day_1_mean_live: double, views_user_day_1_mean_sec_live: double, views_user_day_1_mean_pvr: double, views_user_day_1_mean_sec_pvr: double, views_user_day_2_mean_live: double, views_user_day_2_mean_sec_live: double, views_user_day_2_mean_pvr: double, views_user_day_2_mean_sec_pvr: double, views_user_day_3_mean_live: double, views_user_day_3_mean_sec_live: double, views_user_day_3_mean_pvr: double, views_user_day_3_mean_sec_pvr: double, views_user_day_4_mean_live: double, views_user_day_4_mean_sec_live: double, views_user_day_4_mean_pvr: double, views_user_day_4_mean_sec_pvr: double, views_user_day_5_mean_live: double, views_user_day_5_mean_sec_live: double, views_user_day_5_mean_pvr: double, views_user_day_5_mean_sec_pvr: double, views_user_day_6_mean_live: double, views_user_day_6_mean_sec_live: double, views_user_day_6_mean_pvr: double, views_user_day_6_mean_sec_pvr: double, views_user_hour_0_mean_live: double, views_user_hour_0_mean_sec_live: double, views_user_hour_0_mean_pvr: double, views_user_hour_0_mean_sec_pvr: double, views_user_hour_1_mean_live: double, views_user_hour_1_mean_sec_live: double, views_user_hour_1_mean_pvr: double, views_user_hour_1_mean_sec_pvr: double, views_user_hour_2_mean_live: double, views_user_hour_2_mean_sec_live: double, views_user_hour_2_mean_pvr: double, views_user_hour_2_mean_sec_pvr: double, views_user_hour_3_mean_live: double, views_user_hour_3_mean_sec_live: double, views_user_hour_3_mean_pvr: double, views_user_hour_3_mean_sec_pvr: double, views_user_hour_4_mean_live: double, views_user_hour_4_mean_sec_live: double, views_user_hour_4_mean_pvr: double, views_user_hour_4_mean_sec_pvr: double, views_user_hour_5_mean_live: double, views_user_hour_5_mean_sec_live: double, views_user_hour_5_mean_pvr: double, views_user_hour_5_mean_sec_pvr: double, views_user_hour_6_mean_live: double, views_user_hour_6_mean_sec_live: double, views_user_hour_6_mean_pvr: double, views_user_hour_6_mean_sec_pvr: double, views_user_hour_7_mean_live: double, views_user_hour_7_mean_sec_live: double, views_user_hour_7_mean_pvr: double, views_user_hour_7_mean_sec_pvr: double, views_user_hour_8_mean_live: double, views_user_hour_8_mean_sec_live: double, views_user_hour_8_mean_pvr: double, views_user_hour_8_mean_sec_pvr: double, views_user_hour_9_mean_live: double, views_user_hour_9_mean_sec_live: double, views_user_hour_9_mean_pvr: double, views_user_hour_9_mean_sec_pvr: double, views_user_hour_10_mean_live: double, views_user_hour_10_mean_sec_live: double, views_user_hour_10_mean_pvr: double, views_user_hour_10_mean_sec_pvr: double, views_user_hour_11_mean_live: double, views_user_hour_11_mean_sec_live: double, views_user_hour_11_mean_pvr: double, views_user_hour_11_mean_sec_pvr: double, views_user_hour_12_mean_live: double, views_user_hour_12_mean_sec_live: double, views_user_hour_12_mean_pvr: double, views_user_hour_12_mean_sec_pvr: double, views_user_hour_13_mean_live: double, views_user_hour_13_mean_sec_live: double, views_user_hour_13_mean_pvr: double, views_user_hour_13_mean_sec_pvr: double, views_user_hour_14_mean_live: double, views_user_hour_14_mean_sec_live: double, views_user_hour_14_mean_pvr: double, views_user_hour_14_mean_sec_pvr: double, views_user_hour_15_mean_live: double, views_user_hour_15_mean_sec_live: double, views_user_hour_15_mean_pvr: double, views_user_hour_15_mean_sec_pvr: double, views_user_hour_16_mean_live: double, views_user_hour_16_mean_sec_live: double, views_user_hour_16_mean_pvr: double, views_user_hour_16_mean_sec_pvr: double, views_user_hour_17_mean_live: double, views_user_hour_17_mean_sec_live: double, views_user_hour_17_mean_pvr: double, views_user_hour_17_mean_sec_pvr: double, views_user_hour_18_mean_live: double, views_user_hour_18_mean_sec_live: double, views_user_hour_18_mean_pvr: double, views_user_hour_18_mean_sec_pvr: double, views_user_hour_19_mean_live: double, views_user_hour_19_mean_sec_live: double, views_user_hour_19_mean_pvr: double, views_user_hour_19_mean_sec_pvr: double, views_user_hour_20_mean_live: double, views_user_hour_20_mean_sec_live: double, views_user_hour_20_mean_pvr: double, views_user_hour_20_mean_sec_pvr: double, views_user_hour_21_mean_live: double, views_user_hour_21_mean_sec_live: double, views_user_hour_21_mean_pvr: double, views_user_hour_21_mean_sec_pvr: double, views_user_hour_22_mean_live: double, views_user_hour_22_mean_sec_live: double, views_user_hour_22_mean_pvr: double, views_user_hour_22_mean_sec_pvr: double, views_user_hour_23_mean_live: double, views_user_hour_23_mean_sec_live: double, views_user_hour_23_mean_pvr: double, views_user_hour_23_mean_sec_pvr: double, content_type: int, year: int, genre_Ужасы: int, genre_Анимация: int, genre_Мелодрама: int, genre_Исторические: int, genre_Мистические: int, genre_Мультфильмы: int, genre_Триллер: int, genre_Документальный: int, genre_Хочу всё знать: int, genre_Зарубежные: int, genre_Приключение: int, genre_Семейный: int, genre_Союзмультфильм: int, genre_Западные мультфильмы: int, genre_Фильмы: int, genre_Юмористические: int, genre_Детские песни: int, genre_Развлекательные: int, genre_Игры: int, genre_Передачи: int, genre_Короткометражки: int, genre_Русские мультфильмы: int, genre_Боевики: int, genre_Мелодрамы: int, genre_Мультфильм: int, genre_Для детей: int, genre_Эротика: int, genre_Аниме: int, genre_Фантастические: int, genre_Короткометражные: int, genre_Наши: int, genre_Военные: int, genre_Спортивные: int, genre_Детские: int, genre_Семейные: int, genre_Советское кино: int, genre_Драмы: int, genre_Комедия: int, genre_Криминал: int, genre_О здоровье: int, genre_Мюзиклы: int, genre_Для взрослых: int, genre_Приключения: int, genre_Фильмы в 3D: int, genre_Военный: int, genre_Романтические: int, genre_Познавательные: int, genre_Спорт: int, genre_Охота и рыбалка: int, genre_Фильмы-спектакли: int, genre_Комедии: int, genre_Полнометражные: int, genre_Сериалы: int, genre_Для всей семьи: int, genre_Мультфильмы в 3D: int, genre_Экранизации: int, genre_Документальные: int, genre_Арт-хаус: int, genre_Для самых маленьких: int, genre_Боевик: int, genre_Развивающие: int, genre_Фантастика: int, genre_Биография: int, genre_Сказки: int, genre_Мультсериалы: int, genre_Фэнтези: int, genre_Русские: int, genre_Про животных: int, genre_Кулинария: int, genre_Музыкальные: int, genre_Советские: int, genre_Артхаус: int, genre_Вестерн: int, genre_Научная фантастика: int, genre_Драма: int, genre_Музыкальный: int, genre_Детективы: int, genre_Реалити-шоу: int, genre_Триллеры: int, genre_Видеоигры: int, genre_Прочие: int, genre_Исторический: int, is_limited: int]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_items = df_test.join(df_items, \"item_id\", \"left\")\n",
    "df_test_items.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_train_items.columns:\n",
    "    df_train_items = df_train_items.withColumnRenamed(col, col.replace(\" \", \"_\"))\n",
    "    \n",
    "for col in df_test_items.columns:\n",
    "    df_test_items = df_test_items.withColumnRenamed(col, col.replace(\" \", \"_\"))\n",
    "    \n",
    "df_train_items.write.parquet(\"df_train_items\", mode=\"overwrite\")\n",
    "df_test_items.write.parquet(\"df_test_items\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_items = spark.read.parquet(\"df_train_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_items = spark.read.parquet(\"df_test_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[x for x in df_train_items.columns if x != \"purchase\"],\n",
    "    outputCol='features', handleInvalid=\"keep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = assembler.transform(df_train_items).select(\"purchase\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[purchase: int, features: vector]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier, GBTClassificationModel\n",
    "\n",
    "iteration = 50\n",
    "model = GBTClassificationModel.load(f\"model_{iteration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4:03:09.831484\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "st = datetime.now()\n",
    "\n",
    "iteration = 50\n",
    "gbt = GBTClassifier(labelCol=\"purchase\",\n",
    "                    featuresCol=\"features\", maxIter=iteration, seed=56456)\n",
    "\n",
    "model = gbt.fit(train_data)\n",
    "\n",
    "print(datetime.now() - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2586.fit.\n: org.apache.spark.SparkException: Job 1718 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:954)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:952)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:952)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2164)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2077)\n\tat org.apache.spark.SparkContext$$anonfun$stop$6.apply$mcV$sp(SparkContext.scala:1949)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1948)\n\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:121)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:743)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:742)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:742)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:567)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:201)\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$2.apply(DecisionTreeRegressor.scala:129)\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$2.apply(DecisionTreeRegressor.scala:124)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:124)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.boost(GradientBoostedTrees.scala:330)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.run(GradientBoostedTrees.scala:55)\n\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$train$1.apply(GBTClassifier.scala:206)\n\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$train$1.apply(GBTClassifier.scala:156)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:156)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:58)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-181-c5789a1fd855>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrossval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parallelFitTasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meva\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubModel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m                 \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnFolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/bd9/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m     \u001b[0m__next__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m                    \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/bd9/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parallelFitTasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meva\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubModel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m                 \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnFolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36msingleTask\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msingleTask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelIter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meva\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcollectSubModel\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No models remaining.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitSingleModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfitSingleModel\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfitSingleModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparamMaps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_FitMultipleIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfitSingleModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparamMaps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2586.fit.\n: org.apache.spark.SparkException: Job 1718 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:954)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:952)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:952)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2164)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2077)\n\tat org.apache.spark.SparkContext$$anonfun$stop$6.apply$mcV$sp(SparkContext.scala:1949)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1948)\n\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:121)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:743)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:742)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:742)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:567)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:201)\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$2.apply(DecisionTreeRegressor.scala:129)\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$2.apply(DecisionTreeRegressor.scala:124)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:124)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.boost(GradientBoostedTrees.scala:330)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.run(GradientBoostedTrees.scala:55)\n\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$train$1.apply(GBTClassifier.scala:206)\n\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$train$1.apply(GBTClassifier.scala:156)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:156)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:58)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "st = datetime.now()\n",
    "\n",
    "\n",
    "iteration = 50\n",
    "gbt = GBTClassifier(labelCol=\"purchase\",\n",
    "                    featuresCol=\"features\", maxIter=iteration, seed=56456)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"purchase\")\n",
    "\n",
    "# no parameter search\n",
    "paramGrid = ParamGridBuilder().build()\n",
    "\n",
    "# 6-fold cross validation\n",
    "crossval = CrossValidator(\n",
    "    estimator=gbt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=6)\n",
    "\n",
    "\n",
    "model = crossval.fit(train_data)\n",
    "\n",
    "print(datetime.now() - st)\n",
    "\n",
    "print(\"trained GBT classifier:%s\" % model)\n",
    "\n",
    "# display CV score\n",
    "auc_roc = model.avgMetrics[0]\n",
    "print(\"AUC ROC = %g\" % auc_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write().overwrite().save(f\"model_{iteration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill Null in dt_test_items with for each value\n",
    "\n",
    "for col in dt_test_items.columns:\n",
    "    c = dt_test_items.select(col).filter(F.col(col).isNull()).count()\n",
    "    \n",
    "    if c > 0:\n",
    "        print(col, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "views_user_live_prob 3\n",
      "views_user_mean_sec 3\n",
      "views_user_mean_sec_live 5\n",
      "views_user_mean_sec_pvr 251\n"
     ]
    }
   ],
   "source": [
    "# TODO: fill Null in dt_user_features with for each value\n",
    "\n",
    "for col in dt_user_features.columns:\n",
    "    c = dt_user_features.select(col).filter(F.col(col).isNull()).count()\n",
    "    \n",
    "    if c > 0:\n",
    "        print(col, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(assembler.transform(df_test_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_probability = F.udf(lambda x: float(x[1]), T.FloatType())\n",
    "predictions = predictions.withColumn('purchase', get_probability(F.col('probability')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.select(\"user_id\", \"item_id\", \"purchase\").orderBy(\"user_id\", \"item_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = predictions.toPandas()\n",
    "submission.to_csv(\"lab03.csv\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2156840, 3)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>purchase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1654</td>\n",
       "      <td>336</td>\n",
       "      <td>0.021804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1654</td>\n",
       "      <td>678</td>\n",
       "      <td>0.021804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1654</td>\n",
       "      <td>691</td>\n",
       "      <td>0.021804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1654</td>\n",
       "      <td>696</td>\n",
       "      <td>0.021893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1654</td>\n",
       "      <td>763</td>\n",
       "      <td>0.021804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  purchase\n",
       "0     1654      336  0.021804\n",
       "1     1654      678  0.021804\n",
       "2     1654      691  0.021804\n",
       "3     1654      696  0.021893\n",
       "4     1654      763  0.021804"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
