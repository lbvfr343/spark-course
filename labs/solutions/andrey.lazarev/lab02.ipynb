{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 --executor-memory 4g --executor-cores 1 --driver-memory 3g pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import Row\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, Normalizer, StopWordsRemover\n",
    "from pyspark.sql.functions import col, udf, lit, desc\n",
    "from pyspark.ml.linalg import SparseVector, VectorUDT, DenseVector\n",
    "from pyspark.ml.feature import Normalizer\n",
    "import json\n",
    "import re\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.driver.memory\", \"8g\") \n",
    "conf.set('spark.executor.cores', '3')\n",
    "conf.set('spark.yarn.executor.memoryOverhead', '2G')\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .config(conf=conf)\n",
    "         .appName(\"test\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = spark.read.json('/labs/slaba02/DO_record_per_line.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "|                 cat|                desc| id|lang|                name|      provider|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "|3/business_manage...|This course intro...|  4|  en|Accounting Cycle:...|Canvas Network|\n",
      "|              11/law|This online cours...|  5|  en|American Counter ...|Canvas Network|\n",
      "|5/computer_scienc...|This course is ta...|  6|  fr|Arithmétique: en ...|Canvas Network|\n",
      "|  14/social_sciences|We live in a digi...|  7|  en|Becoming a Dynami...|Canvas Network|\n",
      "|2/biology_life_sc...|This self-paced c...|  8|  en|           Bioethics|Canvas Network|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "collection = [[23126, u'en', u'Compass - powerful SASS library that makes your life easier'], \n",
    "              [21617, u'en', u'Preparing for the AP* Computer Science A Exam \\u2014 Part 2'], \n",
    "              [16627, u'es', u'Aprende Excel: Nivel Intermedio by Alfonso Rinsche'], \n",
    "              [11556, u'es', u'Aprendizaje Colaborativo by UNID Universidad Interamericana para el Desarrollo'], \n",
    "              [16704, u'ru', u'\\u041f\\u0440\\u043e\\u0433\\u0440\\u0430\\u043c\\u043c\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u043d\\u0430 Lazarus'], \n",
    "              [13702, u'ru', u'\\u041c\\u0430\\u0442\\u0435\\u043c\\u0430\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u0430\\u044f \\u044d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u043a\\u0430']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rdd_data = spark.sparkContext.parallelize(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[8] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType(fields = [\n",
    "    StructField('id', IntegerType()),\n",
    "    StructField('lang', StringType()),\n",
    "    StructField('desc', StringType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+--------------------+\n",
      "|   id|lang|                desc|\n",
      "+-----+----+--------------------+\n",
      "|23126|  en|Compass - powerfu...|\n",
      "|21617|  en|Preparing for the...|\n",
      "|16627|  es|Aprende Excel: Ni...|\n",
      "|11556|  es|Aprendizaje Colab...|\n",
      "|16704|  ru|Программирование ...|\n",
      "+-----+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target = spark.createDataFrame(rdd_data, schema=schema)\n",
    "target.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clear_string(series):\n",
    "    regex = re.compile(u'[\\w\\d]{2,}', re.U)\n",
    "    lower_words = series.str.lower()\n",
    "    words = lower_words.str.findall(regex)\n",
    "    return words\n",
    "\n",
    "tokenizer_udf = F.pandas_udf(clear_string, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prev = data.select('id', 'lang', tokenizer_udf(\"desc\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------------------+\n",
      "| id|lang|  clear_string(desc)|\n",
      "+---+----+--------------------+\n",
      "|  4|  en|[this, course, in...|\n",
      "|  5|  en|[this, online, co...|\n",
      "|  6|  fr|[this, course, is...|\n",
      "|  7|  en|[we, live, in, di...|\n",
      "|  8|  en|[this, self, pace...|\n",
      "+---+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prev.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------------------+--------------------+\n",
      "| id|lang|  clear_string(desc)|      words_filtered|\n",
      "+---+----+--------------------+--------------------+\n",
      "|  4|  en|[this, course, in...|[course, introduc...|\n",
      "|  5|  en|[this, online, co...|[online, course, ...|\n",
      "|  6|  fr|[this, course, is...|[course, taught, ...|\n",
      "|  7|  en|[we, live, in, di...|[live, digitally,...|\n",
      "|  8|  en|[this, self, pace...|[self, paced, cou...|\n",
      "+---+----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stop_words = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "swr = StopWordsRemover(inputCol='clear_string(desc)', outputCol=\"words_filtered\", stopWords=stop_words)\n",
    "prev = swr.transform(prev)\n",
    "prev.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------------------+--------------------+--------------------+\n",
      "| id|lang|  clear_string(desc)|      words_filtered|                  tf|\n",
      "+---+----+--------------------+--------------------+--------------------+\n",
      "|  4|  en|[this, course, in...|[course, introduc...|(10000,[36,63,138...|\n",
      "|  5|  en|[this, online, co...|[online, course, ...|(10000,[32,222,36...|\n",
      "|  6|  fr|[this, course, is...|[course, taught, ...|(10000,[30,118,12...|\n",
      "|  7|  en|[we, live, in, di...|[live, digitally,...|(10000,[493,721,8...|\n",
      "|  8|  en|[this, self, pace...|[self, paced, cou...|(10000,[32,115,13...|\n",
      "+---+----+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ht = HashingTF(inputCol=\"words_filtered\", outputCol=\"tf\", numFeatures=10000)\n",
    "result = ht.transform(prev)\n",
    "result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idf = IDF(inputCol='tf', outputCol=\"idf\").fit(result)\n",
    "res = idf.transform(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------------------+--------------------+--------------------+--------------------+\n",
      "| id|lang|  clear_string(desc)|      words_filtered|                  tf|                 idf|\n",
      "+---+----+--------------------+--------------------+--------------------+--------------------+\n",
      "|  4|  en|[this, course, in...|[course, introduc...|(10000,[36,63,138...|(10000,[36,63,138...|\n",
      "|  5|  en|[this, online, co...|[online, course, ...|(10000,[32,222,36...|(10000,[32,222,36...|\n",
      "|  6|  fr|[this, course, is...|[course, taught, ...|(10000,[30,118,12...|(10000,[30,118,12...|\n",
      "|  7|  en|[we, live, in, di...|[live, digitally,...|(10000,[493,721,8...|(10000,[493,721,8...|\n",
      "|  8|  en|[this, self, pace...|[self, paced, cou...|(10000,[32,115,13...|(10000,[32,115,13...|\n",
      "+---+----+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "normalizer = Normalizer(inputCol=\"idf\", outputCol=\"norm\")\n",
    "res_end = normalizer.transform(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| id|lang|  clear_string(desc)|      words_filtered|                  tf|                 idf|                norm|\n",
      "+---+----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  4|  en|[this, course, in...|[course, introduc...|(10000,[36,63,138...|(10000,[36,63,138...|(10000,[36,63,138...|\n",
      "|  5|  en|[this, online, co...|[online, course, ...|(10000,[32,222,36...|(10000,[32,222,36...|(10000,[32,222,36...|\n",
      "|  6|  fr|[this, course, is...|[course, taught, ...|(10000,[30,118,12...|(10000,[30,118,12...|(10000,[30,118,12...|\n",
      "|  7|  en|[we, live, in, di...|[live, digitally,...|(10000,[493,721,8...|(10000,[493,721,8...|(10000,[493,721,8...|\n",
      "|  8|  en|[this, self, pace...|[self, paced, cou...|(10000,[32,115,13...|(10000,[32,115,13...|(10000,[32,115,13...|\n",
      "+---+----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res_end.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.conf.set( \"spark.sql.crossJoin.enabled\" , \"true\" )\n",
    "\n",
    "\n",
    "\n",
    "dot_udf = udf(lambda x,y: float(x.dot(y)), DoubleType())\n",
    "itog = res_end.alias(\"i\").join(res_end.where(col(\"id\") == 13702).alias(\"j\"), how='left')\\\n",
    "    .select(\n",
    "        col(\"i.id\").alias(\"i\"), \n",
    "        col(\"j.id\").alias(\"j\"), \n",
    "        dot_udf(\"i.norm\", \"j.norm\").alias(\"dot\"))\\\n",
    "    .sort(desc('dot'))\\\n",
    "    .select('i', 'dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|    i|\n",
      "+-----+\n",
      "|  864|\n",
      "|25502|\n",
      "|23769|\n",
      "|21079|\n",
      "|26206|\n",
      "|20069|\n",
      "| 8313|\n",
      "| 5399|\n",
      "| 1041|\n",
      "|  467|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itog1=itog.filter('i!=13702').select(\"i\").limit(10)\n",
    "itog1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_df = itog1.select(\"i\").toPandas()\n",
    "end_13702 = collected_df['i'].values.tolist()\n",
    "end_21617\n",
    "end_23126\n",
    "end_16627\n",
    "end_11556\n",
    "end_16704\n",
    "end_13702\n",
    "end= dict({'21617' : end_21617 , '23126' : end_23126 , '16627' : end_16627 , '11556' : end_11556, '16704' : end_16704, '13702' : end_13702  }) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'21617': [21609,\n",
       "  21616,\n",
       "  22298,\n",
       "  21608,\n",
       "  21628,\n",
       "  21630,\n",
       "  21081,\n",
       "  21623,\n",
       "  19417,\n",
       "  21508],\n",
       " '23126': [13665,\n",
       "  14760,\n",
       "  13782,\n",
       "  20638,\n",
       "  24419,\n",
       "  15909,\n",
       "  2724,\n",
       "  25782,\n",
       "  17499,\n",
       "  13348],\n",
       " '16627': [11431, 12247, 17964, 5687, 11575, 17961, 12660, 25010, 5558, 16694],\n",
       " '11556': [16488, 468, 13461, 23357, 19330, 7833, 9289, 10447, 10384, 16929],\n",
       " '16704': [3864, 23407, 25724, 25726, 23864, 1236, 18023, 1247, 25627, 11212],\n",
       " '13702': [864, 25502, 23769, 21079, 26206, 20069, 8313, 5399, 1041, 467]}"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "collected_df = end\n",
    "with open('lab02.json', 'w') as outfile:\n",
    "    json.dump(collected_df , outfile , indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
