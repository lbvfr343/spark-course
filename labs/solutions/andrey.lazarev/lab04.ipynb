{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 --executor-memory 3g --executor-cores 3 --driver-memory 3g pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as t\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, StringIndexer, IndexToString\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.sql.functions import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier, RandomForestClassifier\n",
    "from pyspark.sql.functions import col, desc, pandas_udf, PandasUDFType, udf, regexp_replace, when, asc, lit, broadcast\n",
    "from pyspark.sql.types import StructType, IntegerType, StructField, DateType, StringType, TimestampType, FloatType, ArrayType, LongType\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.sql.functions import struct, to_json\n",
    "from pyspark.sql.functions import shuffle, array, lit\n",
    "from pyspark.sql.functions import col, explode\n",
    "from pyspark.sql.functions import lower\n",
    "from pyspark.sql.functions import struct, to_json\n",
    "from pyspark.ml import Pipeline, PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"gender\", StringType()),\n",
    "    StructField(\"age\", StringType()),\n",
    "    StructField(\"uid\", StringType()),\n",
    "    StructField(\"user_json\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Она расположена на hdfs: /labs/slaba04/.\n",
    "\n",
    "#Поле gender принимает значения F (женщина) и M (мужчина).\n",
    "\n",
    "#Поле age принимает значения диапазона возраста: 18-24, 25-34, 35-44, 45-54, >=55\n",
    "\n",
    "#Поле uid принимает значения уникального ID пользователя (cookies): d50192e5-c44e-4ae8-ae7a-7cfe67c8b777.\n",
    "\n",
    "#Поле user_json имеет внутри json со следующей схемой данных: {\"visits\": [{\"url\": \"url1\", \"timestamp\": \"timestamp1\"}, {\"url\": \"url2\", \"timestamp\": \"timestamp2\"}]}. В нем содержатся непосредственно логи посещения пользователем страниц вместе с временной меткой посещения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/labs/slaba04/gender_age_dataset.txt\", header=True, schema=schema,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------------+--------------------+\n",
      "|gender|  age|                 uid|           user_json|\n",
      "+------+-----+--------------------+--------------------+\n",
      "|     F|18-24|d50192e5-c44e-4ae...|{\"visits\": [{\"url...|\n",
      "+------+-----+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_visit = t.StructType([\n",
    "    t.StructField('visits', t.ArrayType(\n",
    "        t.StructType([\n",
    "        t.StructField('url', StringType(), True),\n",
    "        t.StructField('timestamp', LongType(), True)\n",
    "                      ])\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------------+--------------------+\n",
      "|gender|  age|                 uid|              visits|\n",
      "+------+-----+--------------------+--------------------+\n",
      "|     F|18-24|d50192e5-c44e-4ae...|[[[http://zebra-z...|\n",
      "|     M|25-34|d502331d-621e-472...|[[[http://sweetra...|\n",
      "+------+-----+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=(\n",
    "    df\n",
    "    .select('gender','age','uid', f.col('user_json').cast('string').alias('value'))\n",
    "    .select('gender','age','uid', from_json(f.col('value'), schema_visit).alias('visits'))\n",
    "    \n",
    ")\n",
    "df1.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(gender='F', age='18-24', uid='d50192e5-c44e-4ae8-ae7a-7cfe67c8b777', visits=Row(visits=[Row(url='http://zebra-zoya.ru/200028-chehol-organayzer-dlja-macbook-11-grid-it.html?utm_campaign=397720794&utm_content=397729344&utm_medium=cpc&utm_source=begun', timestamp=1419688144068), Row(url='http://news.yandex.ru/yandsearch?cl4url=chezasite.com/htc/htc-one-m9-delay-86327.html&lr=213&rpt=story', timestamp=1426666298001), Row(url='http://www.sotovik.ru/news/240283-htc-one-m9-zaderzhivaetsja.html', timestamp=1426666298000), Row(url='http://news.yandex.ru/yandsearch?cl4url=chezasite.com/htc/htc-one-m9-delay-86327.html&lr=213&rpt=story', timestamp=1426661722001), Row(url='http://www.sotovik.ru/news/240283-htc-one-m9-zaderzhivaetsja.html', timestamp=1426661722000)]))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(gender='F', age='18-24', uid='d50192e5-c44e-4ae8-ae7a-7cfe67c8b777', visits=[Row(url='http://zebra-zoya.ru/200028-chehol-organayzer-dlja-macbook-11-grid-it.html?utm_campaign=397720794&utm_content=397729344&utm_medium=cpc&utm_source=begun', timestamp=1419688144068), Row(url='http://news.yandex.ru/yandsearch?cl4url=chezasite.com/htc/htc-one-m9-delay-86327.html&lr=213&rpt=story', timestamp=1426666298001), Row(url='http://www.sotovik.ru/news/240283-htc-one-m9-zaderzhivaetsja.html', timestamp=1426666298000), Row(url='http://news.yandex.ru/yandsearch?cl4url=chezasite.com/htc/htc-one-m9-delay-86327.html&lr=213&rpt=story', timestamp=1426661722001), Row(url='http://www.sotovik.ru/news/240283-htc-one-m9-zaderzhivaetsja.html', timestamp=1426661722000)])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=df1.select('gender','age','uid','visits.visits')\n",
    "df2.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- uid: string (nullable = true)\n",
      " |-- visits: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df2.select('gender','age','uid', explode(df2.visits.url).alias('url'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------------+--------------------+\n",
      "|gender|  age|                 uid|                 url|\n",
      "+------+-----+--------------------+--------------------+\n",
      "|     F|18-24|d50192e5-c44e-4ae...|http://zebra-zoya...|\n",
      "+------+-----+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.withColumn('site_name', f.regexp_extract(df3.url, r'\\w+:\\/\\/(www\\.)?(([\\w-]+)(\\.[\\w-]+)*)\\/?', 2)).drop('url').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------------+------------+\n",
      "|gender|  age|                 uid|   site_name|\n",
      "+------+-----+--------------------+------------+\n",
      "|     M|35-44|33dc7928-4226-4d0...|panicnews.ru|\n",
      "|     M|25-34|33ebe1d9-0ef3-486...|  aburmu4.tv|\n",
      "|     F|25-34|33ee5b03-5e7d-478...| ofigenno.cc|\n",
      "+------+-----+--------------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4.groupBy(\"gender\",\"age\",\"uid\").\\\n",
    "                    agg(f.collect_list(\"site_name\").alias(\"site_name\")).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df5.filter(df.gender != '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexGender = StringIndexer(inputCol='gender', outputCol='gender_i')\n",
    "indexAge = StringIndexer(inputCol='age', outputCol='age_i')\n",
    "indexModelGender = indexGender.fit(df5)\n",
    "indexModelAge = indexAge.fit(df5)\n",
    "df_i = indexModelGender.transform(df5)\n",
    "df_i = indexModelAge.transform(df_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------------+-------------------+--------+-----+\n",
      "|gender|  age|                 uid|          site_name|gender_i|age_i|\n",
      "+------+-----+--------------------+-------------------+--------+-----+\n",
      "|     M|18-24|0735ae64-024e-445...|[km.ru, it-fecs.ru]|     0.0|  2.0|\n",
      "+------+-----+--------------------+-------------------+--------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_i.where('gender=\"M\"').show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(numFeatures=100000, binary=False, inputCol=\"site_name\", outputCol=\"site_name_h\")\n",
    "forestG = RandomForestClassifier(featuresCol='site_name_h', labelCol='gender_i', predictionCol='predictionG',\n",
    "                                 probabilityCol='probabilityG', rawPredictionCol='rawPredictionG')\n",
    "forestA = RandomForestClassifier(featuresCol='site_name_h', labelCol='age_i', predictionCol='predictionA',\n",
    "                                 probabilityCol='probabilityA', rawPredictionCol='rawPredictionA')\n",
    "strindG = IndexToString(inputCol='predictionG', outputCol='gender_p', labels=indexModelGender.labels)\n",
    "strindA = IndexToString(inputCol='predictionA', outputCol='age_p', labels=indexModelAge.labels)\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "   \n",
    "    hashingTF,\n",
    "    forestG,\n",
    "    forestA,\n",
    "    strindG,\n",
    "    strindA\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_i.sampleBy('gender_i', fractions={0: 0.8, 1: 0.8}, seed=42).cache()\n",
    "val = df_i.join(train, on=['uid'], how='leftanti').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipiline_model=pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid=pipiline_model.transform(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+----+--------------------+--------+-----+--------------------+--------------------+--------------------+-----------+--------------------+--------------------+-----------+--------+-----+\n",
      "|                 uid|gender| age|           site_name|gender_i|age_i|         site_name_h|      rawPredictionG|        probabilityG|predictionG|      rawPredictionA|        probabilityA|predictionA|gender_p|age_p|\n",
      "+--------------------+------+----+--------------------+--------+-----+--------------------+--------------------+--------------------+-----------+--------------------+--------------------+-----------+--------+-----+\n",
      "|098a0e00-8597-475...|     F|>=55|[jkeks.ru, youtub...|     1.0|  4.0|(100000,[56021,72...|[10.2714519598183...|[0.51357259799091...|        0.0|[8.61539862739998...|[0.43076993136999...|        0.0|       M|25-34|\n",
      "|0a595fa1-bae0-41d...|     M|>=55|[go.mail.ru, avit...|     0.0|  4.0|(100000,[4925,288...|[10.3927615743713...|[0.51963807871856...|        0.0|[8.62454103747262...|[0.43122705187363...|        0.0|       M|25-34|\n",
      "+--------------------+------+----+--------------------+--------+-----+--------------------+--------------------+--------------------+-----------+--------------------+--------------------+-----------+--------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- uid: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- site_name: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- gender_i: double (nullable = false)\n",
      " |-- age_i: double (nullable = false)\n",
      " |-- site_name_h: vector (nullable = true)\n",
      " |-- rawPredictionG: vector (nullable = true)\n",
      " |-- probabilityG: vector (nullable = true)\n",
      " |-- predictionG: double (nullable = false)\n",
      " |-- rawPredictionA: vector (nullable = true)\n",
      " |-- probabilityA: vector (nullable = true)\n",
      " |-- predictionA: double (nullable = false)\n",
      " |-- gender_p: string (nullable = true)\n",
      " |-- age_p: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "valid.show(2)\n",
    "valid.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5151666096001996"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"predictionG\", labelCol=\"gender_i\", metricName='areaUnderROC')\n",
    "\n",
    "evaluator.evaluate(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kill_all():\n",
    "    streams = SparkSession.builder.getOrCreate().streams.active\n",
    "    if streams:\n",
    "        for s in streams:\n",
    "            desc = s.lastProgress[\"sources\"][0][\"description\"]\n",
    "            s.stop()\n",
    "            print(\"Stopped {s}\".format(s=desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_BOOTSTRAP_SERVERS='spark-master-1.newprolab.com:6667'\n",
    "KAFKA_INPUT_TOPIC='input_yuriy.gulynin'\n",
    "KAFKA_OUTPUT_TOPIC='yuriy.gulynin'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_read_df=(\n",
    "    spark.readStream\n",
    "        .format('kafka')\n",
    "        .option('kafka.bootstrap.servers',KAFKA_BOOTSTRAP_SERVERS)\n",
    "        .option('subscribe', KAFKA_INPUT_TOPIC)\n",
    "        .option('startingOffsets', 'earliest')\n",
    "        .option('failOnDataLoss', 'False')\n",
    "        .load()        \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_schema=t.StructType([   \n",
    "        t.StructField(\"uid\", t.StringType(), True),\n",
    "        t.StructField(\"visits\", t.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "visit_schema =  t.ArrayType(\n",
    "    t.StructType([  \n",
    "        t.StructField(\"url\", StringType(), True),\n",
    "        t.StructField(\"timestamp\", LongType(), True)\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/08 13:03:55 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-master-1.newprolab.com:8020/user/yuriy.gulynin/tmp/lab04/checkpointLocation' to trash at: hdfs://spark-master-1.newprolab.com:8020/user/yuriy.gulynin/.Trash/Current/user/yuriy.gulynin/tmp/lab04/checkpointLocation\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f5cac069240>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! hdfs dfs -rm -R /user/yuriy.gulynin/tmp/lab04/checkpointLocation\n",
    "\n",
    "kafka_read_df = (spark\n",
    "    .readStream\n",
    "    .format('kafka')\n",
    "    .option('kafka.bootstrap.servers', KAFKA_BOOTSTRAP_SERVERS)\n",
    "    .option('subscribe', KAFKA_INPUT_TOPIC)\n",
    "    .option('startingOffsets', 'earliest')\n",
    "    .option('failOnDataLoss', 'False')\n",
    "    .load()\n",
    ")\n",
    "\n",
    "clean_df = (kafka_read_df\n",
    "    .select(f.col('value').cast('string').alias('value'))\n",
    "    .select(f.from_json(f.col('value'), event_schema).alias('event'))\n",
    "    .select('evenT.uid', f.from_json(f.col('evenT.visits'), visit_schema).alias('visits'))\n",
    "    .withColumn('url', f.col('visits.url'))\n",
    "    .drop('visits')\n",
    ")\n",
    "\n",
    "clean_df2=clean_df.select('uid', explode(clean_df.url).alias('url'))\n",
    "clean_df2=clean_df2.withColumn('site_name', f.regexp_extract('url', r'\\w+:\\/\\/(www\\.)?(([\\w-]+)(\\.[\\w-]+)*)\\/?', 2)).distinct().drop(\"url\")\n",
    "clean_df2 = clean_df2.groupBy(\"uid\").\\\n",
    "                    agg(f.collect_list(\"site_name\").alias(\"site_name\"))\n",
    "\n",
    "\n",
    "\n",
    "predictions_df = pipiline_model.transform(clean_df2) \\\n",
    ".select('uid', f.col('gender_p').alias('gender'), f.col('age_p').alias('age'))\n",
    "\n",
    "kafka_out_df = predictions_df.select(to_json(struct(*predictions_df.columns)).alias('value'))\n",
    "\n",
    "kafka_write_stream = (\n",
    "    kafka_out_df\n",
    "    .writeStream\n",
    "    .format(\"kafka\")\n",
    "    .outputMode(\"complete\")\n",
    "    .option(\"checkpointLocation\", \"tmp/lab04/checkpointLocation\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS)\n",
    "    .option(\"topic\", KAFKA_OUTPUT_TOPIC)\n",
    ")\n",
    "kafka_write_stream.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped KafkaV2[Subscribe[input_yuriy.gulynin]]\n",
      "Stopped KafkaV2[Subscribe[input_yuriy.gulynin]]\n"
     ]
    }
   ],
   "source": [
    "kill_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
