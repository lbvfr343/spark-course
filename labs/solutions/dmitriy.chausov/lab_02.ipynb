{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.json('/labs/slaba02/DO_record_per_line.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28153"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "|                 cat|                desc| id|lang|                name|      provider|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "|3/business_manage...|This course intro...|  4|  en|Accounting Cycle:...|Canvas Network|\n",
      "|              11/law|This online cours...|  5|  en|American Counter ...|Canvas Network|\n",
      "|5/computer_scienc...|This course is ta...|  6|  fr|Arithmétique: en ...|Canvas Network|\n",
      "|  14/social_sciences|We live in a digi...|  7|  en|Becoming a Dynami...|Canvas Network|\n",
      "|2/biology_life_sc...|This self-paced c...|  8|  en|           Bioethics|Canvas Network|\n",
      "|9/humanities|15/m...|This game-based c...|  9|  en|College Foundatio...|Canvas Network|\n",
      "|  14/social_sciences|What’s in your di...| 10|  en|Digital Literacies I|Canvas Network|\n",
      "|  14/social_sciences|The goal of the D...| 11|  en|Digital Literacie...|Canvas Network|\n",
      "|  14/social_sciences|Ready to explore ...| 12|  en|Digital Tools for...|Canvas Network|\n",
      "|  14/social_sciences|This self-paced c...| 13|  en|Discover Your Val...|Canvas Network|\n",
      "|  12/medicine_health|What is “interpro...| 14|  en|Enhancing Patient...|Canvas Network|\n",
      "|        16/languages|This course prese...| 15|  en|Ethics and Values...|Canvas Network|\n",
      "|         4/chemistry|Chemistry is an i...| 16|  en| Exploring Chemistry|Canvas Network|\n",
      "|8/engineering_tec...|Are you consideri...| 17|  en|Exploring Enginee...|Canvas Network|\n",
      "|   1/arts_music_film|Princess stories ...| 18|  en|Fairy Tales: Orig...|Canvas Network|\n",
      "|        9/humanities|This first instal...| 19|  en|First Peoples to ...|Canvas Network|\n",
      "|  14/social_sciences|This course exami...| 20|  en| Forums for a Future|Canvas Network|\n",
      "|        9/humanities|This course will ...| 21|  en|From the Gilded A...|Canvas Network|\n",
      "|8/engineering_tec...|The field of tech...| 22|  en|Fundamentals of S...|Canvas Network|\n",
      "|  14/social_sciences|Are you a Higher ...| 23|  en|Hybrid Courses: B...|Canvas Network|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Курсы, по которым нужно выдать решение\n",
    "courses = [\n",
    "[23126, u'en', u'Compass - powerful SASS library that makes your life easier'], \n",
    "[21617, u'en', u'Preparing for the AP* Computer Science A Exam \\u2014 Part 2'], \n",
    "[16627, u'es', u'Aprende Excel: Nivel Intermedio by Alfonso Rinsche'], \n",
    "[11556, u'es', u'Aprendizaje Colaborativo by UNID Universidad Interamericana para el Desarrollo'], \n",
    "[16704, u'ru', u'\\u041f\\u0440\\u043e\\u0433\\u0440\\u0430\\u043c\\u043c\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u043d\\u0430 Lazarus'], \n",
    "[13702, u'ru', u'\\u041c\\u0430\\u0442\\u0435\\u043c\\u0430\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u0430\\u044f \\u044d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u043a\\u0430']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Normalizer, Tokenizer, StopWordsRemover\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import ArrayType, StringType, FloatType\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_courses_ids = sorted([row[0] for row in courses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn(\"desc\", f.lower(f.col(\"desc\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"desc\", outputCol=\"desc_words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stop_words = StopWordsRemover.loadDefaultStopWords(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "swr = StopWordsRemover(inputCol=\"desc_words\", outputCol=\"words_filtered\", stopWords=eng_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hashing trick\n",
    "hashingTF = HashingTF(inputCol=\"words_filtered\", outputCol=\"rawFeatures\", numFeatures=10000, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDF\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Собираем pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, swr, hashingTF, idf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_idf = pipeline.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_idf = data_idf.select(\"id\", \"lang\", \"name\", \"words_filtered\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28 ms, sys: 13.1 ms, total: 41 ms\n",
      "Wall time: 32.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Выбираем вектора IDF по курсам для поиска рекомендаций\n",
    "search_courses_df = data_idf.where(f.col(\"id\").isin(search_courses_ids))\n",
    "\n",
    "# Переименовымваем поля для джойна, чтобы не повторялись\n",
    "search_courses_idf = search_courses_df.select(\"id\", \"lang\", \"name\", \"features\") \\\n",
    "    .withColumnRenamed(\"id\", \"search_id\") \\\n",
    "    .withColumnRenamed(\"lang\", \"search_lang\") \\\n",
    "    .withColumnRenamed(\"name\", \"search_name\") \\\n",
    "    .withColumnRenamed(\"features\", \"search_features\") \\\n",
    "\n",
    "# Перемножаем (cross-join) список всех курсов со списком поиска, чтобы просчитать все в один заход\n",
    "joined_data = data_idf.join(f.broadcast(search_courses_idf), data_idf.lang == search_courses_idf.search_lang) \\\n",
    "    .filter(\"lang = search_lang\") \\\n",
    "    .filter(\"id != search_id\")  \n",
    "\n",
    "# Создаем и регистриуем UDF для косинуса угла (cosine simularity )\n",
    "def cos_sim(a,b):\n",
    "    return float(a.dot(b) / (a.norm(2) * b.norm(2)))\n",
    "\n",
    "cos_sim_udf = f.udf(cos_sim, FloatType())\n",
    "\n",
    "# Считаем меру похожести для каждой пары - cosine simularity\n",
    "joined_data = joined_data.withColumn(\"cos_sim\", cos_sim_udf(f.col(\"features\"), f.col(\"search_features\"))) \\\n",
    "                .filter((f.isnan(f.col(\"cos_sim\")) == False) & (f.isnull(f.col(\"cos_sim\")) == False))\n",
    "\n",
    "joined_data.cache()\n",
    "\n",
    "# Собираем все рекомендации в dict\n",
    "result = dict()\n",
    "for course_id in search_courses_ids:\n",
    "    res = joined_data.filter(f.col(\"search_id\") == course_id).orderBy(f.desc(\"cos_sim\"), \"name\", \"id\").limit(10).select(\"id\").collect()\n",
    "    ids = [row[0] for row in res]\n",
    "    result[str(course_id)] = ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'11556': [16488, 13461, 468, 23357, 19330, 16929, 387, 10447, 11554, 9289],\n",
       " '13702': [864, 1216, 7173, 1052, 8313, 17017, 19613, 21017, 17015, 8082],\n",
       " '16627': [11431, 12247, 13021, 25010, 11575, 5687, 5372, 12863, 9598, 22680],\n",
       " '16704': [1365, 20645, 1426, 20105, 8217, 1236, 1164, 1219, 8123, 875],\n",
       " '21617': [21609,\n",
       "  21608,\n",
       "  21616,\n",
       "  21492,\n",
       "  21624,\n",
       "  21623,\n",
       "  21630,\n",
       "  21628,\n",
       "  21508,\n",
       "  21703],\n",
       " '23126': [13782, 13665, 24419, 20638, 2724, 25782, 2633, 2723, 13348, 15909]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пишем в JSON\n",
    "with open('lab02.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat lab02.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
