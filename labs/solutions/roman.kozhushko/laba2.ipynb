{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 5 --executor-memory 4g --executor-cores 1 --driver-memory 3g pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"RIK_lab2\") \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "\n",
    "from pyspark.ml import Transformer, Pipeline\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, Normalizer, StopWordsRemover, CountVectorizer, VectorAssembler\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, FloatType, ArrayType, StringType, IntegerType\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import udf, col, isnan, isnull, broadcast, desc, lower, pandas_udf, row_number, explode\n",
    "\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Курсы данные по варианту\n",
    "given_courses = [\n",
    "    [23126, u'en', u'Compass - powerful SASS library that makes your life easier'], \n",
    "    [21617, u'en', u'Preparing for the AP* Computer Science A Exam \\u2014 Part 2'], \n",
    "    [16627, u'es', u'Aprende Excel: Nivel Intermedio by Alfonso Rinsche'], \n",
    "    [11556, u'es', u'Aprendizaje Colaborativo by UNID Universidad Interamericana para el Desarrollo'], \n",
    "    [16704, u'ru', u'\\u041f\\u0440\\u043e\\u0433\\u0440\\u0430\\u043c\\u043c\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u043d\\u0430 Lazarus'], \n",
    "    [13702, u'ru', u'\\u041c\\u0430\\u0442\\u0435\\u043c\\u0430\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u0430\\u044f \\u044d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u043a\\u0430']\n",
    "]\n",
    "\n",
    "id_given_courses = [x[0] for x in given_courses]\n",
    "\n",
    "# given_courses = spark.createDataFrame(data=given_courses, schema = [\"id\",\"lang\", \"name\"])\n",
    "# given_courses.printSchema()\n",
    "# given_courses.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.json('/labs/slaba02/DO_record_per_line.json')\n",
    "data = data.filter(data.lang.isin('en', 'es', 'ru')).repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClearStringTransformer(Transformer):\n",
    "    \n",
    "  def _transform(self, df: DataFrame):\n",
    "    regex = re.compile(u'[\\w\\d]{3,}', re.U)\n",
    "    transform_udf = udf(lambda x: ' '.join(re.findall(regex, x)), StringType())\n",
    "    return df.withColumn('desc', transform_udf('desc'))\n",
    "\n",
    "class CatToParamTransformer(Transformer):\n",
    "    \n",
    "  def _transform(self, df: DataFrame):\n",
    "    regex = re.compile(u'(\\d+)/', re.U)\n",
    "    transform_udf = udf(lambda x: re.findall(regex, x), ArrayType(StringType()))\n",
    "    return df.withColumn('cat_param', transform_udf('cat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat process\n",
    "CTPT = CatToParamTransformer()\n",
    "cat_count_vectorizer = CountVectorizer(inputCol='cat_param', outputCol=\"cat_vector\", binary=True)\n",
    "\n",
    "# desc process\n",
    "CST = ClearStringTransformer()\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"desc\", outputCol=\"word\")\n",
    "\n",
    "all_stop_words = (StopWordsRemover.loadDefaultStopWords('english') \n",
    "                  + StopWordsRemover.loadDefaultStopWords('russian') \n",
    "                  + StopWordsRemover.loadDefaultStopWords('spanish'))\n",
    "\n",
    "swf = StopWordsRemover(stopWords=all_stop_words, inputCol=tokenizer.getOutputCol(), outputCol='word_swf')\n",
    "\n",
    "hasher = HashingTF(numFeatures=100000, binary=False, inputCol=swf.getOutputCol(), outputCol=\"tf\")\n",
    "# count_vectorizer = CountVectorizer(inputCol=swf.getOutputCol(), outputCol=\"word_vector\", binary=False)\n",
    "\n",
    "idf = IDF(inputCol=hasher.getOutputCol(), outputCol=\"idf_feature\")\n",
    "\n",
    "normalizer = Normalizer(inputCol=idf.getOutputCol(), outputCol=\"norm\")\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\n",
    "    cat_count_vectorizer.getOutputCol(), normalizer.getOutputCol()\n",
    "], outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    CTPT,\n",
    "    cat_count_vectorizer,\n",
    "    CST,\n",
    "    tokenizer,\n",
    "    swf,\n",
    "    hasher,\n",
    "    # count_vectorizer,\n",
    "    idf,\n",
    "    normalizer,\n",
    "    assembler\n",
    "])\n",
    "\n",
    "pipeline_model = pipeline.fit(data)\n",
    "data_feature = pipeline_model.transform(data)\n",
    "data_feature = data_feature.drop('desc', 'word', 'word_swf', 'word_vector', \n",
    "                                 'cat', 'cat_param', 'provider', 'tf', 'idf_feature', \n",
    "                                 'cat_vector', 'norm')\n",
    "# data_feature.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_courses_feature = data_feature.filter(col('id').isin(id_given_courses))\n",
    "all_minus_given_courses = data_feature.join(given_courses_feature, on=\"id\", how=\"leftanti\").coalesce(10).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity\n",
    "dot_udf = udf(lambda x,y: float(x.dot(y)), DoubleType())\n",
    "\n",
    "result = (\n",
    "    given_courses_feature.alias(\"i\")\n",
    "    .join(all_minus_given_courses.alias(\"j\"), col(\"i.lang\") == col(\"j.lang\"))\n",
    "    .select(\n",
    "        col(\"i.id\").alias(\"id\"), \n",
    "        col(\"j.id\").alias(\"recomended_id\"),\n",
    "        col(\"j.name\").alias(\"name\"),\n",
    "        dot_udf(\"i.features\", \"j.features\").alias(\"dot\")\n",
    "    )\n",
    "    .sort(col(\"id\"), col(\"dot\").desc(), col(\"name\"), col(\"recomended_id\"))\n",
    ")\n",
    "\n",
    "result = result.coalesce(10).cache()\n",
    "result.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(\"id\").orderBy(col(\"id\"), col(\"dot\").desc(), col(\"name\"), col(\"recomended_id\"))\n",
    "result = result.withColumn(\"row_number\", row_number().over(windowSpec)).filter(col(\"row_number\") <= 10)\n",
    "result = result.coalesce(10).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_driver = result.select(col(\"id\"), col(\"recomended_id\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_driver_json = {}\n",
    "for x in result_driver:\n",
    "    if x.id not in result_driver_json:\n",
    "        result_driver_json[x.id] = []\n",
    "    result_driver_json[x.id].append(x.recomended_id)\n",
    "result_driver_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('lab02.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(result_driver_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
