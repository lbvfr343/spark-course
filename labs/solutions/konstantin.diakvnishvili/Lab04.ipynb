{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 5 --executor-memory 4g --executor-cores 1 --driver-memory 2g pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"Konstantin Diakvnishvili lab 4 app\") \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Библиотеки \n",
    "import pyspark.sql.types as t\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StringIndexer, IndexToString\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.param.shared import HasOutputCol, HasInputCol\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark import keyword_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# формируем spark DF\n",
    "path = '/labs/slaba04/gender_age_dataset.txt'\n",
    "\n",
    "schema = t.StructType(fields=[\n",
    "    t.StructField('gender', t.StringType()),\n",
    "    t.StructField('age', t.StringType()),\n",
    "    t.StructField('uid', t.StringType()),\n",
    "    t.StructField('user_json', t.StringType()),\n",
    "])\n",
    "\n",
    "train_data = spark.read.csv(path, header=True, schema=schema, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# схема для json с визитами\n",
    "visit_schema = t.ArrayType(\n",
    "    t.StructType([\n",
    "        t.StructField('url', t.StringType(), True),\n",
    "        t.StructField('timestamp', t.LongType(), True)\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# фильтруем, парсим json, достаем url\n",
    "train_data = train_data.filter(train_data.age != '-') \\\n",
    "                       .filter(train_data.gender != '-') \\\n",
    "                       .withColumn('visits', f.from_json(f.col('user_json'), visits_schema )) \\\n",
    "                       .withColumn('url', f.col('visits.visits.url')) \\\n",
    "                       .drop('visits', 'user_json')\n",
    "train_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# модели индексации пола и возраста\n",
    "indexerG = StringIndexer(inputCol='gender', outputCol='gender_i')\n",
    "indexerA = StringIndexer(inputCol='age', outputCol='age_i')\n",
    "indexModelG = indexerG.fit(train_data)\n",
    "indexModelA = indexerA.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# траснформер, парсит url'ы. Удаляет исходный столбец т.к. gropBy слишком долго отрабатывает\n",
    "# DefaultParamsReadable, DefaultParamsWritable - для сохранения модели\n",
    "class ParseURLTransformer(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(ParseURLTransformer, self).__init__()\n",
    "        if inputCol is not None:\n",
    "            self.setInputCol(inputCol)\n",
    "        if outputCol is not None:\n",
    "            self.setOutputCol(outputCol)\n",
    "        \n",
    "    def _transform(self, dataset):\n",
    "        res = dataset.withColumn('url_inter', f.explode(self.getInputCol())) \\\n",
    "                          .withColumn('url_inter', f.expr('parse_url(url_inter, \"HOST\")')) \\\n",
    "                          .withColumn('url_inter', f.lower(f.col('url_inter'))) \\\n",
    "                          .drop(self.getInputCol())\n",
    "        \n",
    "        res_col_list = res.columns\n",
    "        res_col_list.remove('url_inter')\n",
    "        res = res.groupBy(res_col_list) \\\n",
    "                 .agg(f.collect_list('url_inter').alias(self.getOutputCol())) \\\n",
    "#                 .withColumn(\"timestamp\", f.current_timestamp()) \\\n",
    "#                 .withWatermark(\"timestamp\", \"10 minutes\")\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlpt = ParseURLTransformer(inputCol='url', outputCol='url_parsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hasher\n",
    "hasher = HashingTF(numFeatures=130000, binary=False, inputCol=\"url_parsed\", outputCol=\"url_freq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest. Отдельно для пола и возраста\n",
    "forestG = RandomForestClassifier(featuresCol='url_freq', labelCol='gender_i',\n",
    "                                 predictionCol='predictionG', probabilityCol='probabilityG', rawPredictionCol='rawPredictionG')\n",
    "forestA = RandomForestClassifier(featuresCol='url_freq', labelCol='age_i',  \\\n",
    "                                 predictionCol='predictionA', probabilityCol='probabilityA', rawPredictionCol='rawPredictionA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Возвращают индексы возраста и пола обратно в string\n",
    "stringerG = IndexToString(inputCol='predictionG', outputCol='gender_s', labels=indexModelG.labels)\n",
    "stringerA = IndexToString(inputCol='predictionA', outputCol='age_s', labels=indexModelA.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пайплан\n",
    "pipeline = Pipeline(stages=[\n",
    "    urlpt,\n",
    "    hasher,\n",
    "    forestG,\n",
    "    forestA,\n",
    "    stringerG,\n",
    "    stringerA\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# индексация\n",
    "df_visits = indexModelG.transform(train_data)\n",
    "df_visits = indexModelA.transform(df_visits)\n",
    "df_visits.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучение\n",
    "pipeline_model = pipeline.fit(df_visits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# параметры для стрима\n",
    "KAFKA_BOOTSTRAP_SERVER = 'spark-master-1.newprolab.com:6667'\n",
    "KAFKA_INPUT_TOPIC = 'input_konstantin.diakvnishvili'\n",
    "KAFKA_OUTPUT_TOPIC = 'konstantin.diakvnishvili'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# чтение стрима\n",
    "kafka_stream = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format('kafka')\n",
    "    .option('kafka.bootstrap.servers', KAFKA_BOOTSTRAP_SERVER)\n",
    "    .option('subscribe', KAFKA_INPUT_TOPIC)\n",
    "    .option('startingOffsets', 'earliest')\n",
    "    .option('failOnDataLoss', 'False')\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# схемы для входного json'а\n",
    "event_schema = t.StructType([\n",
    "    t.StructField('uid', t.StringType(), True),\n",
    "    t.StructField('visits', t.StringType(), True),\n",
    "])\n",
    "\n",
    "visit_schema2 = t.ArrayType(\n",
    "    t.StructType([\n",
    "        t.StructField('url', t.StringType(), True),\n",
    "        t.StructField('timestamp', t.LongType(), True)\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# парсим входной стрим, парсим json, достаем url\n",
    "parsed_sdf = kafka_stream.select(f.col('value').cast('string').alias('value')) \\\n",
    "                         .select(f.from_json(f.col('value'), event_schema).alias('event')) \\\n",
    "                         .select( 'event.uid', f.from_json(f.col('event.visits'), visit_schema2).alias('visits')) \\\n",
    "                         .withColumn( 'url', f.col('visits.url')) \\\n",
    "                         .drop('visits')\n",
    "parsed_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# прогнзируем\n",
    "predictions_df_test = pipeline_model.transform(parsed_sdf)\n",
    "predictions_df_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выделяем нужные столбцы\n",
    "predictions_df = predictions_df_test.select('uid', f.col('gender_s').alias('gender'), f.col('age_s').alias('age'))\n",
    "predictions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# запись \n",
    "# outputMode=\"complete\" т.к. есть агрегация\n",
    "kafka_write_stream = (\n",
    "    predictions_df\n",
    "    .select(f.to_json(f.struct(*predictions_df.columns)).alias('value'))\n",
    "    .writeStream\n",
    "    .format(\"kafka\")\n",
    "    .outputMode(\"complete\")\n",
    "    .option(\"checkpointLocation\", \"checkpoints/checkpoints_lab04\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVER)\n",
    "    .option(\"topic\", KAFKA_OUTPUT_TOPIC)\n",
    ")\n",
    "kafka_write_stream.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kill_all():\n",
    "    streams = SparkSession.builder.getOrCreate().streams.active\n",
    "    if streams:\n",
    "        for s in streams:\n",
    "            desc = s.lastProgress[\"sources\"][0][\"description\"]\n",
    "            s.stop()\n",
    "            print(\"Stopped {s}\".format(s=desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#убиваем стрим\n",
    "kill_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
