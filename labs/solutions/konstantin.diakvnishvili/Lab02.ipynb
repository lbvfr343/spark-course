{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 5 --executor-memory 4g --executor-cores 1 --driver-memory 2g pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"Konstantin Diakvnishvili lab 2 app\") \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master-5.newprolab.com:4049\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Konstantin Diakvnishvili lab 2 app</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f18d40c04a8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, IDF, StopWordsRemover\n",
    "from pyspark.sql.functions import lower, col, udf, pandas_udf\n",
    "from pyspark.sql.types import FloatType, ArrayType, StringType\n",
    "import pandas as pd\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = spark.read.json(\"/labs/slaba02/DO_record_per_line.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(ArrayType(StringType()))\n",
    "def tokenizer_udf(series):\n",
    "    regex = re.compile(u'[\\w\\d]{2,}', re.U)\n",
    "    words = series.str.findall(regex)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = dataset.withColumn(\"desc_l\", lower(col('desc')))\\\n",
    "                 .withColumn(\"words\", tokenizer_udf('desc_l'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = StopWordsRemover.loadDefaultStopWords(\"english\") + \\\n",
    "             StopWordsRemover.loadDefaultStopWords(\"spanish\") + \\\n",
    "             StopWordsRemover.loadDefaultStopWords(\"russian\")\n",
    "swr = StopWordsRemover(inputCol=\"words\", outputCol=\"words_filtered\", stopWords=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hasher = HashingTF(numFeatures=10000, binary=False, inputCol=swr.getOutputCol(), outputCol=\"tf\")\n",
    "idf = IDF(inputCol=hasher.getOutputCol(), outputCol=\"tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = Pipeline(stages=[\n",
    "    swr,\n",
    "    hasher,\n",
    "    idf\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_model = preprocessing.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = preprocessing_model.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------------------+\n",
      "| id|lang|               tfidf|\n",
      "+---+----+--------------------+\n",
      "|  4|  en|(10000,[36,63,138...|\n",
      "|  5|  en|(10000,[32,222,36...|\n",
      "+---+----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset2 = dataset2.drop(\"cat\", \"desc\", \"desc_l\", \"name\", \"provider\", \"words\", \"words_filtered\", \"tf\").cache()\n",
    "dataset2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(FloatType())\n",
    "def cosin_sim(v, u):\n",
    "      return float(u.dot(v) / (v.norm(2) * u.norm(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ((23126, \"en\"), (21617, \"en\"), (16627, \"es\"), (11556, \"es\"), (16704, \"ru\"), (13702, \"ru\"))\n",
    "result = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in test:\n",
    "    ds2_1 = dataset2.filter(dataset2.id == item[0]).withColumnRenamed(\"id\", \"c_id\")\\\n",
    "                                               .withColumnRenamed(\"lang\", \"c_lang\")\\\n",
    "                                               .withColumnRenamed(\"tfidf\", \"c_tfidf\")\\\n",
    "                                               .cache()\n",
    "    ds2_a = dataset2.filter(dataset2.lang == item[1]).cache()\n",
    "    \n",
    "    dataset3 = ds2_a.join(ds2_1, ds2_a.id != ds2_1.c_id)\\\n",
    "                    .withColumn(\"cosin_sim\", cosin_sim(\"tfidf\", \"c_tfidf\") )\\\n",
    "                    .dropna(subset=\"cosin_sim\")\\\n",
    "                   . orderBy(\"cosin_sim\", ascending=False).limit(10)\n",
    "    \n",
    "    result_list = dataset3.select(\"id\").rdd.flatMap(lambda x: x).collect()\n",
    "    result.update({item[0]: result_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{23126: [13665, 14760, 13782, 20638, 24419, 15909, 2724, 25782, 17499, 13348],\n",
       " 21617: [21609, 21616, 22298, 21608, 21628, 21630, 21081, 21623, 19417, 21508],\n",
       " 16627: [11431, 5687, 17964, 12660, 12247, 17961, 16694, 5558, 11575, 13551],\n",
       " 11556: [16488, 468, 19330, 10447, 23357, 21707, 22710, 13461, 10384, 13776],\n",
       " 16704: [1236, 1247, 1365, 1164, 1273, 20288, 8186, 1233, 8203, 18331],\n",
       " 13702: [864, 28074, 1041, 21079, 8300, 13057, 8313, 21025, 1033, 1111]}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"lab02.json\", \"w\") as json_file:\n",
    "    json.dump(result, json_file, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"23126\": [\r\n",
      "        13665,\r\n",
      "        14760,\r\n",
      "        13782,\r\n",
      "        20638,\r\n",
      "        24419,\r\n",
      "        15909,\r\n",
      "        2724,\r\n",
      "        25782,\r\n",
      "        17499,\r\n",
      "        13348\r\n",
      "    ],\r\n",
      "    \"21617\": [\r\n",
      "        21609,\r\n",
      "        21616,\r\n",
      "        22298,\r\n",
      "        21608,\r\n",
      "        21628,\r\n",
      "        21630,\r\n",
      "        21081,\r\n",
      "        21623,\r\n",
      "        19417,\r\n",
      "        21508\r\n",
      "    ],\r\n",
      "    \"16627\": [\r\n",
      "        11431,\r\n",
      "        5687,\r\n",
      "        17964,\r\n",
      "        12660,\r\n",
      "        12247,\r\n",
      "        17961,\r\n",
      "        16694,\r\n",
      "        5558,\r\n",
      "        11575,\r\n",
      "        13551\r\n",
      "    ],\r\n",
      "    \"11556\": [\r\n",
      "        16488,\r\n",
      "        468,\r\n",
      "        19330,\r\n",
      "        10447,\r\n",
      "        23357,\r\n",
      "        21707,\r\n",
      "        22710,\r\n",
      "        13461,\r\n",
      "        10384,\r\n",
      "        13776\r\n",
      "    ],\r\n",
      "    \"16704\": [\r\n",
      "        1236,\r\n",
      "        1247,\r\n",
      "        1365,\r\n",
      "        1164,\r\n",
      "        1273,\r\n",
      "        20288,\r\n",
      "        8186,\r\n",
      "        1233,\r\n",
      "        8203,\r\n",
      "        18331\r\n",
      "    ],\r\n",
      "    \"13702\": [\r\n",
      "        864,\r\n",
      "        28074,\r\n",
      "        1041,\r\n",
      "        21079,\r\n",
      "        8300,\r\n",
      "        13057,\r\n",
      "        8313,\r\n",
      "        21025,\r\n",
      "        1033,\r\n",
      "        1111\r\n",
      "    ]\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat lab02.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
