{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 5 --executor-memory 4g --executor-cores 1 --driver-memory 2g pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"Konstantin Diakvnishvili lab 3 app\") \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, CountVectorizer, StopWordsRemover, OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.sql.functions import lower, col, udf, pandas_udf, round, split, concat_ws, explode, mean\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType \n",
    "from pyspark.sql.types import ArrayType, DoubleType, BooleanType, FloatType\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType() \\\n",
    "      .add(\"user_id\", IntegerType(), True) \\\n",
    "      .add(\"item_id\", IntegerType(), True) \\\n",
    "      .add(\"purchase\", IntegerType(), True)\n",
    "      \n",
    "df_user = spark.read.format(\"csv\") \\\n",
    "      .option(\"header\", True) \\\n",
    "      .schema(schema) \\\n",
    "      .load(\"/labs/slaba03/laba03_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType() \\\n",
    "      .add(\"user_id\", IntegerType(), True) \\\n",
    "      .add(\"item_id\", IntegerType(), True)\n",
    "      \n",
    "      \n",
    "df_user_test = spark.read.format(\"csv\") \\\n",
    "      .option(\"header\", True) \\\n",
    "      .schema(schema) \\\n",
    "      .load(\"/labs/slaba03/laba03_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_items_schema = StructType(fields=[StructField('item_id', IntegerType()), \n",
    "StructField('channel_id', IntegerType()),\n",
    "StructField('datetime_availability_start', StringType()),\n",
    "StructField('datetime_availability_stop', StringType()),\n",
    "StructField('datetime_show_start', StringType()),\n",
    "StructField('datetime_show_stop', StringType()),\n",
    "StructField('content_type', IntegerType()),\n",
    "StructField('title', StringType(), nullable=True),\n",
    "StructField('year', FloatType(), nullable=True),\n",
    "StructField('genres', StringType()),\n",
    "StructField('region_id', IntegerType()),\n",
    "]) \n",
    "\n",
    "df_items = spark.read.format(\"csv\") \\\n",
    "      .option(\"header\", True) \\\n",
    "      .option(\"sep\", \"\\t\")\\\n",
    "      .schema(read_items_schema) \\\n",
    "      .load(\"/labs/slaba03/laba03_items.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_users_schema = StructType(fields=[StructField('user_id', IntegerType()), \n",
    "StructField('item_id', IntegerType()),\n",
    "StructField('ts_start', IntegerType()),\n",
    "StructField('ts_end', IntegerType()),\n",
    "StructField('item_type', StringType()),\n",
    "]) \n",
    "\n",
    "df_views_programmes = spark.read.format(\"csv\") \\\n",
    "      .option(\"header\", True) \\\n",
    "      .schema(read_users_schema) \\\n",
    "      .load(\"/labs/slaba03/laba03_views_programmes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items = df_items.filter(df_items.content_type == 1)\\\n",
    "                        .withColumn(\"genres\", lower(col('genres'))) \\\n",
    "                        .withColumn(\"title\", lower(col('title'))) \\\n",
    "                        .drop('channel_id', 'datetime_availability_start', 'datetime_availability_stop', \\\n",
    "                              'datetime_show_start', 'datetime_show_stop', 'content_type', 'region_id') \\\n",
    "                        .na.fill(\"\",[\"genres\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(ArrayType(StringType()))\n",
    "def tokenizer_udf(series):\n",
    "    regex = re.compile(u'[\\w\\d]{2,}', re.U)\n",
    "    words = series.str.findall(regex)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items = df_items.withColumn(\"genres_words\", split('genres', ','))\\\n",
    "#                   .withColumn(\"title_words\", tokenizer_udf('title'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = StopWordsRemover.loadDefaultStopWords(\"english\") + \\\n",
    "             StopWordsRemover.loadDefaultStopWords(\"russian\")\n",
    "list_add = ['сурдоперевод', '0', '2', '3', '4', '5', '6', '7', '8','9', 'ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'x']\n",
    "stop_words = stop_words + list_add\n",
    "\n",
    "swr = StopWordsRemover(inputCol=\"title_words\", outputCol=\"title_words_filtered\", stopWords=stop_words)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol=\"genres_words\", outputCol=\"genres_count\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items = cv.fit(df_items).transform(df_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_items = df_user.join(df_items, ['item_id'])\n",
    "df_user_test_items = df_user_test.join(df_items, ['item_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sample = df_user_items.sampleBy('purchase', fractions={0: 0.5, 1:0.5}, seed=5757)\n",
    "df_sample = df_user_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_stat = df_sample.groupby('user_id').agg(mean('purchase').alias('user_purchase_rate')) \\\n",
    "#                        .withColumn('user_purchase_rate', round('user_purchase_rate', 2))\n",
    "df_item_stat = df_sample.groupby('item_id').agg(mean('purchase').alias('item_purchase_rate'))\n",
    "#                        .withColumn('item_purchase_rate', round('item_purchase_rate', 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_items_stat = df_user_items \\\n",
    "                         .join(df_user_stat, on=['user_id'], how='left') \\\n",
    "                         .join(df_item_stat, on=['item_id'], how='left') \\\n",
    "                         .na.fill(0, ['user_purchase_rate', 'item_purchase_rate']) \\\n",
    "                         .repartition(10) \\\n",
    "                         .cache()\n",
    "df_user_test_items_stat = df_user_test_items\\\n",
    "                         .join(df_user_stat, on=['user_id'], how='left') \\\n",
    "                         .join(df_item_stat, on=['item_id'], how='left') \\\n",
    "                         .na.fill(0, ['user_purchase_rate', 'item_purchase_rate']) \\\n",
    "                         .repartition(10) \\\n",
    "                         .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hasher = HashingTF(numFeatures=5000, binary=False, inputCol=\"title_words\", outputCol=\"title_freq\")\n",
    "assembler = VectorAssembler(inputCols=[\"user_purchase_rate\",\"item_purchase_rate\", \"genres_count\"], outputCol=\"features\")\n",
    "gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"purchase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "#    swr,\n",
    "#    hasher,\n",
    "    assembler,\n",
    "    gbt\n",
    "])\n",
    "\n",
    "pipeline_model = pipeline.fit(df_user_items_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pipeline_model.transform(df_user_test_items_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_df = prediction.select('user_id', 'item_id', 'probability').toPandas()\n",
    "test_predictions_df['purchase'] = test_predictions_df['probability'].apply(lambda x: x[1])\n",
    "test_predictions_df = test_predictions_df.drop(['probability'], axis=1).sort_values(['user_id', 'item_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_df.to_csv('lab03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
