{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 --executor-memory 3g --driver-memory 2g pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"Bulatov Nikolai ML lab3\") \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).appName(\"Bulatov Nikolai ML lab3\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -ls /labs/slaba03/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -cat /labs/slaba03/laba03_views_programmes.csv | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType, FloatType, ArrayType\n",
    "\n",
    "item_schema = StructType([\n",
    "    StructField(\"item_id\", IntegerType()),\n",
    "    StructField(\"channel_id\", IntegerType()),\n",
    "    StructField(\"datetime_availability_start\", TimestampType()),\n",
    "    StructField(\"datetime_availability_stop\", TimestampType()),\n",
    "    StructField(\"datetime_show_start\", TimestampType()),\n",
    "    StructField(\"datetime_show_stop\", TimestampType()),\n",
    "    StructField(\"content_type\", IntegerType()),\n",
    "    StructField(\"title\", StringType()),\n",
    "    StructField(\"year\", FloatType()),\n",
    "    StructField(\"genres\", StringType()),\n",
    "    StructField(\"region_id\", IntegerType())\n",
    "])\n",
    "\n",
    "item_df = spark.read\\\n",
    "          .format(\"csv\")\\\n",
    "          .option(\"header\", \"true\")\\\n",
    "          .schema(item_schema)\\\n",
    "          .option(\"sep\", \"\\t\")\\\n",
    "          .load(\"/labs/slaba03/laba03_items.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_df.filter(\"content_type = 1\").groupBy(\"genres\").count().orderBy(\"count\", ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_df.filter(\"item_id = 7101053\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "views_schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType()),\n",
    "    StructField(\"item_id\", IntegerType()),\n",
    "    StructField(\"ts_start\", IntegerType()),\n",
    "    StructField(\"ts_end\", IntegerType()),\n",
    "    StructField(\"item_type\", StringType())\n",
    "])\n",
    "\n",
    "views_df = spark.read\\\n",
    "          .format(\"csv\")\\\n",
    "          .option(\"header\", \"true\")\\\n",
    "          .schema(views_schema)\\\n",
    "          .option(\"sep\", \",\")\\\n",
    "          .load(\"/labs/slaba03/laba03_views_programmes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "views_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "views_df.groupBy(\"item_type\").count().orderBy(\"count\", ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_agg_df = views_df\\\n",
    "    .join(item_df, \"item_id\", \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_agg_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType()),\n",
    "    StructField(\"item_id\", IntegerType()),\n",
    "    StructField(\"purchase\", IntegerType())\n",
    "])\n",
    "\n",
    "train_df = spark.read\\\n",
    "          .format(\"csv\")\\\n",
    "          .option(\"header\", \"true\")\\\n",
    "          .schema(test_schema)\\\n",
    "          .option(\"sep\", \",\")\\\n",
    "          .load(\"/labs/slaba03/laba03_train.csv\")\n",
    "\n",
    "train_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "item_agg_df = item_df.select(\"item_id\", \"year\",\\\n",
    "                             f.regexp_replace(\"title\", r\"[^\\pL0-9\\p{Space}]\",\"\" ).alias(\"title\"),\\\n",
    "                             f.regexp_replace(f.regexp_replace(f.regexp_replace(f.trim(f.col(\"genres\")) , r\"[^\\pL0-9,\\p{Space}]\",\"\"),\" \",\"_\"),\",\",\" \").alias(\"genres\"))\\\n",
    "    .withColumn(\"item_year_group\",\\\n",
    "                f.when(f.col(\"year\") < f.lit(1970), f.lit(\"1960\"))\\\n",
    "                 .when(f.col(\"year\") < f.lit(1980), f.lit(\"1970\"))\\\n",
    "                 .when(f.col(\"year\") < f.lit(1990), f.lit(\"1980\"))\\\n",
    "                 .when(f.col(\"year\") < f.lit(2000), f.lit(\"1990\"))\\\n",
    "                 .when(f.col(\"year\") < f.lit(2005), f.lit(\"2000\"))\\\n",
    "                 .when(f.col(\"year\") < f.lit(2010), f.lit(\"2005\"))\\\n",
    "                 .when(f.col(\"year\") < f.lit(2015), f.lit(\"2010\"))\\\n",
    "                 .otherwise(f.col(\"year\").cast(\"int\").cast(\"string\")))\\\n",
    "    .join(train_df.filter(\"purchase = 1\"), \"item_id\", \"left\")\\\n",
    "    .groupBy(\"item_id\", \"year\", \"title\", \"genres\", \"item_year_group\").agg(f.count(f.lit(1)).alias(\"item_pay_cnt\"))\\\n",
    "    .withColumn(\"item_pay_rate\", f.col(\"item_pay_cnt\") / f.count(f.lit(1)).over(Window.rowsBetween(-sys.maxsize,sys.maxsize)))\\\n",
    "    .fillna( { \"genres\":\"n/a\", \n",
    "               \"item_year_group\":0, \n",
    "               \"year\":0,\n",
    "               \"item_pay_cnt\":0,\n",
    "               \"item_pay_rate\":0.0} ).cache()\n",
    "item_agg_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_agg_df.select(\"item_pay_rate\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agg_df = train_df.filter(\"purchase = 1\")\\\n",
    "    .groupBy(\"user_id\").agg(f.sum(\"purchase\").alias(\"user_buy_cnt\"))\\\n",
    "    .withColumn(\"user_buy_rate\", f.col(\"user_buy_cnt\") / f.count(f.lit(1)).over(Window.rowsBetween(-sys.maxsize,sys.maxsize)))\\\n",
    "\n",
    "user_agg_df.show(10)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "year_window = Window.partitionBy(\"user_id\", \"item_year_group\")\n",
    "year_rnk_window = Window.partitionBy(\"user_id\").orderBy(f.col(\"year_cnt\").desc())\n",
    "\n",
    "user_year_agg_df = train_df.filter(\"purchase = 1\")\\\n",
    "    .join(item_agg_df, \"item_id\", \"inner\")\\\n",
    "    .withColumn(\"year_cnt\", f.count(f.lit(1)).over(year_window))\\\n",
    "    .withColumn(\"year_rank\", f.dense_rank().over(year_rnk_window))\\\n",
    "    .filter(\"year_rank <= 3\")\\\n",
    "    .select(\"user_id\", \"item_year_group\").distinct()\\\n",
    "    .groupBy(\"user_id\").agg(f.collect_list(\"item_year_group\").alias(\"top_3_year_group\")).cache()\n",
    "\n",
    "user_year_agg_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "genre_rnk_window = Window.partitionBy(\"user_id\").orderBy(f.col(\"count\").desc())\n",
    "\n",
    "user_genre_agg_df = train_df\\\n",
    "    .join(item_agg_df, \"item_id\", \"inner\")\\\n",
    "    .select(\"user_id\", \"purchase\", f.explode(f.split(\"genres\",\" \")).alias(\"genre_item\"))\\\n",
    "    .groupBy(\"user_id\", \"genre_item\").agg(f.sum(\"purchase\").alias(\"count\"))\\\n",
    "    .withColumn(\"genre_rank\", f.dense_rank().over(genre_rnk_window))\\\n",
    "    .filter(\"genre_rank <= 3\")\\\n",
    "    .select(\"user_id\", \"genre_item\").distinct()\\\n",
    "    .groupBy(\"user_id\").agg(f.collect_list(\"genre_item\").alias(\"top_3_genre_group\")).cache()\n",
    "\n",
    "user_genre_agg_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Transformer, Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, StringIndexer, OneHotEncoder, HashingTF, IDF, VectorAssembler\n",
    "from pyspark.ml.param.shared import HasOutputCol, HasInputCol\n",
    "from pyspark import keyword_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetItemInfoTransformer(Transformer):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None):\n",
    "        super(GetItemInfoTransformer, self).__init__()\n",
    "            \n",
    "    def _transform(self, dataset):\n",
    "        return dataset.join(item_agg_df, \"item_id\", \"left\")\n",
    "    #dataset.withColumn(self.getOutputCol(), f.md5(f.col(self.getInputCol()).cast(\"string\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_transformer = GetItemInfoTransformer()\n",
    "year_group_indexer = StringIndexer(inputCol=\"item_year_group\", outputCol=\"yearGIndex\")\n",
    "year_group_vector = OneHotEncoder(inputCol=year_group_indexer.getOutputCol(), outputCol=\"year_group_vec\")\n",
    "genre_tokenizer = Tokenizer(inputCol=\"genres\", outputCol=\"genres_tok\")\n",
    "title_tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"title_tok\")\n",
    "genre_converter = CountVectorizer(inputCol = genre_tokenizer.getOutputCol(), outputCol=\"genres_vec\", binary=True)\n",
    "stop_words =\\\n",
    "    StopWordsRemover.loadDefaultStopWords(\"russian\") + \\\n",
    "    StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "title_swr=StopWordsRemover(inputCol=\"title_tok\", outputCol=\"title_swr\", stopWords=stop_words)\n",
    "title_htf = HashingTF(inputCol=title_swr.getOutputCol(), outputCol=\"title_tf\", numFeatures=1000)\n",
    "title_idf = IDF(inputCol=title_htf.getOutputCol(), outputCol=\"title_idf\")\n",
    "\n",
    "item_transformer_list = [\\\n",
    "        item_transformer,\\\n",
    "        year_group_indexer,\\\n",
    "        year_group_vector,\\\n",
    "        genre_tokenizer,\\\n",
    "        genre_converter,\\\n",
    "        title_tokenizer,\\\n",
    "        title_swr,\\\n",
    "        title_htf,\\\n",
    "        title_idf\\\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_item = Pipeline(stages=item_transformer_list)\n",
    "transform_item_model = transform_item.fit(train_df)\n",
    "transform_item_model.transform(train_df).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetUserInfoTransformer(Transformer):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None):\n",
    "        super(GetUserInfoTransformer, self).__init__()\n",
    "            \n",
    "    def _transform(self, dataset):\n",
    "        df = dataset\\\n",
    "            .join(user_agg_df, \"user_id\", \"left\")\\\n",
    "            .join(user_year_agg_df, \"user_id\", \"left\")\\\n",
    "            .join(user_genre_agg_df, \"user_id\", \"left\")\\\n",
    "            .withColumn(\"top_3_genre_group_\", f.coalesce(\"top_3_genre_group\", f.array().cast(\"array<string>\")))\\\n",
    "            .withColumn(\"top_3_year_group_\", f.coalesce(\"top_3_year_group\", f.array().cast(\"array<string>\")))\\\n",
    "            .fillna({ \"user_buy_cnt\":0,\n",
    "                      \"user_buy_rate\":0.0\n",
    "                    })\n",
    "        return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_transformer = GetUserInfoTransformer()\n",
    "genre_top_converter = CountVectorizer(inputCol = \"top_3_genre_group_\", outputCol=\"top_genres_vec\", binary=True)\n",
    "year_top_converter = CountVectorizer(inputCol = \"top_3_year_group_\", outputCol=\"top_year_vec\", binary=True)\n",
    "\n",
    "user_transformer_list = [\n",
    "         user_transformer\n",
    "        ,genre_top_converter\n",
    "        ,year_top_converter\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_user = Pipeline(stages=user_transformer_list)\n",
    "transform_user_model = transform_user.fit(train_df)\n",
    "transform_user_model.transform(train_df).cache().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list=[\n",
    "\"year_group_vec\",\n",
    "\"item_pay_rate\",\n",
    "\"title_idf\",\n",
    "\"genres_vec\",\n",
    "\"user_buy_rate\",\n",
    "\"top_genres_vec\",\n",
    "\"top_year_vec\"\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_list, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType()),\n",
    "    StructField(\"item_id\", IntegerType()),\n",
    "    StructField(\"purchase\", IntegerType())\n",
    "])\n",
    "\n",
    "test_df = spark.read\\\n",
    "          .format(\"csv\")\\\n",
    "          .option(\"header\", \"true\")\\\n",
    "          .schema(test_schema)\\\n",
    "          .option(\"sep\", \",\")\\\n",
    "          .load(\"/labs/slaba03/laba03_test.csv\")\n",
    "\n",
    "test_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features', labelCol=\"purchase\" , maxIter=200,regParam=0.1)\n",
    "lr_pipeline_stages = item_transformer_list + user_transformer_list + [assembler, lr]\n",
    "\n",
    "estimator = Pipeline(stages=lr_pipeline_stages)\n",
    "lr_model = estimator.fit(train_df)\n",
    "train_res = lr_model.transform(train_df).cache()\n",
    "\n",
    "train_res.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"probability\", labelCol=\"purchase\", metricName='areaUnderROC')\n",
    "\n",
    "roc=evaluator.evaluate(train_res)\n",
    "roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "predictions = lr_model.transform(test_df).cache()\n",
    "\n",
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select(\"probability\").take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@f.udf(ArrayType(FloatType()))\n",
    "def to_list(dense_vector):\n",
    "    return dense_vector.toArray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab03_csv = predictions.select(\"user_id\",\"item_id\", to_list(\"probability\").getItem(1).alias(\"purchase\")).orderBy(\"user_id\",\"item_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab03_csv.toPandas().to_csv('lab03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
