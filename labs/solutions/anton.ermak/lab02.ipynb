{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В ноутбуке: токенизирует описания к курсам, приводим в векторную форму, считаем меру близости и ранжируем "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spark.sesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 2 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import Row\n",
    "import json\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .config(conf=conf)\n",
    "         .appName(\"lab02\")\n",
    "         .getOrCreate())\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача\n",
    "По имеющимся данным портала eclass.cc построить content-based рекомендации по образовательным курсам. Запрещено использовать библиотеки pandas, sklearn и аналогичные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Описание данных\n",
    "Имеются следующие данные на вход:\n",
    "\n",
    "- набор данных о всех курсах. Датасет можно взять с HDFS по адресу: `/labs/slaba02/DO_record_per_line.json`\n",
    "- `id` курсов, для которых надо дать рекомендации (указаны в Личном кабинете)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные выглядят следующим образом:\n",
    "\n",
    "```\n",
    "{\"lang\": \"en\",\n",
    "\"name\": \"Accounting Cycle: The Foundation of Business Measurement and Reporting\",\n",
    "\"cat\": \"3/business_management|6/economics_finance\",\n",
    "\"provider\": \"Canvas Network\",\n",
    "\"id\": 4,\n",
    "\"desc\": \"This course introduces the basic financial statements used by most businesses, as well as the essential tools used to prepare them. This course will serve as a resource to help business students succeed in their upcoming university-level accounting classes, and as a refresher for upper division accounting students who are struggling to recall elementary concepts essential to more advanced accounting topics. Business owners will also benefit from this class by gaining essential skills necessary to organize and manage information pertinent to operating their business. At the conclusion of the class, students will understand the balance sheet, income statement, and cash flow statement. They will be able to differentiate between cash basis and accrual basis techniques, and know when each is appropriate. They\\u2019ll also understand the accounting equation, how to journalize and post transactions, how to adjust and close accounts, and how to prepare key financial reports. All material for this class is written and delivered by the professor, and can be previewed here. Students must have access to a spreadsheet program to participate.\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   3 hdfs hdfs   69519728 2022-01-06 18:46 /labs/slaba02/DO_record_per_line.json\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /labs/slaba02/DO_record_per_line.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "|                 cat|                desc| id|lang|                name|      provider|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "|3/business_manage...|This course intro...|  4|  en|Accounting Cycle:...|Canvas Network|\n",
      "|              11/law|This online cours...|  5|  en|American Counter ...|Canvas Network|\n",
      "|5/computer_scienc...|This course is ta...|  6|  fr|Arithmétique: en ...|Canvas Network|\n",
      "|  14/social_sciences|We live in a digi...|  7|  en|Becoming a Dynami...|Canvas Network|\n",
      "|2/biology_life_sc...|This self-paced c...|  8|  en|           Bioethics|Canvas Network|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "courses_desc = spark.read.json('/labs/slaba02/DO_record_per_line.json')\n",
    "courses_desc.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Результат\n",
    "Для каждого id курса из личного кабинета необходимо дать топ-10 наиболее похожих на него курсов. Рекомендованные курсы должны быть того же языка, что и курс, для которого строится рекомендация.\n",
    "\n",
    "Выходной формат — json — должен иметь следующую структуру:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"123\": [\n",
    "    5372,\n",
    "    16663,\n",
    "    23114,\n",
    "    13079,\n",
    "    13084,\n",
    "    ...\n",
    "  ],\n",
    "  \"456\": [\n",
    "    ...\n",
    "  ],\n",
    "  \"789\": [\n",
    "    ...\n",
    "  ],\n",
    "  \"123456\": [\n",
    "    ...\n",
    "  ],\n",
    "  \"456789\": [\n",
    "    ...\n",
    "  ],\n",
    "  \"987654\": [\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Советы\n",
    "Для подбора рекомендаций следует использовать меру TFIDF, а в качестве метрики для ранжирования — косинус угла между TFIDF-векторами для разных курсов.\n",
    "\n",
    "Что такое TFIDF? TF — это term frequency: по сути, сколько раз слово встречается в этом документе. Если мы сделаем такой word count по каждому документу, то получим вектор, который как-то характеризует этот документ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы сравним вектора, рассчитав дистанцую между ними, то получим вывод – насколько похожи эти тексты. Назовем этот подход наивным.\n",
    "\n",
    "Этот подход наивен, потому что мы как бы присваиваем одинаковый вес каждому слову, которое у нас есть в тексте. А что если мы попробуем как-то повысить значимость тех слов, которые часто встречаются только в этом тексте? Для этого мы посчитаем DF – document frequency: по сути, число документов, в которых есть вхождение этого слова. Мы хотим \"штрафовать\" слово за частое появление в документах, поэтому делаем инверсию этой величины – буква I в TFIDF. Теперь для каждого слова мы будем считать TF и делить на IDF. Так мы получим другой вектор для нашего документа. Он может быть более правильным для наших задач.\n",
    "\n",
    "TFIDF нужно считать для описаний курсов (desc). При извлечении слов из описания словом считаем то, что состоит из латинских или кириллических букв или цифр, знаки препинания и прочие символы не учитываются.\n",
    "\n",
    "Для поиска слов можно использовать такой код на Python (может быть проблема с распознаванием юникода)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "regex = re.compile(u'[\\w\\d]{2,}', re.U)\n",
    "regex.findall(string.lower())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сам TFIDF реализован в Spark, писать с нуля вычисления не требуется. При вычислении TF с помощью HashingTF использовалось число фичей: 10000. То есть: `tf = HashingTF(10000)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пайплайн = Препроцессинг + Токенизатор + HashingTF + IDF + join dataset'ов + cos_sim (udf) + формирование рек-ций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, HashingTF, IDF, Normalizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import udf, col, lower, isnan, isnull, broadcast, desc, lower\n",
    "from pyspark.sql.types import FloatType, ArrayType, StringType\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "|                 cat|                desc| id|lang|                name|      provider|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "|3/business_manage...|This course intro...|  4|  en|Accounting Cycle:...|Canvas Network|\n",
      "|              11/law|This online cours...|  5|  en|American Counter ...|Canvas Network|\n",
      "|5/computer_scienc...|This course is ta...|  6|  fr|Arithmétique: en ...|Canvas Network|\n",
      "|  14/social_sciences|We live in a digi...|  7|  en|Becoming a Dynami...|Canvas Network|\n",
      "|2/biology_life_sc...|This self-paced c...|  8|  en|           Bioethics|Canvas Network|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "courses_desc = spark.read.json('/labs/slaba02/DO_record_per_line.json')\n",
    "courses_desc.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = courses_desc.select('id','lang','desc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28153"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27810"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc.select('desc').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"desc\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = tokenizer.transform(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordListRus = StopWordsRemover.loadDefaultStopWords(\"russian\")\n",
    "stopwordListEng = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "stopwordListEsp = StopWordsRemover.loadDefaultStopWords(\"spanish\")\n",
    "\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", \n",
    "                           outputCol=\"words_wo_stops\" ,\n",
    "                           stopWords=stopwordListRus + stopwordListEng + stopwordListEsp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = remover.transform(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------------------+--------------------+--------------------+\n",
      "| id|lang|                desc|               words|      words_wo_stops|\n",
      "+---+----+--------------------+--------------------+--------------------+\n",
      "|  4|  en|This course intro...|[this, course, in...|[course, introduc...|\n",
      "|  5|  en|This online cours...|[this, online, co...|[online, course, ...|\n",
      "|  6|  fr|This course is ta...|[this, course, is...|[course, taught, ...|\n",
      "+---+----+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compute TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasher_freq = HashingTF(numFeatures=10000, \n",
    "                        binary=True, \n",
    "                        inputCol='words_wo_stops', \n",
    "                        outputCol=\"word_vector_freq\")\n",
    "DATA_freq = hasher_freq.transform(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------------------+--------------------+--------------------+--------------------+\n",
      "| id|lang|                desc|               words|      words_wo_stops|    word_vector_freq|\n",
      "+---+----+--------------------+--------------------+--------------------+--------------------+\n",
      "|  4|  en|This course intro...|[this, course, in...|[course, introduc...|(10000,[36,42,63,...|\n",
      "|  5|  en|This online cours...|[this, online, co...|[online, course, ...|(10000,[32,222,29...|\n",
      "|  6|  fr|This course is ta...|[this, course, is...|[course, taught, ...|(10000,[30,41,246...|\n",
      "|  7|  en|We live in a digi...|[we, live, in, a,...|[live, digitally,...|(10000,[493,721,8...|\n",
      "|  8|  en|This self-paced c...|[this, self-paced...|[self-paced, cour...|(10000,[32,65,115...|\n",
      "|  9|  en|This game-based c...|[this, game-based...|[game-based, cour...|(10000,[56,268,30...|\n",
      "| 10|  en|What’s in your di...|[what’s, in, your...|[what’s, digital,...|(10000,[1045,2044...|\n",
      "| 11|  en|The goal of the D...|[the, goal, of, t...|[goal, digital, l...|(10000,[87,157,15...|\n",
      "| 12|  en|Ready to explore ...|[ready, to, explo...|[ready, explore, ...|(10000,[161,164,8...|\n",
      "| 13|  en|This self-paced c...|[this, self-paced...|[self-paced, cour...|(10000,[26,1072,1...|\n",
      "| 14|  en|What is “interpro...|[what, is, “inter...|[“interprofession...|(10000,[63,145,23...|\n",
      "| 15|  en|This course prese...|[this, course, pr...|[course, presents...|(10000,[32,65,77,...|\n",
      "| 16|  en|Chemistry is an i...|[chemistry, is, a...|[chemistry, integ...|(10000,[32,273,30...|\n",
      "| 17|  en|Are you consideri...|[are, you, consid...|[considering, car...|(10000,[695,2486,...|\n",
      "| 18|  en|Princess stories ...|[princess, storie...|[princess, storie...|(10000,[316,364,6...|\n",
      "| 19|  en|This first instal...|[this, first, ins...|[first, installme...|(10000,[768,855,1...|\n",
      "| 20|  en|This course exami...|[this, course, ex...|[course, examines...|(10000,[273,317,4...|\n",
      "| 21|  en|This course will ...|[this, course, wi...|[course, explore,...|(10000,[148,157,1...|\n",
      "| 22|  en|The field of tech...|[the, field, of, ...|[field, technical...|(10000,[128,177,2...|\n",
      "| 23|  en|Are you a Higher ...|[are, you, a, hig...|[higher, ed, inst...|(10000,[332,527,6...|\n",
      "+---+----+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_freq.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|word_vector_freq                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(10000,[36,42,63,138,150,157,177,204,362,534,603,724,766,1023,1072,1355,1390,1446,1463,1523,1670,1697,1712,2015,2092,2414,2460,2523,2577,2757,3034,3231,3368,3496,3792,3834,3849,3869,3903,3986,4114,4140,4224,4295,4364,4372,4436,4978,5017,5374,6158,6245,6395,6470,6541,6642,6697,6863,7008,7282,7290,7298,7688,7735,7772,7779,7956,7973,8140,8164,8370,8534,8579,8624,8644,8922,9328,9347,9540,9605,9953,9970],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_freq.select('word_vector_freq').show(1,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28153"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_freq.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compute idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"word_vector_freq\", \n",
    "          outputCol=\"features\").fit(DATA_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = idf.transform(DATA_freq.select('id','lang','word_vector_freq'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=4, lang='en', word_vector_freq=SparseVector(10000, {36: 1.0, 42: 1.0, 63: 1.0, 138: 1.0, 150: 1.0, 157: 1.0, 177: 1.0, 204: 1.0, 362: 1.0, 534: 1.0, 603: 1.0, 724: 1.0, 766: 1.0, 1023: 1.0, 1072: 1.0, 1355: 1.0, 1390: 1.0, 1446: 1.0, 1463: 1.0, 1523: 1.0, 1670: 1.0, 1697: 1.0, 1712: 1.0, 2015: 1.0, 2092: 1.0, 2414: 1.0, 2460: 1.0, 2523: 1.0, 2577: 1.0, 2757: 1.0, 3034: 1.0, 3231: 1.0, 3368: 1.0, 3496: 1.0, 3792: 1.0, 3834: 1.0, 3849: 1.0, 3869: 1.0, 3903: 1.0, 3986: 1.0, 4114: 1.0, 4140: 1.0, 4224: 1.0, 4295: 1.0, 4364: 1.0, 4372: 1.0, 4436: 1.0, 4978: 1.0, 5017: 1.0, 5374: 1.0, 6158: 1.0, 6245: 1.0, 6395: 1.0, 6470: 1.0, 6541: 1.0, 6642: 1.0, 6697: 1.0, 6863: 1.0, 7008: 1.0, 7282: 1.0, 7290: 1.0, 7298: 1.0, 7688: 1.0, 7735: 1.0, 7772: 1.0, 7779: 1.0, 7956: 1.0, 7973: 1.0, 8140: 1.0, 8164: 1.0, 8370: 1.0, 8534: 1.0, 8579: 1.0, 8624: 1.0, 8644: 1.0, 8922: 1.0, 9328: 1.0, 9347: 1.0, 9540: 1.0, 9605: 1.0, 9953: 1.0, 9970: 1.0}), features=SparseVector(10000, {36: 3.8519, 42: 4.4647, 63: 3.5913, 138: 3.7562, 150: 2.9256, 157: 1.7886, 177: 3.3297, 204: 4.0634, 362: 3.5026, 534: 1.9127, 603: 5.3038, 724: 3.8222, 766: 5.0637, 1023: 2.9059, 1072: 1.632, 1355: 2.3396, 1390: 4.2615, 1446: 2.4134, 1463: 3.4232, 1523: 1.4921, 1670: 3.4157, 1697: 1.5605, 1712: 3.6121, 2015: 2.9998, 2092: 3.9319, 2414: 3.9033, 2460: 0.5629, 2523: 4.4709, 2577: 3.5926, 2757: 4.9031, 3034: 3.2478, 3231: 2.4816, 3368: 4.535, 3496: 1.8112, 3792: 1.4488, 3834: 2.0125, 3849: 1.5913, 3869: 4.8473, 3903: 3.2032, 3986: 3.3723, 4114: 5.098, 4140: 2.2789, 4224: 2.2056, 4295: 4.4313, 4364: 3.9935, 4372: 3.8029, 4436: 3.8125, 4978: 2.135, 5017: 3.6429, 5374: 1.7756, 6158: 3.3347, 6245: 4.264, 6395: 2.7844, 6470: 3.7367, 6541: 3.1858, 6642: 4.724, 6697: 2.9884, 6863: 3.2635, 7008: 3.1274, 7282: 3.028, 7290: 4.647, 7298: 3.7001, 7688: 4.5517, 7735: 3.8029, 7772: 4.0531, 7779: 1.5401, 7956: 3.716, 7973: 3.499, 8140: 5.2687, 8164: 1.6804, 8370: 5.2415, 8534: 2.3294, 8579: 2.2076, 8624: 3.2994, 8644: 2.5308, 8922: 2.8528, 9328: 3.8946, 9347: 1.533, 9540: 3.3287, 9605: 2.0441, 9953: 4.6619, 9970: 1.9238}))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.take(1)#show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28153"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- word_vector_freq: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------------------+\n",
      "| id|lang|            features|\n",
      "+---+----+--------------------+\n",
      "|  4|  en|(10000,[36,42,63,...|\n",
      "|  5|  en|(10000,[32,222,29...|\n",
      "|  6|  fr|(10000,[30,41,246...|\n",
      "|  7|  en|(10000,[493,721,8...|\n",
      "|  8|  en|(10000,[32,65,115...|\n",
      "|  9|  en|(10000,[56,268,30...|\n",
      "| 10|  en|(10000,[1045,2044...|\n",
      "| 11|  en|(10000,[87,157,15...|\n",
      "| 12|  en|(10000,[161,164,8...|\n",
      "| 13|  en|(10000,[26,1072,1...|\n",
      "| 14|  en|(10000,[63,145,23...|\n",
      "| 15|  en|(10000,[32,65,77,...|\n",
      "| 16|  en|(10000,[32,273,30...|\n",
      "| 17|  en|(10000,[695,2486,...|\n",
      "| 18|  en|(10000,[316,364,6...|\n",
      "| 19|  en|(10000,[768,855,1...|\n",
      "| 20|  en|(10000,[273,317,4...|\n",
      "| 21|  en|(10000,[148,157,1...|\n",
      "| 22|  en|(10000,[128,177,2...|\n",
      "| 23|  en|(10000,[332,527,6...|\n",
      "+---+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf = tfidf.select('id','lang','features')\n",
    "tfidf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### similarity calc for specific descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## func \n",
    "cosine_similarity = udf(lambda v, u: float(v.dot(u) / (v.norm(2) * u.norm(2))), FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23126 is done\n",
      "21617 is done\n",
      "16627 is done\n",
      "11556 is done\n",
      "16704 is done\n",
      "13702 is done\n"
     ]
    }
   ],
   "source": [
    "cosine_similarity = udf(lambda v, u: float(v.dot(u) / (v.norm(2) * u.norm(2))), FloatType())\n",
    "\n",
    "# individual ids from personal account\n",
    "film_list = [23126, 21617, 16627, 11556, 16704, 13702]\n",
    "answers = {}\n",
    "\n",
    "for id_ in film_list:\n",
    "    single_data = (tfidf.filter(F.col('id') == id_)\n",
    "                   .withColumnRenamed('id','single_id')\n",
    "                   .withColumnRenamed('features','single_feature')\n",
    "                   .join(\n",
    "                       tfidf.withColumn('single_id', F.lit(id_)),\n",
    "                       how='left',\n",
    "                       on=['single_id','lang'])\n",
    "                   .filter(F.col('id') != id_)\n",
    "                   .withColumn('cosine', cosine_similarity(F.col('single_feature'), F.col('features')))\n",
    "                   .filter(~F.isnan(F.col('cosine')))\n",
    "                   .sort(F.col('cosine').desc())\n",
    "                  )\n",
    "\n",
    "    ids = single_data.select(F.col('id')).take(10)\n",
    "    answers[id_] = [ids[i][0] for i in range(10)]\n",
    "    \n",
    "    print(str(id_) + ' is done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{23126: [25782, 23718, 7222, 14760, 23822, 11528, 24373, 5550, 13665, 25468],\n",
       " 21617: [21609, 21608, 21616, 21492, 21700, 21716, 21703, 21706, 21587, 21618],\n",
       " 16627: [11431, 12247, 16694, 5356, 9563, 5680, 5687, 23506, 17964, 23369],\n",
       " 11556: [12679, 22710, 16488, 17910, 468, 387, 19394, 18005, 272, 12884],\n",
       " 16704: [1236, 8186, 1164, 1365, 875, 8207, 8154, 1376, 1219, 20645],\n",
       " 13702: [864, 21079, 13057, 1041, 1033, 915, 1217, 1216, 1173, 21025]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lab02.json\", \"w\") as f:\n",
    "    json.dump(answers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
