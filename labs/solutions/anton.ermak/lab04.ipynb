{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В ноутбуке: строим модели предсказания пола и возраста с spark streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[GIT Link](https://github.com/newprolab/sber-spark-ds-10/blob/main/labs/lab04.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spark.sesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 2 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import Row\n",
    "import json\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .config(conf=conf)\n",
    "         .appName(\"lab04\")\n",
    "         .getOrCreate())\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача\n",
    "У вас имеются данные: логи посещения пользователей по разным сайтам рунета. По этим пользователям вам также известны: пол и возрастная категория. Вам нужно будет в real-time спрогнозировать эти характеристики по пользователям, о которых у вас нет этой информации, но будут все те же самые логи посещения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Описание данных\n",
    "Обучающая выборка, с которой вы будете работать, выглядит следующим образом:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "gender\tage\tuid\tuser_json\n",
    "F\t18-24\td50192e5-c44e-4ae8-ae7a-7cfe67c8b777\t{\"visits\": [{\"url\": \"http://zebra-zoya.ru/200028-chehol-organayzer-dlja-macbook-11-grid-it.html?utm_campaign=397720794&utm_content=397729344&utm_medium=cpc&utm_source=begun\", \"timestamp\": 1419688144068}, {\"url\": \"http://news.yandex.ru/yandsearch?cl4url=chezasite.com/htc/htc-one-m9-delay-86327.html&lr=213&rpt=story\", \"timestamp\": 1426666298001}, {\"url\": \"http://www.sotovik.ru/news/240283-htc-one-m9-zaderzhivaetsja.html\", \"timestamp\": 1426666298000}, {\"url\": \"http://news.yandex.ru/yandsearch?cl4url=chezasite.com/htc/htc-one-m9-delay-86327.html&lr=213&rpt=story\", \"timestamp\": 1426661722001}, {\"url\": \"http://www.sotovik.ru/news/240283-htc-one-m9-zaderzhivaetsja.html\", \"timestamp\": 1426661722000}]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Она расположена на hdfs: `/labs/slaba04/`\n",
    "- Поле `gender` принимает значения `F` (женщина) и `M` (мужчина)\n",
    "- Поле `age` принимает значения диапазона возраста: `18-24`, `25-34`, `35-44`, `45-54`, `>=55`\n",
    "- Поле `uid` принимает значения уникального ID пользователя (cookies): d50192e5-c44e-4ae8-ae7a-7cfe67c8b777.\n",
    "- Поле `user_json` имеет внутри json со следующей схемой данных: `{\"visits\": [{\"url\": \"url1\", \"timestamp\": \"timestamp1\"}, {\"url\": \"url2\", \"timestamp\": \"timestamp2\"}]}`. В нем содержатся непосредственно логи посещения пользователем страниц вместе с временной меткой посещения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные, по которым вам надо будет построить прогноз (тестовая выборка), хранятся в `Kafka` в таком виде:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{\"uid\": \"bdd48781-8243-493d-8ae6-794050a4417f\",\n",
    " \"visits\": \"[{\"url\": \"http://vk.com/feed\", \"timestamp\": 1425077901001}, {\"url\": \"http://big-cards.a5ltd.com/app/html/vk.html?api_url=http://api.vk.com/api.php&api_id=1804162&api_settings=663823&viewer_id=40849488&viewer_type=2&sid=1b77e9ca975573fed6e31746ca58ea2509f3588d918e7dab2a291f7f7579ae7dab6c21058c837b74b1fbb&secret=37339dfcbf&access_token=06c53bd8f9f9a0afad23b4e166e448495396259fc5a1c5e5243474df82591bd5d0afe877d993ea4a82a81&user_id=40849488&group_id=0&is_app_user=1&auth_key=d7c91ad4bb27e20cb3ed81810f5649db&language=0&parent_language=0&ad_info=elsdcqvcsvftaqxtawjsxht b0q8htjxuvbbjrvbnwojfji2ha8h&is_secure=0&ads_app_id=1804162_903aa103fa48b1e378&referrer=menu&lc_name=2c073eed&hash=\", \"timestamp\": 1425077901000}]\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Название топика, в который мы будем присылать данные: `input_ivan.ivanov`, где вместо `ivan.ivanov` ваш логин от личного кабинета. Данные вам будут поступать по нажатию кнопки \"Проверить\" в личном кабинете. **Вам топик создавать не нужно**. Он будет автоматом создаваться при записи. Чистить его тоже не надо, потому что Spark Streaming хранит оффсеты, то есть знает, что он уже считывал, что еще не считывал.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Порт брокера Kafka: `6667`. Hostname: `spark-master-1.newprolab.com` или `spark-node-1.newprolab.com`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Результат\n",
    "Вам в свою очередь нужно будет, считывая данные из Kafka, делать прогноз по ним в Spark Streaming и отправлять в real-time эти прогнозные значения в свой другой топик в Kafka в формате:\n",
    "```\n",
    "{\"uid\": \"fe1dba8f-3131-439f-9031-851c0da0f126\", \"gender\": \"M\", \"age\": \"25-34\"}\n",
    "{\"uid\": \"d50192e5-c44e-4ae8-ae7a-7cfe67c8b777\", \"gender\": \"F\", \"age\": \"18-24\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Проверка\n",
    "Проверка лабы устроена следующим образом:\n",
    "\n",
    "- Вы готовите свою модель\n",
    "- Настраиваете чтение данных из одного топика и запись в другой топик\n",
    "- Запускаете свой скрипт в стриминговым режиме\n",
    "- Идет подключение к топику на чтение данных\n",
    "- На странице лабы нажимаете кнопку \"Проверить\"\n",
    "- Вам в топик текут данные\n",
    "- Модель обрабатывает входные данные\n",
    "- Ваш скрипт сохраняет данные в другой топик\n",
    "- Чекер на странице лабы выдает результат проверки <br/>\n",
    "Проверка осуществляется автоматическим скриптом на странице лабы в личном кабинете. Наш чекер в конечном итоге подключится к вашему финальному топику в Kafka и сравнит ваш прогноз с эталонным ответом. На выходе вы получите свой score. В качестве него метрика accuracy: доля пользователей от общего числа, по которым вы верно угадали и пол, и возрастную категорию. Если accuracy будет больше 0.25, то лаба будет засчитана."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Библиотеки \n",
    "import pyspark.sql.types as t\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StringIndexer, IndexToString\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "-rw-r--r--   3 hdfs hdfs  655090069 2022-01-06 18:46 /labs/slaba04/gender_age_dataset.txt\r\n"
     ]
    }
   ],
   "source": [
    "# папка с датасетом на HDFS\n",
    "!hdfs dfs -ls /labs/slaba04/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender\tage\tuid\tuser_json\r\n",
      "F\t18-24\td50192e5-c44e-4ae8-ae7a-7cfe67c8b777\t{\"visits\": [{\"url\": \"http://zebra-zoya.ru/200028-chehol-organayzer-dlja-macbook-11-grid-it.html?utm_campaign=397720794&utm_content=397729344&utm_medium=cpc&utm_source=begun\", \"timestamp\": 1419688144068}, {\"url\": \"http://news.yandex.ru/yandsearch?cl4url=chezasite.com/htc/htc-one-m9-delay-86327.html&lr=213&rpt=story\", \"timestamp\": 1426666298001}, {\"url\": \"http://www.sotovik.ru/news/240283-htc-one-m9-zaderzhivaetsja.html\", \"timestamp\": 1426666298000}, {\"url\": \"http://news.yandex.ru/yandsearch?cl4url=chezasite.com/htc/htc-one-m9-delay-86327.html&lr=213&rpt=story\", \"timestamp\": 1426661722001}, {\"url\": \"http://www.sotovik.ru/news/240283-htc-one-m9-zaderzhivaetsja.html\", \"timestamp\": 1426661722000}]}\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "# датасет для обучения модели\n",
    "# Note: проверить таргет на наличие пустых значений\n",
    "!hdfs dfs -cat /labs/slaba04/gender_age_dataset.txt | head -n2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# формируем spark DF\n",
    "path = '/labs/slaba04/gender_age_dataset.txt'\n",
    "\n",
    "schema = t.StructType(fields=[\n",
    "    t.StructField('gender', t.StringType()),\n",
    "    t.StructField('age', t.StringType()),\n",
    "    t.StructField('uid', t.StringType()),\n",
    "    t.StructField('user_json', t.StringType()),\n",
    "])\n",
    "\n",
    "train_data = spark.read.csv(path, header=True, schema=schema, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------------+--------------------+\n",
      "|gender|  age|                 uid|           user_json|\n",
      "+------+-----+--------------------+--------------------+\n",
      "|     F|18-24|d50192e5-c44e-4ae...|{\"visits\": [{\"url...|\n",
      "|     M|25-34|d502331d-621e-472...|{\"visits\": [{\"url...|\n",
      "+------+-----+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# схема для json с визитами\n",
    "visits_schema = t.StructType([\n",
    "    t.StructField('visits', t.ArrayType(\n",
    "        t.StructType([\n",
    "            t.StructField('url', t.StringType(), True),\n",
    "            t.StructField('timestamp', t.LongType(), True)\n",
    "        ])\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(visits,ArrayType(StructType(List(StructField(url,StringType,true),StructField(timestamp,LongType,true))),true),true)))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visits_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# извлечем из визитов пользователя URL, чтобы применить HashingTF\n",
    "# используем explode, parse_url, collect_list\n",
    "# parse JSON column + extract URL from visits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------------+--------------------+\n",
      "|gender|  age|                 uid|           user_json|\n",
      "+------+-----+--------------------+--------------------+\n",
      "|     F|18-24|d50192e5-c44e-4ae...|{\"visits\": [{\"url...|\n",
      "|     M|25-34|d502331d-621e-472...|{\"visits\": [{\"url...|\n",
      "|     F|25-34|d50237ea-747e-48a...|{\"visits\": [{\"url...|\n",
      "|     F|25-34|d502f29f-d57a-46b...|{\"visits\": [{\"url...|\n",
      "|     M| >=55|d503c3b2-a0c2-4f4...|{\"visits\": [{\"url...|\n",
      "+------+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_flattened = train_data.withColumn(\"visits\", f.from_json(f.col(\"user_json\"), visits_schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- uid: string (nullable = true)\n",
      " |-- user_json: string (nullable = true)\n",
      " |-- visits: struct (nullable = true)\n",
      " |    |-- visits: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |-- timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df_flattened.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_flattened= spark_df_flattened.withColumn(\"visit\", f.explode(\"visits.visits\").alias(\"visit\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- uid: string (nullable = true)\n",
      " |-- user_json: string (nullable = true)\n",
      " |-- visits: struct (nullable = true)\n",
      " |    |-- visits: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |-- timestamp: long (nullable = true)\n",
      " |-- visit: struct (nullable = true)\n",
      " |    |-- url: string (nullable = true)\n",
      " |    |-- timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df_flattened.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[parsing link](https://docs.databricks.com/sql/language-manual/functions/parse_url.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_flattened = ( spark_df_flattened \n",
    "    .withColumn(\"site\", f.expr(\"parse_url(visit.url, 'HOST')\")) \n",
    "    .drop(\"visits\", \"visit\",\"user_json\")\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------------+--------------+\n",
      "|gender|  age|                 uid|          site|\n",
      "+------+-----+--------------------+--------------+\n",
      "|     F|18-24|d50192e5-c44e-4ae...| zebra-zoya.ru|\n",
      "|     F|18-24|d50192e5-c44e-4ae...|news.yandex.ru|\n",
      "|     F|18-24|d50192e5-c44e-4ae...|www.sotovik.ru|\n",
      "|     F|18-24|d50192e5-c44e-4ae...|news.yandex.ru|\n",
      "|     F|18-24|d50192e5-c44e-4ae...|www.sotovik.ru|\n",
      "+------+-----+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df_flattened.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------\n",
      " gender | F                                    \n",
      " age    | 18-24                                \n",
      " uid    | d50192e5-c44e-4ae8-ae7a-7cfe67c8b777 \n",
      " site   | zebra-zoya.ru                        \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df_flattened.show(1, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5829507"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df_flattened.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final = ( spark_df_flattened \n",
    "    .groupBy(\"gender\", \"age\", \"uid\") \n",
    "    .agg(f.collect_list(\"site\") \n",
    "    .alias(\"sites\"))\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+--------------------+--------------------+\n",
      "|gender|age|                 uid|               sites|\n",
      "+------+---+--------------------+--------------------+\n",
      "|     -|  -|13292e10-60bf-435...|[dateandtime.info...|\n",
      "|     -|  -|13f91463-8386-44c...|[go.mail.ru, pass...|\n",
      "|     -|  -|1dec593b-4dc5-41b...|[www.sport-expres...|\n",
      "|     -|  -|1ebc678e-8253-424...|[www.enter.ru, ww...|\n",
      "|     -|  -|1eecf70e-bb1c-4cb...|[www.licey.net, v...|\n",
      "+------+---+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_final.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41138"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_final.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41138"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_final.select('uid').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final = train_final.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41138"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_final.select('uid').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and clean data + Hashing TF + RandomForest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_final.randomSplit([0.75, 0.25], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashing_TF = HashingTF(\n",
    "    inputCol='sites', \n",
    "    outputCol='features', \n",
    "    numFeatures=10000, \n",
    "    binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer_age = StringIndexer(\n",
    "    inputCol='age', \n",
    "    outputCol='ageIndex').fit(train_final)\n",
    "indexer_gender = StringIndexer(\n",
    "    inputCol='gender', \n",
    "    outputCol='genderIndex').fit(train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_age = RandomForestClassifier(\n",
    "    featuresCol = 'features', \n",
    "    labelCol = 'ageIndex', \n",
    "    predictionCol='age_pred', \n",
    "    rawPredictionCol='age_raw_pred',\n",
    "    probabilityCol = 'age_probab')\n",
    "\n",
    "rf_gender = RandomForestClassifier(\n",
    "    featuresCol = 'features', \n",
    "    labelCol = 'genderIndex',\n",
    "    predictionCol='gender_pred', \n",
    "    rawPredictionCol='gender_raw_pred',\n",
    "    probabilityCol = 'gender_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter_age = IndexToString(\n",
    "    inputCol='age_pred', \n",
    "    outputCol='PredictedAge', \n",
    "    labels=indexer_age.labels)\n",
    "\n",
    "converter_gender = IndexToString(\n",
    "    inputCol='gender_pred', \n",
    "    outputCol='PredictedGender', \n",
    "    labels=indexer_gender.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    stages=[hashing_TF, indexer_age, indexer_gender, rf_age, rf_gender, converter_age, converter_gender])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(train_final)\n",
    "predictions = model.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy for age\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"ageIndex\", predictionCol=\"age_pred\")\n",
    "evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy for age\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"genderIndex\", predictionCol=\"gender_pred\")\n",
    "evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_BOOTSTRAP_SERVER = 'spark-node-1.newprolab.com:6667'\n",
    "KAFKA_INPUT_TOPIC = 'input_anton.ermak'\n",
    "KAFKA_OUTPUT_TOPIC = 'anton.ermak'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# чтение в статическом режиме\n",
    "\n",
    "kafka_read_df = (\n",
    "    spark.read\n",
    "    .format('kafka')\n",
    "    .option('kafka.bootstrap.servers', KAFKA_BOOTSTRAP_SERVER)\n",
    "    .option('subscribe', KAFKA_INPUT_TOPIC)\n",
    "    .option('startingOffsets', 'earliest')\n",
    "    .option('failOnDataLoss', 'False')\n",
    "    .load()\n",
    "    .cache()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_read_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|uid|visits|\n",
      "+---+------+\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Парсинг бинарного файла из кафки\n",
    "\n",
    "event_schema = t.StructType([\n",
    "    t.StructField('uid', t.StringType(), True),\n",
    "    t.StructField('visits', t.StringType(), True),\n",
    "])\n",
    "\n",
    "\n",
    "visit_schema = t.ArrayType(\n",
    "    t.StructType([\n",
    "        t.StructField('url', t.StringType(), True),\n",
    "        t.StructField('timestamp', t.LongType(), True)\n",
    "    ])\n",
    ")\n",
    "\n",
    "\n",
    "clean_df = (\n",
    "    kafka_read_df\n",
    "    .select(f.col('value').cast('string').alias('value'))\n",
    "    .select(f.from_json(f.col('value'), event_schema).alias('event'))\n",
    "    .select(\n",
    "        'event.uid', \n",
    "        f.from_json(f.col('event.visits'), visit_schema).alias('visits')\n",
    "    )\n",
    ")\n",
    "\n",
    "clean_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# извлечение url\n",
    "# применение модели, сохранение предсказаний в predictions_df  \n",
    "\n",
    "prep_df = ( clean_df \n",
    "    .withColumn(\"visit\", f.explode(\"visits\").alias(\"visit\")) \n",
    "    .withColumn(\"site\", f.expr(\"parse_url(visit.url, 'HOST')\")) \n",
    "    .drop(\"visits\", \"visit\") \n",
    "    .groupBy(\"uid\") \n",
    "    .agg(f.collect_list(\"site\").alias(\"sites\"))\n",
    "          )\n",
    "\n",
    "predictions_df = ( model.transform(prep_df) \n",
    "    .select(\"uid\", \"PredictedGender\", \"PredictedAge\") \n",
    "    .withColumnRenamed(\"PredictedAge\",\"age\") \n",
    "    .withColumnRenamed(\"PredictedGender\",\"gender\")\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+\n",
      "|uid|gender|age|\n",
      "+---+------+---+\n",
      "+---+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.select('uid').distinct().count(), predictions_df.select('uid').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оборачивание предсказания обратно в json\n",
    "\n",
    "kafka_out_df = (\n",
    "    predictions_df \n",
    "    .select(f.to_json(f.struct(*predictions_df.columns)).alias('value')).limit(5)\n",
    ")\n",
    "\n",
    "# Запись в выходной топик\n",
    "\n",
    "(\n",
    "    kafka_out_df\n",
    "    .write\n",
    "    .format('kafka')\n",
    "    .option('kafka.bootstrap.servers', KAFKA_BOOTSTRAP_SERVER)\n",
    "    .option('topic', KAFKA_OUTPUT_TOPIC)\n",
    "    .save()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_out_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### streaming format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# чтение стрима\n",
    "\n",
    "kafka_stream = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format('kafka')\n",
    "    .option('kafka.bootstrap.servers', KAFKA_BOOTSTRAP_SERVER)\n",
    "    .option('subscribe', KAFKA_INPUT_TOPIC)\n",
    "    .option('startingOffsets', 'earliest')\n",
    "    .option('failOnDataLoss', 'False')\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_stream.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# предсказание и запись\n",
    "\n",
    "clean_df = (\n",
    "    kafka_stream\n",
    "    .select(f.col('value').cast('string').alias('value'))\n",
    "    .select(f.from_json(f.col('value'), event_schema).alias('event'))\n",
    "    .select(\n",
    "        'event.uid', \n",
    "        f.from_json(f.col('event.visits'), visit_schema).alias('visits')\n",
    "    )\n",
    ")\n",
    "\n",
    "prep_df = ( clean_df \n",
    "    .withColumn(\"visit\", f.explode(\"visits\").alias(\"visit\")) \n",
    "    .withColumn(\"site\", f.expr(\"parse_url(visit.url, 'HOST')\")) \n",
    "    .drop(\"visits\", \"visit\") \n",
    "    .groupBy(\"uid\") \n",
    "    .agg(f.collect_list(\"site\").alias(\"sites\"))\n",
    "          )\n",
    "\n",
    "predictions_df = ( model.transform(prep_df) \n",
    "    .select(\"uid\", \"PredictedGender\", \"PredictedAge\") \n",
    "    .withColumnRenamed(\"PredictedAge\",\"age\") \n",
    "    .withColumnRenamed(\"PredictedGender\",\"gender\")\n",
    "                 )\n",
    "\n",
    "kafka_write_stream = (\n",
    "    predictions_df\n",
    "    .select(f.to_json(f.struct(*predictions_df.columns)).alias('value'))\n",
    "    .writeStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"checkpointLocation\", \"checkpoints/checkpoints_lab04\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVER)\n",
    "    .option(\"topic\", KAFKA_OUTPUT_TOPIC)\n",
    "    .outputMode(\"complete\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 'KafkaV2[Subscribe[input_anton.ermak]]')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streams = SparkSession.builder.getOrCreate().streams.active\n",
    "len(streams), streams[0].lastProgress[\"sources\"][0][\"description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_write_stream.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_write_stream.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
