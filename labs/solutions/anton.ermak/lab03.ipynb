{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В ноутбуке: строим модель бинарной классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[GIT Link](https://github.com/newprolab/sber-spark-ds-10/blob/main/labs/lab03.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spark.sesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 2 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import Row\n",
    "import json\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .config(conf=conf)\n",
    "         .appName(\"lab03\")\n",
    "         .getOrCreate())\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача\n",
    "В вашем распоряжении имеется уже предобработанный и очищенный датасет с фактами покупок абонентами телепередач от компании E-Contenta. По доступным вам данным, нужно предсказать вероятность покупки других передач этими, а, возможно, и другими абонентами. При решении задачи запрещено использовать библиотеки pandas, sklearn (кроме sklearn.metrics), xgboost и другие. Если scikit-learn (например, но и другие тоже) обернут в классы Transformer и Estimator, то их можно использовать.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Описание данных\n",
    "Для выполнения работы вам следует взять все файлы из папки на HDFS /labs/slaba03/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "-rw-r--r--   3 hdfs hdfs   91066524 2022-01-06 18:46 /labs/slaba03/laba03_items.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs   29965581 2022-01-06 18:46 /labs/slaba03/laba03_test.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs   74949368 2022-01-06 18:46 /labs/slaba03/laba03_train.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs  871302535 2022-01-06 18:46 /labs/slaba03/laba03_views_programmes.csv\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /labs/slaba03/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В laba03_train.csv содержатся факты покупки (колонка purchase) пользователями (колонка user_id) телепередач (колонка item_id). Такой формат файла вам уже знаком."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`laba03_items.csv` — дополнительные данные по items. В данном файле много лишней или ненужной информации, так что задача её фильтрации и отбора ложится на вас. Поля в файле, на которых хотелось бы остановиться:\n",
    "\n",
    "- `item_id` — primary key. Соответствует item_id в предыдущем файле.\n",
    "- `content_type` — тип телепередачи (1 — платная, 0 — бесплатная). Вас интересуют платные передачи.\n",
    "- `title` — название передачи, текстовое поле.\n",
    "- `year` — год выпуска передачи, число.\n",
    "- `genres` — поле с жанрами передачи, разделёнными через запятую."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`laba03_test.csv` — тестовый датасет без указанного целевого признака purchase, который вам и предстоит предсказать.\n",
    "\n",
    "Дополнительный файл `laba03_views_programmes.csv` по просмотрам передач с полями:\n",
    "\n",
    "- `ts_start` — время начала просмотра.\n",
    "- `ts_end` — время окончания просмотра.\n",
    "- `item_type` — тип просматриваемого контента:\n",
    "- `live` — просмотр \"вживую\", в момент показа контента в эфире.\n",
    "- `pvr` — просмотр в записи, после показа контента в эфире."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Результат\n",
    "Предсказание целевой переменной \"купит/не купит\" — хорошо знакомая вам задача бинарной классификации. Поскольку нам важны именно вероятности отнесения пары (пользователь, товар) к классу \"купит\" (1), то, на самом деле, вы можете подойти к проблеме с разных сторон:\n",
    "\n",
    "Как просто к задаче бинарной классификации. У вас есть два датасета, которые можно каким-то образом объединить, дополнительно обработать и сделать предсказания классификаторами (Spark ML).\n",
    "Как к разработке рекомендательной системы: рекомендовать пользователю user_id топ-N лучших телепередач, которые были найдены по методике user-user / item-item коллаборативной фильтрации.\n",
    "Как к задаче факторизации матриц: алгоритмы SVD, ALS, FM/FFM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Советы\n",
    "На качество прогноза в большей степени влияет качество признаков, которые вы сможете придумать из имеющихся данных, нежели выбор и сложность алгоритма.\n",
    "Качество входных данных также имеет сильное значение. Существует фраза \"garbage in – garbage out\". Мусор на входе – мусор на выходе. Потратьте время на подготовку и предобработку данных. Путь к успеху в третьей лабораторной:\n",
    "Сосредоточьтесь на формировании следующих фичей: по файлу laba03_train.csv сформируйте признаки, характеризирующие как интенсивно покупает пользователь и \"покупаемость\" item'ов\n",
    "возьмите достаточно мощную модель (например GBTClassifier из pyspark'а)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, OneHotEncoderEstimator, StringIndexer, VectorAssembler, CountVectorizer\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get train and test ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ( \n",
    "    spark.read\n",
    "    .option('header', 'True')\n",
    "    .option('sep', ',')\n",
    "    .csv('/labs/slaba03/laba03_test.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+\n",
      "|user_id|item_id|purchase|\n",
      "+-------+-------+--------+\n",
      "|   1654|  94814|    null|\n",
      "|   1654|  93629|    null|\n",
      "|   1654|   9980|    null|\n",
      "|   1654|  95099|    null|\n",
      "|   1654|  11265|    null|\n",
      "+-------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ( \n",
    "    spark.read\n",
    "    .option('header', 'True')\n",
    "    .option('sep', ',')\n",
    "    .csv('/labs/slaba03/laba03_train.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+\n",
      "|user_id|item_id|purchase|\n",
      "+-------+-------+--------+\n",
      "|   1654|  74107|       0|\n",
      "|   1654|  89249|       0|\n",
      "|   1654|  99982|       0|\n",
      "|   1654|  89901|       0|\n",
      "|   1654| 100504|       0|\n",
      "+-------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- item_id: string (nullable = true)\n",
      " |-- purchase: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.withColumn(\"purchase\", train.purchase.cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpecAgg  = Window.partitionBy('user_id')\n",
    "users_activity = train.withColumn('user_activity', F.sum('purchase').over(windowSpecAgg)/F.count('purchase').over(windowSpecAgg))\\\n",
    ".select('user_id', 'user_activity')\\\n",
    ".dropDuplicates(['user_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: string, user_activity: double]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1941"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_activity.select('user_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1941"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_activity.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1941"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.select('user_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpecAgg  = Window.partitionBy('item_id')\n",
    "items_intenstiy = train.withColumn(\n",
    "    'items_intenstiy', F.sum('purchase').over(windowSpecAgg)/F.count('purchase').over(windowSpecAgg))\\\n",
    ".select('item_id', 'items_intenstiy')\\\n",
    ".dropDuplicates(['item_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3704"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_intenstiy.select('item_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3704"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_intenstiy.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3704"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.select('item_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train0 = train.join(users_activity, how='left', on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = train0.join(items_intenstiy, how='left', on='item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5032624"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5032624"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- item_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- purchase: integer (nullable = true)\n",
      " |-- user_activity: double (nullable = true)\n",
      " |-- items_intenstiy: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# items_freq = test.groupby('item_id').count().withColumnRenamed('count', 'items_cnt')\n",
    "# users_activity = test.groupby('user_id').count().withColumnRenamed('count', 'users_cnt')\n",
    "\n",
    "# test_new = test.join(items_freq, how='left', on='item_id')\n",
    "# test_new = test_new.join(users_activity, how='left', on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_new.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get and preprocess data on items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = ( \n",
    "    spark.read\n",
    "    .option('header', 'True')\n",
    "    .option('sep', '\\t')\n",
    "    .csv('/labs/slaba03/laba03_items.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = items.select('item_id', 'content_type', 'title', 'year', 'genres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------------------\n",
      " item_id      | 65667                                         \n",
      " content_type | 1                                             \n",
      " title        | на пробах только девушки (all girl auditions) \n",
      " year         | 2013.0                                        \n",
      " genres       | Эротика                                       \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items.show(1, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- item_id: string (nullable = true)\n",
      " |-- content_type: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|content_type| count|\n",
      "+------------+------+\n",
      "|           0|631864|\n",
      "|           1|  3704|\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items.select('content_type').groupby('content_type').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# further work with only paid items\n",
    "items = items.filter(F.col('content_type')==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|item_id|\n",
      "+-------+\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check that items are exhausive to train and test\n",
    "test.select('item_id').join(items, how='left_anti', on='item_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|item_id|\n",
      "+-------+\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.select('item_id').join(items, how='left_anti', on='item_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- item_id: string (nullable = true)\n",
      " |-- content_type: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# items = items.fillna('', 'genres')\n",
    "# tokenizer = Tokenizer(inputCol='genres', outputCol='genres_split')\n",
    "# items = tokenizer.transform(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = items.fillna('', 'genres')\n",
    "genres_split = F.udf(lambda x: x.split(','), ArrayType(StringType()))\n",
    "items = items.withColumn('genres', genres_split(F.col('genres')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol=\"genres\", outputCol=\"genres_features\")\n",
    "model = cv.fit(items)\n",
    "items = model.transform(items).drop('genres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[item_id: string, content_type: string, title: string, year: string, genres_features: vector]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = train1.join(items.select('item_id', 'genres_features'), how='left', on='item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[item_id: string, user_id: string, purchase: int, user_activity: double, items_intenstiy: double, genres_features: vector]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-------------+---------------+\n",
      "|item_id|user_id|purchase|user_activity|items_intenstiy|\n",
      "+-------+-------+--------+-------------+---------------+\n",
      "|      0|      0|       0|            0|              0|\n",
      "+-------+-------+--------+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check for nan colums\n",
    "train2_cols=[\"item_id\",\"user_id\", \"purchase\", \"user_activity\", \"items_intenstiy\"]\n",
    "train2.select([F.count(F.when(F.isnan(c) | F.col(c).isNull(), c)).alias(c) for c in train2_cols]\n",
    "   ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prep for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assembler = (\n",
    "    VectorAssembler()\n",
    "    .setInputCols(['user_activity', 'items_intenstiy', 'genres_features']) # 'year',\n",
    "    .setOutputCol('features')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_for_model = assembler.transform(train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_for_model.randomSplit([0.75, 0.25], seed = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'purchase', maxIter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel = lr.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lrModel.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[item_id: string, user_id: string, purchase: int, user_activity: double, items_intenstiy: double, genres_features: vector, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------------------+\n",
      "|user_id|item_id|         probability|\n",
      "+-------+-------+--------------------+\n",
      "| 510087| 100140|[0.99821724412245...|\n",
      "| 517612| 100140|[0.99832471089242...|\n",
      "| 520446| 100140|[0.99817804077684...|\n",
      "| 566758| 100140|[0.99832542454817...|\n",
      "| 613775| 100140|[0.99832475100011...|\n",
      "+-------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select('user_id','item_id','probability').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------+\n",
      "|user_id|item_id|    purchase|\n",
      "+-------+-------+------------+\n",
      "| 510087| 100140|0.0017827558|\n",
      "| 517612| 100140|0.0016752891|\n",
      "| 520446| 100140|0.0018219593|\n",
      "| 566758| 100140|0.0016745755|\n",
      "| 613775| 100140| 0.001675249|\n",
      "+-------+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_element = F.udf(lambda v:float(v[1]), FloatType())\n",
    "predictions.select('user_id', 'item_id', get_element('probability').alias('purchase')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Train 0.8731953545669617\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol = 'purchase')\n",
    "print('ROC AUC Train', evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel = lr.fit(train_for_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: string, item_id: string, purchase: string]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_activity_df = train.groupBy('user_id').agg(\n",
    "    (F.sum('purchase')/F.count('purchase')\n",
    "    ).alias('user_activity'))\n",
    "\n",
    "item_intensivity_df = train.groupBy('item_id').agg(\n",
    "    (F.sum('purchase')/F.count('purchase')\n",
    "    ).alias('item_intensivity'))\n",
    "\n",
    "\n",
    "test = test.join(user_activity_df, how='left', on='user_id')\n",
    "test = test.join(item_intensivity_df, how='left', on='item_id')\n",
    "test = test.join(items.select('item_id', 'genres_features'), how='left', on='item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+--------------------+----------------+---------------+\n",
      "|item_id|user_id|purchase|       user_activity|item_intensivity|genres_features|\n",
      "+-------+-------+--------+--------------------+----------------+---------------+\n",
      "| 100140| 763464|    null|3.933910306845004E-4|             0.0| (84,[2],[1.0])|\n",
      "| 100140| 775479|    null|0.001520912547528517|             0.0| (84,[2],[1.0])|\n",
      "| 100140| 887290|    null|                 0.0|             0.0| (84,[2],[1.0])|\n",
      "| 100140| 895407|    null|0.003097173828881146|             0.0| (84,[2],[1.0])|\n",
      "| 100140| 896973|    null|7.727975270479134E-4|             0.0| (84,[2],[1.0])|\n",
      "+-------+-------+--------+--------------------+----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler_test = (\n",
    "    VectorAssembler()\n",
    "    .setInputCols(['user_activity', 'item_intensivity', 'genres_features']) \n",
    "    .setOutputCol('features')\n",
    ")\n",
    "\n",
    "test_model = assembler_test.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+--------------------+----------------+---------------+--------------------+\n",
      "|item_id|user_id|purchase|       user_activity|item_intensivity|genres_features|            features|\n",
      "+-------+-------+--------+--------------------+----------------+---------------+--------------------+\n",
      "| 100140| 748042|    null|0.001530807500956...|             0.0| (84,[2],[1.0])|(86,[0,4],[0.0015...|\n",
      "| 100140| 855465|    null|0.001552795031055...|             0.0| (84,[2],[1.0])|(86,[0,4],[0.0015...|\n",
      "| 100140| 905618|    null|                 0.0|             0.0| (84,[2],[1.0])|      (86,[4],[1.0])|\n",
      "| 100140| 871154|    null|                 0.0|             0.0| (84,[2],[1.0])|      (86,[4],[1.0])|\n",
      "+-------+-------+--------+--------------------+----------------+---------------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_model.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### log regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_test_predictions = lrModel.transform(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[item_id: string, user_id: string, purchase: string, user_activity: double, item_intensivity: double, genres_features: vector, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важно! Для точной проверки не забудьте отсортировать полученный файл по возрастанию идентификаторов пользователей (user_id), а затем — по возрастанию идентификаторов передач (item_id)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_pd = log_test_predictions.select('user_id', 'item_id', get_element('probability').alias('purchase')) \\\n",
    ".orderBy('user_id', 'item_id') \\\n",
    ".toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
