{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 4 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, StopWordsRemover, Tokenizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import ArrayType, StringType, FloatType\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import pandas_udf, lower, col, udf, isnan, isnull, broadcast, desc, concat\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .config(conf=conf)\n",
    "         .appName(\"test\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_courses = {\n",
    "    23126: 'en',\n",
    "    21617: 'en',\n",
    "    16627: 'es',\n",
    "    11556: 'es',\n",
    "    16704: 'ru',\n",
    "    13702: 'ru'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.json(\"/labs/slaba02/DO_record_per_line.json\")\n",
    "data = data.filter(f.col(\"desc\") != ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = list(result_courses.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn(\"desc\", lower(col(\"desc\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_string(series):\n",
    "    regex = re.compile(u'[\\w\\d]{2,}', re.U) #re.compile(u'[\\w\\d]{2,}', re.U)\n",
    "    words = series.str.findall(regex)\n",
    "    return words\n",
    "\n",
    "tokenizer_udf = pandas_udf(clear_string, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_tokenized_data = data.withColumn(\"token\", tokenizer_udf(\"desc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_en = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "stop_words_rus = StopWordsRemover.loadDefaultStopWords(\"russian\")\n",
    "stop_words_es = StopWordsRemover.loadDefaultStopWords(\"spanish\")\n",
    "stop_words = stop_words_en+stop_words_rus+stop_words_es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(inputCol=\"token\", outputCol=\"no_stop_words\", stopWords=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = remover.transform(pre_tokenized_data)\n",
    "\n",
    "tokenized_en = tokenized_data.filter(f.col(\"lang\") == \"en\")\n",
    "tokenized_es = tokenized_data.filter(f.col(\"lang\") == \"es\")\n",
    "tokenized_ru = tokenized_data.filter(f.col(\"lang\") == \"ru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = HashingTF(inputCol=\"no_stop_words\",outputCol=\"tf_f\", numFeatures=10000, binary=False)\n",
    "idf = IDF(inputCol=\"tf_f\", outputCol=\"idf_f\") #minDocFreq = 5,\n",
    "from pyspark.ml.feature import Normalizer \n",
    "t = Normalizer(inputCol=\"idf_f\", outputCol=\"norm_idf_f\")\n",
    "\n",
    "hashed_data_en = tf.transform(tokenized_en)\n",
    "idfModel_en = idf.fit(hashed_data_en)\n",
    "idfed_data_en = idfModel_en.transform(hashed_data_en)\n",
    "normalized_en = t.transform(idfed_data_en)\n",
    "\n",
    "\n",
    "hashed_data_es = tf.transform(tokenized_es)\n",
    "idfModel_es = idf.fit(hashed_data_es)\n",
    "idfed_data_es = idfModel_es.transform(hashed_data_es)\n",
    "normalized_es = t.transform(idfed_data_es)\n",
    "\n",
    "\n",
    "hashed_data_ru = tf.transform(tokenized_ru)\n",
    "idfModel_ru = idf.fit(hashed_data_ru)\n",
    "idfed_data_ru = idfModel_ru.transform(hashed_data_ru)\n",
    "normalized_ru = t.transform(idfed_data_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = normalized_en.unionAll(normalized_es).unionAll(normalized_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}\n",
    "for id_cor, lng in langs:\n",
    "    vector_cor = normalized_data.filter(normalized_data.id == int(id_cor)).collect()[0]['norm_idf_f'].toArray()\n",
    "    tf_idf_cos = f.udf(lambda x: float(x.dot(vector_cor)), FloatType())\n",
    "    predictions = normalized_data.where((normalized_data.id != int(id_cor)) & (normalized_data.lang == lng))\\\n",
    "                .withColumn('tf_idf_cos',tf_idf_cos(normalized_data['norm_idf_f']))\\\n",
    "                .orderBy(f.desc('tf_idf_cos'), f.asc('name'), f.asc('id'))\\\n",
    "                .head(10)\n",
    "    pre_final = [i['id'] for i in predictions]\n",
    "    result.update({str(id_cor): pre_final})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('lab02.json', 'w') as js:\n",
    "    js.write(json.dumps(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
