{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import Row\n",
    "import json\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .config(conf=conf)\n",
    "         .appName(\"rez_lab02\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "|                 cat|                desc| id|lang|                name|      provider|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "|3/business_manage...|This course intro...|  4|  en|Accounting Cycle:...|Canvas Network|\n",
      "|              11/law|This online cours...|  5|  en|American Counter ...|Canvas Network|\n",
      "|5/computer_scienc...|This course is ta...|  6|  fr|Arithmétique: en ...|Canvas Network|\n",
      "|  14/social_sciences|We live in a digi...|  7|  en|Becoming a Dynami...|Canvas Network|\n",
      "|2/biology_life_sc...|This self-paced c...|  8|  en|           Bioethics|Canvas Network|\n",
      "|9/humanities|15/m...|This game-based c...|  9|  en|College Foundatio...|Canvas Network|\n",
      "|  14/social_sciences|What’s in your di...| 10|  en|Digital Literacies I|Canvas Network|\n",
      "|  14/social_sciences|The goal of the D...| 11|  en|Digital Literacie...|Canvas Network|\n",
      "|  14/social_sciences|Ready to explore ...| 12|  en|Digital Tools for...|Canvas Network|\n",
      "|  14/social_sciences|This self-paced c...| 13|  en|Discover Your Val...|Canvas Network|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.read.json('/labs/slaba02/DO_record_per_line.json')\n",
    "rdd.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[23126, 'en', 'Compass - powerful SASS library that makes your life easier'], [21617, 'en', 'Preparing for the AP* Computer Science A Exam — Part 2'], [16627, 'es', 'Aprende Excel: Nivel Intermedio by Alfonso Rinsche'], [11556, 'es', 'Aprendizaje Colaborativo by UNID Universidad Interamericana para el Desarrollo'], [16704, 'ru', 'Программирование на Lazarus'], [13702, 'ru', 'Математическая экономика']]\n"
     ]
    }
   ],
   "source": [
    "to_make_recommends = [[23126, u'en', u'Compass - powerful SASS library that makes your life easier'], [21617, u'en', u'Preparing for the AP* Computer Science A Exam \\u2014 Part 2'], [16627, u'es', u'Aprende Excel: Nivel Intermedio by Alfonso Rinsche'], [11556, u'es', u'Aprendizaje Colaborativo by UNID Universidad Interamericana para el Desarrollo'], [16704, u'ru', u'\\u041f\\u0440\\u043e\\u0433\\u0440\\u0430\\u043c\\u043c\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u043d\\u0430 Lazarus'], [13702, u'ru', u'\\u041c\\u0430\\u0442\\u0435\\u043c\\u0430\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u0430\\u044f \\u044d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u043a\\u0430']]\n",
    "print(to_make_recommends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, max as max_, col, rank, lit\n",
    "from pyspark.sql.types import IntegerType, DoubleType, ArrayType, StringType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(s_):\n",
    "    regex = re.compile(u'[\\w\\d]{2,}', re.U)\n",
    "    return regex.findall(s_.lower())\n",
    "get_tokens_ = udf(lambda x: get_tokens(x), ArrayType(StringType()))\n",
    "\n",
    "def cos_(x, y):\n",
    "    if(x == None or y == None):\n",
    "        return np.nan\n",
    "    else:\n",
    "        if float(x.norm(2)*y.norm(2))!=0:\n",
    "            return float(x.dot(y)/(x.norm(2)*y.norm(2)))\n",
    "        else:\n",
    "            return float(-1)\n",
    "cosinus_ = udf(lambda x,y: cos_(x, y), DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "result_dict = {}\n",
    "len_ = len(to_make_recommends)\n",
    "for i in range(0, len_):\n",
    "    cur_course = to_make_recommends[i]\n",
    "    df_ = rdd\\\n",
    "    .filter(col('lang')==cur_course[1])\n",
    "\n",
    "    t_ = df_\\\n",
    "    .withColumn('words',get_tokens_(col('desc')))\n",
    "    \n",
    "    ht = HashingTF(inputCol='words', outputCol='vector_tf', numFeatures=10000)\n",
    "    tf_ = ht.transform(t_)\n",
    "\n",
    "    idf_m = IDF(inputCol='vector_tf', outputCol='vector_')  \n",
    "    idf_r = idf_m.fit(tf_ ) \n",
    "    idf_ = idf_r.transform(tf_)\n",
    "    \n",
    "    v_ = idf_\\\n",
    "    .filter(col('id')==cur_course[0])\\\n",
    "    .withColumnRenamed('id','id_base')\\\n",
    "    .withColumnRenamed('vector_','vector_base')\\\n",
    "    .select('id_base','vector_base')\n",
    "\n",
    "    tv_ = idf_\\\n",
    "    .join(v_,col('id')!=col('id_base'))\\\n",
    "    .withColumn('cos', cosinus_(col('vector_base'), col('vector_')))\\\n",
    "    .select('id', 'id_base', 'name', 'cos')\n",
    "\n",
    "    window = Window.orderBy(tv_['cos'].desc(), tv_['name'])\n",
    "    t_ = tv_.select('id', 'cos', rank().over(window).alias('rank')).filter(col('rank') <= 10).collect()\n",
    "    l_ = [int(row[0]) for row in t_]\n",
    "    result_dict[str(cur_course[0])] = l_\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'23126': [14760, 13665, 13782, 20638, 24419, 15909, 2724, 25782, 17499, 13348], '21617': [21609, 21616, 22298, 21608, 21630, 21628, 21508, 21623, 21081, 19417], '16627': [11431, 17961, 17964, 5687, 12247, 16694, 5558, 12660, 11575, 9563], '11556': [10384, 16488, 468, 22710, 13461, 21707, 19330, 23357, 10447, 9465], '16704': [1219, 1327, 20362, 1228, 26980, 55, 1236, 1247, 1365, 913, 20095], '13702': [864, 21079, 1111, 792, 1410, 8123, 1041, 1033, 8313, 1396]}\n"
     ]
    }
   ],
   "source": [
    "print(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"23126\": [14760, 13665, 13782, 20638, 24419, 15909, 2724, 25782, 17499, 13348], \"21617\": [21609, 21616, 22298, 21608, 21630, 21628, 21508, 21623, 21081, 19417], \"16627\": [11431, 17961, 17964, 5687, 12247, 16694, 5558, 12660, 11575, 9563], \"11556\": [10384, 16488, 468, 22710, 13461, 21707, 19330, 23357, 10447, 9465], \"16704\": [1219, 1327, 20362, 1228, 26980, 55, 1236, 1247, 1365, 913, 20095], \"13702\": [864, 21079, 1111, 792, 1410, 8123, 1041, 1033, 8313, 1396]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "json_result = json.dumps(result_dict)\n",
    "print(json_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"lab02.json\",\"w\") as json_res_: json_res_.write(json_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
