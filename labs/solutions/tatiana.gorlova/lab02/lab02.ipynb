{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8d3513b7698b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 5 --executor-memory 4g --executor-cores 1 --driver-memory 3g pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"PGG-try2\") \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.jp-OutputArea-output pre {white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.jp-OutputArea-output pre {white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import SparseVector, DenseVector\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover,VectorAssembler\n",
    "import pyspark.sql.functions as f_\n",
    "# from pyspark.sql.types import FloatType, DoubleType, DecimalType, StringType, ArrayType \n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml import Pipeline\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@f_.udf(FloatType())\n",
    "def cosine_similarity(v,u):\n",
    "    return float(v.dot(u) / (v.norm(2) * u.norm(2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en', 'es', 'ru'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "personal_conditions=[[23126, u'en', u'Compass - powerful SASS library that makes your life easier'], [21617, u'en', u'Preparing for the AP* Computer Science A Exam \\u2014 Part 2'], [16627, u'es', u'Aprende Excel: Nivel Intermedio by Alfonso Rinsche'], [11556, u'es', u'Aprendizaje Colaborativo by UNID Universidad Interamericana para el Desarrollo'], [16704, u'ru', u'\\u041f\\u0440\\u043e\\u0433\\u0440\\u0430\\u043c\\u043c\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u043d\\u0430 Lazarus'], [13702, u'ru', u'\\u041c\\u0430\\u0442\\u0435\\u043c\\u0430\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u0430\\u044f \\u044d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u043a\\u0430']]\n",
    "pc_ids=[p[0] for p in personal_conditions]\n",
    "set([p[1] for p in personal_conditions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp_clear= f_.lower(f_.regexp_replace('desc',r'[^\\pL0-9\\p{Space}]','' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(set(\n",
    "    StopWordsRemover.loadDefaultStopWords(\"english\") + \n",
    "    StopWordsRemover.loadDefaultStopWords(\"russian\") + \n",
    "    StopWordsRemover.loadDefaultStopWords(\"spanish\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем данные \n",
    "data = spark.read.json('/labs/slaba02/DO_record_per_line.json')\n",
    "# Чистом данные\n",
    "courses_data=data.select(*data.columns ,regexp_clear.alias('doc')).drop('desc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"doc\", outputCol=\"words\")\n",
    "swr=StopWordsRemover(inputCol=\"words\", outputCol=\"words_censored\", stopWords=stop_words)\n",
    "tf = HashingTF(inputCol=\"words_censored\", outputCol=\"tf\")\n",
    "# hashingTF = HashingTF(numFeatures =10000, inputCol=\"words\", outputCol=\"tf\")\n",
    "tfidf = IDF(inputCol=\"tf\", outputCol=\"idf\")\n",
    "pipeline = Pipeline(stages=[\n",
    "    tokenizer,\n",
    "    swr,\n",
    "    tf,\n",
    "    tfidf\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc=pipeline.fit(courses_data)\n",
    "tfidf_data=calc.transform(courses_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross=tfidf_data.alias('a').crossJoin(tfidf_data.where(f_.col('id').isin(pc_ids)).alias('b'))\\\n",
    ".select('a.*'\n",
    "        ,f_.col('b.id').alias('pc_id')\n",
    "        ,f_.col('b.lang').alias('pc_lang')\n",
    "        ,f_.col('b.name').alias('pc_name')\n",
    "        ,f_.col('b.idf').alias('pc_tfidf')\n",
    "       )\\\n",
    ".filter(''' a.lang=pc_lang and pc_id !=id ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross2=cross.select('*',cosine_similarity('idf','pc_tfidf').alias('cos'))\\\n",
    ".filter(''' cos !='NaN' ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross3=cross2.select('*', f_.row_number().over(Window.partitionBy(\"pc_id\")\\\n",
    ".orderBy(\n",
    "     f_.col('cos').desc()\n",
    "    ,f_.col('name')\n",
    "    ,f_.col('id'))).alias('n'))\\\n",
    ".filter('n<=10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11556 : [16488, 468, 13461, 22710, 23357, 10447, 19330, 21707, 11523, 9465]\n",
      "13702 : [864, 21079, 8123, 1041, 28074, 13057, 1396, 1052, 1033, 8300]\n",
      "16627 : [11431, 12247, 12660, 5687, 17964, 16694, 12598, 11575, 12863, 21704]\n",
      "16704 : [1236, 1247, 1228, 1365, 1164, 1233, 1273, 20288, 8186, 8203]\n",
      "21617 : [21609, 21608, 21616, 21492, 21624, 21623, 21703, 21630, 21628, 21508]\n",
      "23126 : [14760, 13665, 13782, 15909, 19270, 25782, 17499, 13348, 7153, 25071]\n"
     ]
    }
   ],
   "source": [
    "recomendation_=cross3.groupBy(f_.col('pc_id')).agg(f_.collect_list(f_.col('id')).alias('top_10_ids')).orderBy('pc_id').collect()\n",
    "recomendation={}\n",
    "for r in recomendation_:\n",
    "    recomendation[str(r[0])]=r[1]\n",
    "    print(str(r[0]),':',r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lab02.json', 'w') as fp:\n",
    "    json.dump(recomendation, fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "11556 : [16488, 468, 13461, 22710, 23357, 10447, 19330, 21707, 11523, 9465]\n",
    "13702 : [864, 21079, 8123, 1041, 28074, 13057, 1396, 1052, 1033, 8300]\n",
    "16627 : [11431, 12247, 12660, 5687, 17964, 16694, 12598, 11575, 12863, 21704]\n",
    "16704 : [1236, 1247, 1228, 1365, 1164, 1233, 1273, 20288, 8186, 8203]\n",
    "21617 : [21609, 21608, 21616, 21492, 21624, 21623, 21703, 21630, 21628, 21508]\n",
    "23126 : [14760, 13665, 13782, 15909, 19270, 25782, 17499, 13348, 7153, 25071]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
