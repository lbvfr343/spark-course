{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Задача\n",
    "В вашем распоряжении имеется уже предобработанный и очищенный датасет с фактами покупок абонентами телепередач от компании \n",
    "E-Contenta. По доступным вам данным, нужно предсказать вероятность покупки других передач этими, а, возможно, и другими \n",
    "абонентами. При решении задачи запрещено использовать библиотеки pandas, sklearn (кроме sklearn.metrics), xgboost и другие. \n",
    "Если scikit-learn (например, но и другие тоже) обернут в классы Transformer и Estimator, то их можно использовать.\n",
    "\n",
    "Описание данных\n",
    "Для выполнения работы вам следует взять все файлы из папки на HDFS /labs/slaba03/.\n",
    "\n",
    "Давайте посмотрим, что у нас есть:\n",
    "\n",
    "$ hdfs dfs -ls /labs/slaba03/\n",
    "Found 4 items\n",
    "-rw-r--r--   3 hdfs hdfs   91066524 2019-03-17 21:07 /labs/slaba03/laba03_items.csv\n",
    "-rw-r--r--   3 hdfs hdfs   29965581 2019-03-17 21:07 /labs/slaba03/laba03_test.csv\n",
    "-rw-r--r--   3 hdfs hdfs   74949368 2019-03-17 21:07 /labs/slaba03/laba03_train.csv\n",
    "-rw-r--r--   3 hdfs hdfs  871302535 2019-03-17 21:07 /labs/slaba03/laba03_views_programmes.csv\n",
    "В laba03_train.csv содержатся факты покупки (колонка purchase) пользователями (колонка user_id) телепередач (колонка item_id). \n",
    "Такой формат файла вам уже знаком.\n",
    "\n",
    "laba03_items.csv — дополнительные данные по items. В данном файле много лишней или ненужной информации, так что задача её \n",
    "фильтрации и отбора ложится на вас. Поля в файле, на которых хотелось бы остановиться:\n",
    "\n",
    "item_id — primary key. Соответствует item_id в предыдущем файле.\n",
    "content_type — тип телепередачи (1 — платная, 0 — бесплатная). Вас интересуют платные передачи.\n",
    "title — название передачи, текстовое поле.\n",
    "year — год выпуска передачи, число.\n",
    "genres — поле с жанрами передачи, разделёнными через запятую.\n",
    "\n",
    "laba03_test.csv — тестовый датасет без указанного целевого признака purchase, который вам и предстоит предсказать.\n",
    "\n",
    "Дополнительный файл laba03_views_programmes.csv по просмотрам передач с полями:\n",
    "ts_start — время начала просмотра.\n",
    "ts_end — время окончания просмотра.\n",
    "item_type — тип просматриваемого контента:\n",
    "live — просмотр \"вживую\", в момент показа контента в эфире.\n",
    "pvr — просмотр в записи, после показа контента в эфире.\n",
    "\n",
    "Результат\n",
    "Предсказание целевой переменной \"купит/не купит\" — хорошо знакомая вам задача бинарной классификации. Поскольку нам важны именно\n",
    "вероятности отнесения пары (пользователь, товар) к классу \"купит\" (1), то, на самом деле, вы можете подойти к проблеме с разных\n",
    "сторон:\n",
    "\n",
    "Как просто к задаче бинарной классификации. У вас есть два датасета, которые можно каким-то образом объединить, дополнительно \n",
    "обработать и сделать предсказания классификаторами (Spark ML).\n",
    "Как к разработке рекомендательной системы: рекомендовать пользователю user_id топ-N лучших телепередач, которые были найдены по\n",
    "    методике user-user / item-item коллаборативной фильтрации.\n",
    "Как к задаче факторизации матриц: алгоритмы SVD, ALS, FM/FFM.\n",
    "Советы\n",
    "На качество прогноза в большей степени влияет качество признаков, которые вы сможете придумать из имеющихся данных, нежели выбор\n",
    "и сложность алгоритма.\n",
    "Качество входных данных также имеет сильное значение. Существует фраза \"garbage in – garbage out\". Мусор на входе – мусор на \n",
    "выходе. Потратьте время на подготовку и предобработку данных. Путь к успеху в третьей лабораторной:\n",
    "    \n",
    "Сосредоточьтесь на формировании следующих фичей: по файлу laba03_train.csv сформируйте признаки, характеризирующие как \n",
    "    интенсивно покупает пользователь и \"покупаемость\" item'ов\n",
    "возьмите достаточно мощную модель (например GBTClassifier из pyspark'а)\n",
    "\n",
    "Проверка\n",
    "Эта лаба проходит в формате соревнования. Для вас оно начинается, когда вы успешно пройдёте минимальный порог — AUC должен \n",
    "составить не менее 0.79. После этого вы увидите лидерборд и сможете следить за результатами других участников.\n",
    "\n",
    "Как уже было сказано, мы будем оценивать ваш алгоритм по метрике ROC AUC. Чекеру требуются вероятности в диапазоне [0.0, 1.0] \n",
    "отнесения пары (пользователь, товар) в тестовой выборке к классу \"1\" (купит).\n",
    "\n",
    "Важно! Для точной проверки не забудьте отсортировать полученный файл по возрастанию идентификаторов пользователей (user_id), \n",
    "а затем — по возрастанию идентификаторов передач (item_id).\n",
    "\n",
    ",user_id,item_id,purchase\n",
    "0,1654,336,0.021805684684958027\n",
    "1,1654,678,0.021805684684958027\n",
    "2,1654,691,0.021805684684958027\n",
    "3,1654,696,0.021805684684958027\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 2 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "from pyspark import Row\n",
    "import json\n",
    "import re\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "spark.conf.set(\"spark.sql.crossJoin.enabled\", True) # for cartesian product usage\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .config(conf=conf)\n",
    "         .appName(\"test\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(fields=[\n",
    "    StructField('item_id', IntegerType()),\n",
    "    StructField('user_id', IntegerType()),\n",
    "    StructField('purchase',IntegerType())\n",
    "])\n",
    "df1 = spark.read.csv(\"hdfs:///labs/slaba03/laba03_train.csv\",header='true',schema=schema)\n",
    "df2 = spark.read.csv(\"hdfs:///labs/slaba03/laba03_test.csv\", header='true',schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = df1.groupBy('user_id').mean('purchase').coalesce(10).cache()\n",
    "items = df1.groupBy('item_id').mean('purchase').coalesce(10)\n",
    "\n",
    "df2 = df2.join(users, on = 'user_id', how = 'outer').coalesce(10).withColumnRenamed('avg(purchase)', 'ave_user').cache()\n",
    "df2 = df2.join(items, on = 'item_id', how = 'outer').coalesce(10).withColumnRenamed('avg(purchase)', 'ave_item').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "w1 = Window.partitionBy('user_id')\n",
    "w2 = Window.partitionBy('item_id')\n",
    "\n",
    "df1 = df1.withColumn('ave_user', f.avg('purchase').over(w1)) \\\n",
    "         .withColumn('ave_item', f.avg('purchase').over(w2)).coalesce(10).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "vas = VectorAssembler(inputCols = ['ave_user', 'ave_item'], outputCol = 'features')\n",
    "gbt = GBTClassifier(featuresCol = 'features', labelCol = 'purchase')\n",
    "ppl = Pipeline(stages=[vas,gbt]).fit(df1)\n",
    "predict = ppl.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "firstelement = udf(lambda v : float(v[1]), FloatType())\n",
    "result = predict.select('user_id', 'item_id', firstelement('probability').alias('purchase')).coalesce(1).cache()\n",
    "result = result.orderBy('user_id', 'item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_p = result.toPandas()\n",
    "#result_p.to_csv('lab03.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
