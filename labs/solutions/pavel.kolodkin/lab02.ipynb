{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Задача\n",
    "По имеющимся данным портала eclass.cc построить content-based рекомендации по образовательным курсам.\n",
    "Запрещено использовать библиотеки pandas, sklearn и аналогичные.\n",
    "\n",
    "Описание данных\n",
    "Имеются следующие данные на вход:\n",
    "\n",
    "набор данных о всех курсах. Датасет можно взять с HDFS по адресу: /labs/slaba02/DO_record_per_line.json\n",
    "id курсов, для которых надо дать рекомендации (указаны в Личном кабинете).\n",
    "\n",
    "\n",
    "Результат\n",
    "Для каждого id курса из личного кабинета необходимо дать топ-10 наиболее похожих на него курсов. \n",
    "Рекомендованные курсы должны быть того же языка, что и курс, для которого строится рекомендация.\n",
    "\n",
    "Выходной формат — json — должен иметь следующую структуру:\n",
    "\n",
    "{\n",
    "  \"123\": [\n",
    "    5372,\n",
    "    16663,\n",
    "    23114,\n",
    "    13079,\n",
    "    13084,\n",
    "    ...\n",
    "  ],\n",
    "  \"456\": [\n",
    "    ...\n",
    "  ],\n",
    "  \"789\": [\n",
    "    ...\n",
    "  ],\n",
    "  \"123456\": [\n",
    "    ...\n",
    "  ],\n",
    "  \"456789\": [\n",
    "    ...\n",
    "  ],\n",
    "  \"987654\": [\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "Ключи json — это id курсов, для которых строится рекомендация.\n",
    "Для каждого такого ключа в качестве значения задается массив рекомендованных курсов, состоящий из их id,\n",
    "отсортированных по убыванию метрики. При равенстве значений метрики курсов сортируются лексикографически по названию.\n",
    "\n",
    "Также возможна очень редкая ситуация (в основном с русскоязычными курсами), когда в рекомендацию попадут два дубликата одного\n",
    "курса, но с разными id. Таких дубликатов очень мало относительно числа курсов, но все равно рекомендуется их сортировать\n",
    "в следующей последовательности: по метрике (убывание) => по названию (лексикографически по возрастанию) => по возрастанию id.\n",
    "\n",
    "Также вы можете найти так называемый submission-файл по следующему пути на\n",
    "мастер-ноде: /share/submission-files/slaba02/lab02.json. \n",
    "обладает правильной структурой и форматом, но неправильными значениями. Идея в том, что он проходит необходимые требования \n",
    "чекера именно по структуре, и вам не придется пытаться понять, в чем дело, а также почему чекер не принимает ваш файл.\n",
    "\n",
    "Советы\n",
    "Для подбора рекомендаций следует использовать меру TFIDF, а в качестве метрики для ранжирования — косинус угла между \n",
    "TFIDF-векторами для разных курсов\n",
    "\n",
    "Что такое TFIDF? TF — это term frequency: по сути, сколько раз слово встречается в этом документе. Если мы сделаем такой\n",
    "    word count по каждому документу, то получим вектор, который как-то характеризует этот документ.\n",
    "\n",
    "мама - 2\n",
    "мыла - 1\n",
    "раму - 1\n",
    "лапу - 1\n",
    "роза - 2\n",
    "упала - 3\n",
    "Если мы сравним вектора, рассчитав дистанцую между ними, то получим вывод – насколько похожи эти тексты.\n",
    "Назовем этот подход наивным. Этот подход наивен, потому что мы как бы присваиваем одинаковый вес каждому слову,\n",
    "которое у нас есть в тексте. А что если мы попробуем как-то повысить значимость тех слов, которые часто встречаются \n",
    "только в этом тексте? Для этого мы посчитаем DF – document frequency: по сути, число документов, в которых есть вхождение \n",
    "    этого слова. Мы хотим \"штрафовать\" слово за частое появление в документах, поэтому делаем инверсию этой величины – \n",
    "    буква I в TFIDF. Теперь для каждого слова мы будем считать TF и делить на IDF. Так мы получим другой вектор для нашего \n",
    "    документа. Он может быть более правильным для наших задач.\n",
    "\n",
    "TFIDF нужно считать для описаний курсов (desc). При извлечении слов из описания словом считаем то, что состоит из латинских\n",
    "или кириллических букв или цифр, знаки препинания и прочие символы не учитываются.\n",
    "\n",
    "Для поиска слов можно использовать такой код на Python (может быть проблема с распознаванием юникода).\n",
    "import re\n",
    "regex = re.compile(u'[\\w\\d]{2,}', re.U)\n",
    "regex.findall(string.lower())\n",
    " \n",
    "Сам TFIDF реализован в Spark, писать с нуля вычисления не требуется. При вычислении TF с помощью HashingTF использовалось \n",
    "число фичей: 10000. То есть:\n",
    " \n",
    " tf = HashingTF(10000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 2 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark import Row\n",
    "import json\n",
    "import re\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "spark.conf.set(\"spark.sql.crossJoin.enabled\", True)\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .config(conf=conf)\n",
    "         .appName(\"test\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"hdfs:///labs/slaba02/DO_record_per_line.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(string):\n",
    "    regex = re.compile(u'[\\w\\d]{2,}', re.U)\n",
    "    return regex.findall(string.lower())\n",
    "tokenization = f.udf(tokenization, ArrayType(StringType()))\n",
    "df = df.withColumn(\"words\", tokenization(df.desc))\n",
    "df = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=10000).transform(df)\n",
    "df = IDF(inputCol=\"rawFeatures\", outputCol=\"features\").fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[23126, 'en', 'Compass - powerful SASS library that makes your life easier'],\n",
       " [21617, 'en', 'Preparing for the AP* Computer Science A Exam — Part 2'],\n",
       " [16627, 'es', 'Aprende Excel: Nivel Intermedio by Alfonso Rinsche'],\n",
       " [11556,\n",
       "  'es',\n",
       "  'Aprendizaje Colaborativo by UNID Universidad Interamericana para el Desarrollo'],\n",
       " [16704, 'ru', 'Программирование на Lazarus'],\n",
       " [13702, 'ru', 'Математическая экономика']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# id courses to make recommendations\n",
    "a = [[23126, u'en', u'Compass - powerful SASS library that makes your life easier'], \n",
    "     [21617, u'en', u'Preparing for the AP* Computer Science A Exam \\u2014 Part 2'], \n",
    "     [16627, u'es', u'Aprende Excel: Nivel Intermedio by Alfonso Rinsche'], \n",
    "     [11556, u'es', u'Aprendizaje Colaborativo by UNID Universidad Interamericana para el Desarrollo'], \n",
    "     [16704, u'ru', u'\\u041f\\u0440\\u043e\\u0433\\u0440\\u0430\\u043c\\u043c\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u043d\\u0430 Lazarus'], \n",
    "     [13702, u'ru', u'\\u041c\\u0430\\u0442\\u0435\\u043c\\u0430\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u0430\\u044f \\u044d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u043a\\u0430']]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@f.udf(returnType=DoubleType())\n",
    "def cos_sim(v,u):\n",
    "    try:\n",
    "        return float(v.dot(u))/float(v.norm(2)*u.norm(2))\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{23126: [14760, 13665, 13782, 20638, 24419, 15909, 2724, 25782, 17499, 13348], 21617: [21609, 21616, 21608, 22298, 21630, 21628, 21623, 21508, 21081, 19417], 16627: [11431, 11575, 12247, 17964, 5687, 17961, 16694, 12660, 25010, 5558], 11556: [16488, 468, 13461, 23357, 19330, 7833, 9289, 10447, 22710, 11340], 16704: [1236, 1247, 1365, 1273, 20288, 1164, 8186, 1233, 8203, 875], 13702: [864, 21079, 8313, 1041, 28074, 8300, 1033, 13057, 21025, 1111]}\n"
     ]
    }
   ],
   "source": [
    "dic = dict()\n",
    "\n",
    "for i in a:\n",
    "    df2 = df.filter((df.id != i[0]) & (df.lang == i[1]))\n",
    "    df3 = df.filter(df.id == i[0])[['features']]\n",
    "    df4 = df2.join(df3.withColumnRenamed('features', 'temp'))\n",
    "    df4 = df4.withColumn('cos_sim', cos_sim(df4.features, df4.temp))\n",
    "    dic[i[0]] = list(df4.orderBy(df4.cos_sim.desc(), df4.name, df4.id).limit(10)[['id']].toPandas()['id'])\n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lab02.json', 'w') as f:\n",
    "    json.dump(dic, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
