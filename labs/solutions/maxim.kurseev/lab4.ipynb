{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лаба 4. Прогнозирование пола и возрастной категории — Spark Streaming\n",
    "\n",
    "[git laba4](https://github.com/newprolab/sber-spark-ds-10/blob/main/labs/lab04.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У вас имеются данные: логи посещения пользователей по разным сайтам рунета. \n",
    "\n",
    "По этим пользователям вам также известны: пол и возрастная категория. \n",
    "\n",
    "Вам нужно будет в real-time спрогнозировать эти характеристики по пользователям, о которых у вас нет этой информации, но будут все те же самые логи посещения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Описание данных**\n",
    "\n",
    "Обучающая выборка, с которой вы будете работать, выглядит следующим образом:\n",
    "\n",
    "\n",
    "Она расположена на hdfs: `/labs/slaba04/`.\n",
    "\n",
    "Поле `gender` принимает значения F (женщина) и M (мужчина).\n",
    "\n",
    "Поле `age` принимает значения диапазона возраста: 18-24, 25-34, 35-44, 45-54, >=55\n",
    "\n",
    "Поле `uid` принимает значения уникального ID пользователя (cookies): d50192e5-c44e-4ae8-ae7a-7cfe67c8b777.\n",
    "\n",
    "Поле user_json имеет внутри json со следующей схемой данных:\n",
    "\n",
    "{\"visits\": [{\"url\": \"url1\", \"timestamp\": \"timestamp1\"}, {\"url\": \"url2\", \"timestamp\": \"timestamp2\"}]}. \n",
    "\n",
    "В нем содержатся непосредственно логи посещения пользователем страниц вместе с временной меткой посещения.\n",
    "\n",
    "То есть все то же самое, только без поля gender и age и немного с другой схемой данных. Название топика, в который мы будем присылать данные: input_ivan.ivanov, где вместо ivan.ivanov ваш логин от личного кабинета. Данные вам будут поступать по нажатию кнопки \"Проверить\" в личном кабинете. Вам топик создавать не нужно. Он будет автоматом создаваться при записи. Чистить его тоже не надо, потому что Spark Streaming хранит оффсеты, то есть знает, что он уже считывал, что еще не считывал.\n",
    "\n",
    "Порт брокера Kafka: 6667. Hostname: spark-master-1.newprolab.com или spark-node-1.newprolab.com.\n",
    "\n",
    "**Результат**\n",
    "Вам в свою очередь нужно будет, считывая данные из Kafka, делать прогноз по ним в Spark Streaming и отправлять в real-time эти прогнозные значения в свой другой топик в Kafka в формате:\n",
    "\n",
    "{\"uid\": \"fe1dba8f-3131-439f-9031-851c0da0f126\", \"gender\": \"M\", \"age\": \"25-34\"}\n",
    "\n",
    "{\"uid\": \"d50192e5-c44e-4ae8-ae7a-7cfe67c8b777\", \"gender\": \"F\", \"age\": \"18-24\"}\n",
    "\n",
    "Название топика — ваш логин в личном кабинете: `ivan.ivanov`. \n",
    "\n",
    "После отправки данных чекер будет ждать 90 секунд, чтобы вы обработали входные данные из топика input_ivan.ivanov и записали результаты прогноза уже в этот топик."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 --executor-memory 3g --driver-memory 2g pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, ArrayType, BinaryType\n",
    "from pyspark.sql.types import ByteType\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "import json\n",
    "from pyspark.ml.linalg import DenseVector, SparseVector\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.ml.feature import  VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"Kurseev Maxim Spark 4 lab\") \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).appName(\"Kurseev Maxim Spark 4 lab\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as T\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StringIndexer, IndexToString\n",
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat /labs/slaba04/gender_age_dataset.txt | head -n2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# формируем spark DF\n",
    "path = '/labs/slaba04/gender_age_dataset.txt'\n",
    "\n",
    "schema = t.StructType(fields=[\n",
    "    t.StructField('gender', t.StringType()),\n",
    "    t.StructField('age', t.StringType()),\n",
    "    t.StructField('uid', t.StringType()),\n",
    "    t.StructField('user_json', t.StringType()),\n",
    "])\n",
    "\n",
    "train_data = spark.read.csv(path, header=True, schema=schema, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# схема для json с визитами\n",
    "visits_schema = t.StructType([\n",
    "    t.StructField('visits', t.ArrayType(\n",
    "        t.StructType([\n",
    "            t.StructField('url', t.StringType(), True),\n",
    "            t.StructField('timestamp', t.LongType(), True)\n",
    "        ])\n",
    "    ))\n",
    "])\n",
    "\n",
    "spark_df_flattened = train_data.withColumn(\"visits\", f.from_json(f.col(\"user_json\"), visits_schema))\n",
    "spark_df_flattened= spark_df_flattened.withColumn(\"visit\", f.explode(\"visits.visits\").alias(\"visit\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_flattened = ( spark_df_flattened \n",
    "    .withColumn(\"site\", f.expr(\"parse_url(visit.url, 'HOST')\")) \n",
    "    .drop(\"visits\", \"visit\",\"user_json\")\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final = ( spark_df_flattened \n",
    "    .groupBy(\"gender\", \"age\", \"uid\") \n",
    "    .agg(f.collect_list(\"site\") \n",
    "    .alias(\"sites\"))\n",
    "              )\n",
    "\n",
    "train_final = train_final.na.drop()\n",
    "train_final.select('uid').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_final.randomSplit([0.75, 0.25], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashing_TF = HashingTF(\n",
    "    inputCol='sites', \n",
    "    outputCol='features', \n",
    "    numFeatures=10000, \n",
    "    binary=False)\n",
    "\n",
    "\n",
    "indexer_age = StringIndexer(\n",
    "    inputCol='age', \n",
    "    outputCol='ageIndex').fit(train_final)\n",
    "indexer_gender = StringIndexer(\n",
    "    inputCol='gender', \n",
    "    outputCol='genderIndex').fit(train_final)\n",
    "\n",
    "\n",
    "rf_age = RandomForestClassifier(\n",
    "    featuresCol = 'features', \n",
    "    labelCol = 'ageIndex', \n",
    "    predictionCol='age_pred', \n",
    "    rawPredictionCol='age_raw_pred',\n",
    "    probabilityCol = 'age_probab')\n",
    "\n",
    "\n",
    "rf_gender = RandomForestClassifier(\n",
    "    featuresCol = 'features', \n",
    "    labelCol = 'genderIndex',\n",
    "    predictionCol='gender_pred', \n",
    "    rawPredictionCol='gender_raw_pred',\n",
    "    probabilityCol = 'gender_prob')\n",
    "\n",
    "\n",
    "converter_age = IndexToString(\n",
    "    inputCol='age_pred', \n",
    "    outputCol='PredictedAge', \n",
    "    labels=indexer_age.labels)\n",
    "\n",
    "converter_gender = IndexToString(\n",
    "    inputCol='gender_pred', \n",
    "    outputCol='PredictedGender', \n",
    "    labels=indexer_gender.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    stages=[hashing_TF, indexer_age, indexer_gender, rf_age, rf_gender, converter_age, converter_gender])\n",
    "\n",
    "model = pipeline.fit(train_final)\n",
    "predictions = model.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy for age\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"ageIndex\", predictionCol=\"age_pred\")\n",
    "evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "\n",
    "# calculate accuracy for age\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"genderIndex\", predictionCol=\"gender_pred\")\n",
    "evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_BOOTSTRAP_SERVER = 'spark-node-2.newprolab.com:6667'\n",
    "KAFKA_INPUT_TOPIC = 'input_maxim.kurseev'\n",
    "KAFKA_OUTPUT_TOPIC = 'maxim.kurseev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# чтение в статическом режиме\n",
    "\n",
    "kafka_read_df = (\n",
    "    spark.read\n",
    "    .format('kafka')\n",
    "    .option('kafka.bootstrap.servers', KAFKA_BOOTSTRAP_SERVER)\n",
    "    .option('subscribe', KAFKA_INPUT_TOPIC)\n",
    "    .option('startingOffsets', 'earliest')\n",
    "    .option('failOnDataLoss', 'False')\n",
    "    .load()\n",
    "    .cache()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Парсинг бинарного файла из кафки\n",
    "\n",
    "event_schema = t.StructType([\n",
    "    t.StructField('uid', t.StringType(), True),\n",
    "    t.StructField('visits', t.StringType(), True),\n",
    "])\n",
    "\n",
    "\n",
    "visit_schema = t.ArrayType(\n",
    "    t.StructType([\n",
    "        t.StructField('url', t.StringType(), True),\n",
    "        t.StructField('timestamp', t.LongType(), True)\n",
    "    ])\n",
    ")\n",
    "\n",
    "\n",
    "clean_df = (\n",
    "    kafka_read_df\n",
    "    .select(f.col('value').cast('string').alias('value'))\n",
    "    .select(f.from_json(f.col('value'), event_schema).alias('event'))\n",
    "    .select(\n",
    "        'event.uid', \n",
    "        f.from_json(f.col('event.visits'), visit_schema).alias('visits')\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# извлечение url\n",
    "# применение модели, сохранение предсказаний в predictions_df  \n",
    "\n",
    "prep_df = ( clean_df \n",
    "    .withColumn(\"visit\", f.explode(\"visits\").alias(\"visit\")) \n",
    "    .withColumn(\"site\", f.expr(\"parse_url(visit.url, 'HOST')\")) \n",
    "    .drop(\"visits\", \"visit\") \n",
    "    .groupBy(\"uid\") \n",
    "    .agg(f.collect_list(\"site\").alias(\"sites\"))\n",
    "          )\n",
    "\n",
    "predictions_df = ( model.transform(prep_df) \n",
    "    .select(\"uid\", \"PredictedGender\", \"PredictedAge\") \n",
    "    .withColumnRenamed(\"PredictedAge\",\"age\") \n",
    "    .withColumnRenamed(\"PredictedGender\",\"gender\")\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оборачивание предсказания обратно в json\n",
    "\n",
    "kafka_out_df = (\n",
    "    predictions_df \n",
    "    .select(f.to_json(f.struct(*predictions_df.columns)).alias('value')).limit(5)\n",
    ")\n",
    "\n",
    "# Запись в выходной топик\n",
    "\n",
    "(\n",
    "    kafka_out_df\n",
    "    .write\n",
    "    .format('kafka')\n",
    "    .option('kafka.bootstrap.servers', KAFKA_BOOTSTRAP_SERVER)\n",
    "    .option('topic', KAFKA_OUTPUT_TOPIC)\n",
    "    .save()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# чтение стрима\n",
    "\n",
    "kafka_stream = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format('kafka')\n",
    "    .option('kafka.bootstrap.servers', KAFKA_BOOTSTRAP_SERVER)\n",
    "    .option('subscribe', KAFKA_INPUT_TOPIC)\n",
    "    .option('startingOffsets', 'earliest')\n",
    "    .option('failOnDataLoss', 'False')\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# предсказание и запись\n",
    "\n",
    "clean_df = (\n",
    "    kafka_stream\n",
    "    .select(f.col('value').cast('string').alias('value'))\n",
    "    .select(f.from_json(f.col('value'), event_schema).alias('event'))\n",
    "    .select(\n",
    "        'event.uid', \n",
    "        f.from_json(f.col('event.visits'), visit_schema).alias('visits')\n",
    "    )\n",
    ")\n",
    "\n",
    "prep_df = ( clean_df \n",
    "    .withColumn(\"visit\", f.explode(\"visits\").alias(\"visit\")) \n",
    "    .withColumn(\"site\", f.expr(\"parse_url(visit.url, 'HOST')\")) \n",
    "    .drop(\"visits\", \"visit\") \n",
    "    .groupBy(\"uid\") \n",
    "    .agg(f.collect_list(\"site\").alias(\"sites\"))\n",
    "          )\n",
    "\n",
    "predictions_df = ( model.transform(prep_df) \n",
    "    .select(\"uid\", \"PredictedGender\", \"PredictedAge\") \n",
    "    .withColumnRenamed(\"PredictedAge\",\"age\") \n",
    "    .withColumnRenamed(\"PredictedGender\",\"gender\")\n",
    "                 )\n",
    "\n",
    "kafka_write_stream = (\n",
    "    predictions_df\n",
    "    .select(f.to_json(f.struct(*predictions_df.columns)).alias('value'))\n",
    "    .writeStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"checkpointLocation\", \"checkpoints/checkpoints_lab04\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVER)\n",
    "    .option(\"topic\", KAFKA_OUTPUT_TOPIC)\n",
    "    .outputMode(\"complete\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streams = SparkSession.builder.getOrCreate().streams.active\n",
    "len(streams), streams[0].lastProgress[\"sources\"][0][\"description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_write_stream.isActive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Stop session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_write_stream.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
