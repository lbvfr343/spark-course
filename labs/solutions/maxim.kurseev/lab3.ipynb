{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[git laba3](https://github.com/newprolab/sber-spark-ds-10/blob/main/labs/lab03.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лаба 3. Рекомендательная система видеоконтента с implicit feedback – Spark ML\n",
    "\n",
    "В вашем распоряжении имеется уже предобработанный и очищенный датасет с фактами покупок абонентами телепередач от компании E-Contenta. По доступным вам данным, нужно предсказать вероятность покупки других передач этими, а, возможно, и другими абонентами. При решении задачи запрещено использовать библиотеки pandas, sklearn (кроме sklearn.metrics), xgboost и другие. Если scikit-learn (например, но и другие тоже) обернут в классы Transformer и Estimator, то их можно использовать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Результат**\n",
    "\n",
    "Предсказание целевой переменной \"купит/не купит\" — хорошо знакомая вам задача бинарной классификации. Поскольку нам важны именно вероятности отнесения пары (пользователь, товар) к классу \"купит\" (`1`), то, на самом деле, вы можете подойти к проблеме с разных сторон:\n",
    "\n",
    "1. Как просто к задаче бинарной классификации. У вас есть два датасета, которые можно каким-то образом объединить, дополнительно обработать и сделать предсказания классификаторами (Spark ML). \n",
    "2. Как к разработке рекомендательной системы: рекомендовать пользователю `user_id` топ-N лучших телепередач, которые были найдены по методике user-user / item-item коллаборативной фильтрации. \n",
    "3. Как к задаче факторизации матриц: алгоритмы SVD, ALS, FM/FFM.\n",
    "\n",
    "\n",
    "**Советы**\n",
    "\n",
    "1. На качество прогноза в большей степени влияет качество признаков, которые вы сможете придумать из имеющихся данных, нежели выбор и сложность алгоритма.\n",
    "2. Качество входных данных также имеет сильное значение. Существует фраза \"garbage in – garbage out\". Мусор на входе – мусор на выходе. Потратьте время на подготовку и предобработку данных. Путь к успеху в третьей лабораторной:\n",
    "3. Сосредоточьтесь на формировании следующих фичей: по файлу laba03_train.csv сформируйте признаки, характеризирующие как интенсивно покупает пользователь и \"покупаемость\" item'ов\n",
    "4. возьмите достаточно мощную модель (например GBTClassifier из pyspark'а)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 --executor-memory 3g --driver-memory 2g pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, ArrayType, BinaryType\n",
    "from pyspark.sql.types import ByteType\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "import json\n",
    "from pyspark.ml.linalg import DenseVector, SparseVector\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.ml.feature import  VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"Kurseev Maxim Spark 3 lab\") \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).appName(\"Kurseev Maxim Spark 3 lab\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master-5.newprolab.com:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Kurseev Maxim Spark 3 lab</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f15b566f4e0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train and Test files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_train = StructType(fields=[\n",
    "    StructField('user_id', IntegerType()),\n",
    "    StructField('item_id', IntegerType()),\n",
    "    StructField('purchase', ByteType())\n",
    "])\n",
    "\n",
    "schema_test = StructType(fields=[\n",
    "    StructField('user_id', IntegerType()),\n",
    "    StructField('item_id', IntegerType()),\n",
    "])\n",
    "\n",
    "train = spark.read.csv('/labs/slaba03/laba03_train.csv', header=True, schema=schema_train)\n",
    "test = spark.read.csv('/labs/slaba03/laba03_test.csv', header=True, schema=schema_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|user_id|item_id|\n",
      "+-------+-------+\n",
      "|   1654|  94814|\n",
      "|   1654|  93629|\n",
      "|   1654|   9980|\n",
      "+-------+-------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------+-------+--------+\n",
      "|user_id|item_id|purchase|\n",
      "+-------+-------+--------+\n",
      "|   1654|  74107|       0|\n",
      "|   1654|  89249|       0|\n",
      "|   1654|  99982|       0|\n",
      "+-------+-------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show(3)\n",
    "train.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**items**\n",
    "\n",
    "`item_id` — primary key. Соответствует item_id в предыдущем файле.\n",
    "\n",
    "`content_type` — тип телепередачи (1 — платная, 0 — бесплатная). Вас интересуют платные передачи.\n",
    "\n",
    "`title` — название передачи, текстовое поле.\n",
    "\n",
    "`year` — год выпуска передачи, число.\n",
    "\n",
    "`genres` — поле с жанрами передачи, разделёнными через запятую."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------\n",
      " item_id | 65667   \n",
      " genres  | Эротика \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema_item = StructType(fields=[\n",
    "    StructField('item_id', IntegerType()),\n",
    "    StructField('channel_id', StringType()),\n",
    "    StructField('datetime_availability_start', StringType()),\n",
    "    StructField('datetime_availability_stop', StringType()),\n",
    "    StructField('datetime_show_start', StringType()),\n",
    "    StructField('datetime_show_stop', StringType()),\n",
    "    StructField('content_type', IntegerType()),\n",
    "    StructField('title', StringType()),\n",
    "    StructField('year', IntegerType()),\n",
    "    StructField('genres', StringType()),\n",
    "    StructField('region_id', IntegerType()),\n",
    "])\n",
    "\n",
    "items = spark.read.csv('/labs/slaba03/laba03_items.csv', header=True, sep='\\t',schema=schema_item)\n",
    "items = (items\n",
    "         .filter(\n",
    "             (~F.col('item_id').isNull())\n",
    "             & (F.col('content_type') == 1)\n",
    "         )\n",
    "         .drop('channel_id','region_id','datetime_show_start','datetime_availability_start',\n",
    "               'datetime_availability_stop','datetime_show_stop', 'content_type', 'title')\n",
    "         .na.fill(value=2010,subset=[\"year\"])\n",
    "         .na.fill(value='Мультфильмы',subset=[\"genres\"])\n",
    "         .drop('year') ## потому что только 2010 остается\n",
    "        )\n",
    "\n",
    "items.show(1,False,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|item_id|genres|\n",
      "+-------+------+\n",
      "|      0|     0|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items.select([F.count(F.when(F.isnan(c) | F.col(c).isNull(), c)).alias(c) for c in items.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**views_programmes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## нет в items активных ни одного совпадения\n",
    "\n",
    "read_users_schema = StructType(fields=[StructField('user_id', IntegerType()), \n",
    "StructField('item_id', IntegerType()),\n",
    "StructField('ts_start', IntegerType()),\n",
    "StructField('ts_end', IntegerType()),\n",
    "StructField('item_type', StringType()),\n",
    "]) \n",
    "\n",
    "\n",
    "\n",
    "views_programmes = spark.read.format(\"csv\") \\\n",
    "      .option(\"header\", True) \\\n",
    "      .schema(read_users_schema) \\\n",
    "      .load(\"/labs/slaba03/laba03_views_programmes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------\n",
      " user_id   | 0          \n",
      " item_id   | 7101053    \n",
      " ts_start  | 1491409931 \n",
      " ts_end    | 1491411600 \n",
      " item_type | live       \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "views_programmes.show(1,False,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+\n",
      "|item_id|genres_features|\n",
      "+-------+---------------+\n",
      "|65667  |(83,[20],[1.0])|\n",
      "|65669  |(83,[20],[1.0])|\n",
      "|65668  |(83,[20],[1.0])|\n",
      "|65671  |(83,[20],[1.0])|\n",
      "|65670  |(83,[20],[1.0])|\n",
      "+-------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## func \n",
    "genres_split = F.udf(lambda x: x.split(','), ArrayType(StringType()))\n",
    "items = items.withColumn('genres', genres_split(F.col('genres')))\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"genres\", outputCol=\"genres_features\")\n",
    "model = cv.fit(items)\n",
    "items = model.transform(items).drop('genres')\n",
    "items.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpecAgg  = Window.partitionBy('user_id')\n",
    "train = train.withColumn('user_activity', F.sum('purchase').over(windowSpecAgg)/F.count('purchase').over(windowSpecAgg))\n",
    "\n",
    "windowSpecAgg  = Window.partitionBy('item_id')\n",
    "train = train.withColumn('items_intenstiy', F.sum('purchase').over(windowSpecAgg)/F.count('purchase').over(windowSpecAgg))\n",
    "\n",
    "train = train.join(items, how='left', on='item_id')\n",
    "\n",
    "# check for nan colums\n",
    "cols=[\"item_id\",\"user_id\", \"purchase\", \"user_activity\", \"items_intenstiy\"]\n",
    "train.select([F.count(F.when(F.isnan(c) | F.col(c).isNull(), c)).alias(c) for c in cols]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+--------------------+--------------------+--------------------+\n",
      "|item_id|user_id|purchase|       user_activity|     items_intenstiy|     genres_features|\n",
      "+-------+-------+--------+--------------------+--------------------+--------------------+\n",
      "|   8389| 754230|       0|0.027575641516660282|0.005979073243647235|(83,[6,13,19,22],...|\n",
      "|   8389| 780033|       0|7.757951900698216E-4|0.005979073243647235|(83,[6,13,19,22],...|\n",
      "|   8389| 798454|       0|3.840245775729646...|0.005979073243647235|(83,[6,13,19,22],...|\n",
      "|   8389| 825061|       0|0.001931247585940...|0.005979073243647235|(83,[6,13,19,22],...|\n",
      "|   8389| 833685|       0|0.007500986971969996|0.005979073243647235|(83,[6,13,19,22],...|\n",
      "+-------+-------+--------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = (\n",
    "    VectorAssembler()\n",
    "    .setInputCols(['user_activity', 'items_intenstiy', 'genres_features']) \n",
    "    .setOutputCol('features')\n",
    ")\n",
    "\n",
    "train_fitting = assembler.transform(train)\n",
    "train_df, test_df = train_fitting.randomSplit([0.8, 0.2], seed = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+--------------------+--------------------+--------------------+--------------------+\n",
      "|item_id|user_id|purchase|       user_activity|     items_intenstiy|     genres_features|            features|\n",
      "+-------+-------+--------+--------------------+--------------------+--------------------+--------------------+\n",
      "|   8389| 520446|       0|0.003053435114503...|0.005979073243647235|(83,[6,13,19,22],...|(85,[0,1,8,15,21,...|\n",
      "|   8389| 619378|       0|3.859513701273639...|0.005979073243647235|(83,[6,13,19,22],...|(85,[0,1,8,15,21,...|\n",
      "|   8389| 625678|       0|0.005886970172684459|0.005979073243647235|(83,[6,13,19,22],...|(85,[0,1,8,15,21,...|\n",
      "+-------+-------+--------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* понять через grid_search 2 гиперпараметра `maxIter` и `stepSize`!\n",
    "* понять что predict работает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxDepth = 5   # This indicates how deep the built tree can be\n",
    "maxBins  = 32\n",
    "maxIter  = 70  # number of trees\n",
    "stepSize = 0.1 # learing rate\n",
    "\n",
    "gbt = GBTClassifier(labelCol=\"purchase\", \n",
    "                    featuresCol=\"features\",\n",
    "                    maxDepth=maxDepth,\n",
    "                    maxBins=maxBins,\n",
    "                    maxIter=maxIter,\n",
    "                    stepSize=stepSize,\n",
    "                   )\n",
    "\n",
    "model = gbt.fit(train_df)\n",
    "predictions = model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol = 'purchase', metricName='areaUnderROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Train 0.9368114083960376\n"
     ]
    }
   ],
   "source": [
    "print('ROC AUC Train', evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+--------------------+\n",
      "|item_id|user_id|purchase|         probability|\n",
      "+-------+-------+--------+--------------------+\n",
      "|   8389| 556825|       0|[0.97772420621832...|\n",
      "|   8389| 566701|       0|[0.97562889312552...|\n",
      "|   8389| 613775|       0|[0.98341494594472...|\n",
      "|   8389| 639612|       0|[0.98209654982554...|\n",
      "|   8389| 703514|       0|[0.98345630435580...|\n",
      "+-------+-------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select('item_id','user_id','purchase','probability').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_activity_df = train.groupBy('user_id').agg(\n",
    "    (F.sum('purchase')/F.count('purchase')\n",
    "    ).alias('user_activity'))\n",
    "\n",
    "item_intensivity_df = train.groupBy('item_id').agg(\n",
    "    (F.sum('purchase')/F.count('purchase')\n",
    "    ).alias('item_intensivity'))\n",
    "\n",
    "\n",
    "test = test.join(user_activity_df, how='left', on='user_id')\n",
    "test = test.join(item_intensivity_df, how='left', on='item_id')\n",
    "test = test.join(items, how='left', on='item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler_test = (\n",
    "    VectorAssembler()\n",
    "    .setInputCols(['user_activity', 'item_intensivity', 'genres_features']) \n",
    "    .setOutputCol('features')\n",
    ")\n",
    "\n",
    "test_model = assembler_test.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------------------+--------------------+--------------------+--------------------+\n",
      "|item_id|user_id|       user_activity|    item_intensivity|     genres_features|            features|\n",
      "+-------+-------+--------------------+--------------------+--------------------+--------------------+\n",
      "|   8389| 566758|3.766478342749529E-4|0.005979073243647235|(83,[6,13,19,22],...|(85,[0,1,8,15,21,...|\n",
      "|   8389| 816426|                 0.0|0.005979073243647235|(83,[6,13,19,22],...|(85,[1,8,15,21,24...|\n",
      "|   8389| 892290|7.719027402547279E-4|0.005979073243647235|(83,[6,13,19,22],...|(85,[0,1,8,15,21,...|\n",
      "|   8389| 901323|3.846153846153846E-4|0.005979073243647235|(83,[6,13,19,22],...|(85,[0,1,8,15,21,...|\n",
      "+-------+-------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_model.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = model.transform(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prob = F.udf(lambda x: float(x[1]), FloatType())\n",
    "\n",
    "predictions_test = (predictions_test\n",
    "                    .withColumn('purchase', get_prob(F.col('probability')))\n",
    "                    .select('user_id', 'item_id', 'purchase')\n",
    "                    .orderBy('user_id', 'item_id')\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------+\n",
      "|user_id|item_id|purchase   |\n",
      "+-------+-------+-----------+\n",
      "|1654   |336    |0.016343297|\n",
      "|1654   |678    |0.016343297|\n",
      "|1654   |691    |0.016343297|\n",
      "|1654   |696    |0.016566597|\n",
      "+-------+-------+-----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_test.show(4, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_pandas = predictions_test.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_pandas.to_csv('lab03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "**Grid search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[gbt])\n",
    "\n",
    "paramGrid = ParamGridBuilder().addGrid(gbt.maxIter, [30, 60])\\\n",
    "                              .addGrid(gbt.stepSize, [0.1, 0.3])\\\n",
    "                              .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid,\n",
    "                              evaluator=evaluator, numFolds=4, parallelism=4)\n",
    "\n",
    "cv_model = crossval.fit(train_fitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Stop session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
