{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 2 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import Row\n",
    "import json\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .config(conf=conf)\n",
    "         .appName(\"test\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -ls /labs/slaba03/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import to_date, udf\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Чтение файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = spark.read.csv('/labs/slaba03/laba03_items.csv', sep='\\t', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = spark.read.csv('/labs/slaba03/laba03_test.csv', sep=',', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = spark.read.csv('/labs/slaba03/laba03_train.csv', sep=',', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "views_programmes = spark.read.csv('/labs/slaba03/laba03_views_programmes.csv', sep=',', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Жанры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выделяем первый (возможно основной жанр)\n",
    "item = item.withColumn(\"First\", F.split(F.col(\"genres\"), \",\").getItem(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## История"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция расчета агрегатов \n",
    "def get_aggs_all(tbl, field, aggs, alias=''):\n",
    "    if alias!='':\n",
    "        alias='_{}'.format(alias)\n",
    "    df_tbl = tbl\n",
    "    df_values = df_tbl.select('user_id', *[df_tbl[field].alias(x) for x in aggs])\n",
    "    df_values = df_values.groupby('user_id').agg(dict(zip(aggs,aggs)))\n",
    "    schema = StructType([StructField(x.name, StringType(), False) if x.name=='user_id' else \\\n",
    "                  StructField('agg_all_' + str(x.name).split('(')[0] + '{}'.format(alias), FloatType(), True)\\\n",
    "                  for x in df_values.schema])\n",
    "    df_values = df_values.rdd.map(lambda row: [row.user_id]+ [float(x) if x!=None else None for x in row[1:]])\\\n",
    "    .toDF(schema).fillna(0)\n",
    "    return df_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Расчет длительности просмотров\n",
    "views_programmes = views_programmes.withColumn('DiffInSeconds',F.col(\"ts_end\").cast(\"long\") - F.col('ts_start').cast(\"long\"))\\\n",
    ".select('user_id', 'item_id', F.from_unixtime('ts_start').alias('ts_start'), \n",
    "                        F.from_unixtime('ts_end').alias('ts_end'), 'item_type', 'DiffInSeconds')\\\n",
    ".withColumn('DiffInMins', F.round(F.col('DiffInSeconds')/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = get_aggs_all(views_programmes, 'DiffInMins', ['sum', 'count', 'avg', 'max', 'min'])\n",
    "\n",
    "tbl = views_programmes.groupby('user_id', 'item_id').agg(F.sum('DiffInMins').alias('col'))\n",
    "res2 = get_aggs_all(tbl, 'col', ['sum', 'count', 'avg', 'max', 'min'], 'group')\n",
    "\n",
    "tbl = views_programmes.filter(F.col('item_type')=='pvr')\n",
    "res3 = get_aggs_all(tbl, 'DiffInMins', ['sum', 'count', 'avg', 'max', 'min'], 'pvr')\n",
    "\n",
    "tbl = views_programmes.filter(F.col('item_type')=='live')\n",
    "res4 = get_aggs_all(tbl, 'DiffInMins', ['sum', 'count', 'avg', 'max', 'min'], 'live')\n",
    "\n",
    "res = res1.join(res2, 'user_id', 'inner').join(res3, 'user_id', 'left').join(res4, 'user_id', 'left').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Киллер фича"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "killer = get_aggs_all(train, 'purchase', ['sum', 'avg'], 'killer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alls = train.join(test, train.columns, 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alls = alls.join(killer, 'user_id', 'left').join(res, 'user_id', 'left')\\\n",
    ".join(item.select(*[x for x in item.columns if x not in ['channel_id',\n",
    " 'datetime_availability_start',\n",
    " 'datetime_availability_stop',\n",
    " 'datetime_show_start',\n",
    " 'datetime_show_stop',\n",
    " 'content_type',\n",
    " 'title',\n",
    " 'genres',\n",
    " 'region_id',]]), 'item_id', 'left').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"First\", outputCol=\"first_le\", ) \n",
    "alls = indexer.setHandleInvalid(\"keep\").fit(alls).transform(alls) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alls = alls.withColumn('year_n', F.col('year').cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alls.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [x for x in alls.columns if x not in ['item_id', 'user_id', 'purchase', 'First', 'year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alls_tr = alls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=cols, outputCol='features')\n",
    "stream_df = assembler.transform(alls_tr.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_df.select('item_id', 'user_id', 'purchase', 'features').head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol='features', labelCol=\"purchase\", maxIter=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(numTrees=100, maxDepth=4, featuresCol='features', labelCol=\"purchase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(featuresCol='features', labelCol='purchase', maxDepth=3, maxIter=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sp = stream_df.select('item_id', 'user_id', F.col('purchase').cast(FloatType()), 'features')\\\n",
    ".join(train.select('item_id', 'user_id'), ['item_id', 'user_id'], 'inner')\n",
    "test_sp = stream_df.select('item_id', 'user_id', 'features')\\\n",
    ".join(test.select('item_id', 'user_id'), ['item_id', 'user_id'], 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = rf.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_model = gbt.fit(train_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = gbt_model.transform(test_sp)\n",
    "predictions_train = gbt_model.transform(train_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test.select('user_id', 'item_id', 'probability')\\\n",
    ".withColumn('purchase', vector_to_array(F.col('probability')).getItem(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test.sort('user_id', 'item_id').repartition(1).write.mode('overwrite').csv('lab03', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
