{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 4 --driver-memory 8g pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark import Row\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer, OneHotEncoderEstimator, VectorAssembler, StringIndexer\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier, DecisionTreeClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Imputer, OneHotEncoderEstimator, VectorAssembler, StringIndexer\n",
    "from pyspark.ml.linalg import VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .config(conf=conf)\n",
    "         .appName(\"KMP_lab03_2v\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master-4.newprolab.com:4047\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fea6f5d74e0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Читаем и обрабатываем данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "-rw-r--r--   3 hdfs hdfs   91066524 2022-01-06 18:46 /labs/slaba03/laba03_items.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs   29965581 2022-01-06 18:46 /labs/slaba03/laba03_test.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs   74949368 2022-01-06 18:46 /labs/slaba03/laba03_train.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs  871302535 2022-01-06 18:46 /labs/slaba03/laba03_views_programmes.csv\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /labs/slaba03/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Телепередачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`laba03_items.csv`** — дополнительные данные по items. В данном файле много лишней или ненужной информации, так что задача её фильтрации и отбора ложится на вас. Поля в файле, на которых хотелось бы остановиться:\n",
    "\n",
    "- `item_id` — primary key. Соответствует item_id в предыдущем файле.\n",
    "- `content_type` — тип телепередачи (1 — платная, 0 — бесплатная). Вас интересуют платные передачи.\n",
    "- `title` — название передачи, текстовое поле.\n",
    "- `year` — год выпуска передачи, число.\n",
    "- `genres` — поле с жанрами передачи, разделёнными через запятую."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = spark.read.csv('/labs/slaba03/laba03_items.csv', sep=\"\\t\", header=True)\\\n",
    "             .filter(F.col(\"content_type\") == F.lit(1))\n",
    "items = items.withColumn(\"year\", F.col(\"year\").cast(T.IntegerType()))\\\n",
    "    .withColumn(\"datetime_availability_start\", F.to_timestamp(F.col(\"datetime_availability_start\"), \n",
    "                                                        \"yyyy-MM-dd'T'HH:mm:ss'Z'\"))\\\n",
    "    .withColumn(\"datetime_availability_stop\", F.to_timestamp(F.col(\"datetime_availability_stop\"), \n",
    "                                                        \"yyyy-MM-dd'T'HH:mm:ss'Z'\"))\\\n",
    "    .withColumn(\"datetime_show_start\", F.to_timestamp(F.col(\"datetime_show_start\"), \n",
    "                                                        \"yyyy-MM-dd'T'HH:mm:ss'Z'\"))\\\n",
    "    .withColumn(\"datetime_show_stop\", F.to_timestamp(F.col(\"datetime_show_stop\"), \n",
    "                                                        \"yyyy-MM-dd'T'HH:mm:ss'Z'\"))\\\n",
    "    .withColumn(\"title\", F.lower(F.col(\"title\")))\\\n",
    "    .withColumn(\"genres\", F.lower(F.col(\"genres\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = items.filter(F.col(\"year\").isNotNull())\n",
    "items = items.filter(F.col(\"genres\").isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разметим жанры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3668, 3668)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items.select(F.col(\"item_id\")).count(), items.select(F.col(\"item_id\")).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              genres|count|\n",
      "+--------------------+-----+\n",
      "|ужасы,триллеры,за...|   79|\n",
      "|мультфильмы,детск...|   72|\n",
      "|  комедии,зарубежные|   66|\n",
      "|  эротика,зарубежные|   58|\n",
      "|        комедии,наши|   53|\n",
      "|             эротика|   51|\n",
      "|комедии,драмы,зар...|   50|\n",
      "|    драмы,зарубежные|   48|\n",
      "|триллеры,драмы,за...|   46|\n",
      "|    ужасы,зарубежные|   45|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items.groupBy(\"genres\").count().orderBy(F.col(\"count\").desc()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Т.к. жанр может встретиться в 1 фильме несколько раз, отметим ТОП 10 жанров. Остальные в Прочее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_top = Counter(\",\".join([gen[0].replace(\" \", \"\") for gen in items.select(F.col(\"genres\")).collect()]).split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('зарубежные', 1739),\n",
       " ('драмы', 957),\n",
       " ('комедии', 857),\n",
       " ('триллеры', 655),\n",
       " ('русские', 582),\n",
       " ('боевики', 543),\n",
       " ('наши', 517),\n",
       " ('мелодрамы', 473),\n",
       " ('приключения', 437),\n",
       " ('длядетей', 427)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(genres_top.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = items.withColumn(\"gen_top1\", \n",
    "                         F.when(F.col(\"genres\").like(\"%зарубежные%\"), F.lit(1))\\\n",
    "                         .otherwise(F.lit(0)))\\\n",
    "             .withColumn(\"gen_top2\", \n",
    "                         F.when(F.col(\"genres\").like(\"%драмы%\"), F.lit(1))\\\n",
    "                         .otherwise(F.lit(0)))\\\n",
    "             .withColumn(\"gen_top3\", \n",
    "                         F.when(F.col(\"genres\").like(\"%комедии%\"), F.lit(1))\\\n",
    "                         .otherwise(F.lit(0)))\\\n",
    "             .withColumn(\"gen_top4\", \n",
    "                         F.when(F.col(\"genres\").like(\"%триллеры%\"), F.lit(1))\\\n",
    "                         .otherwise(F.lit(0)))\\\n",
    "             .withColumn(\"gen_top5\", \n",
    "                         F.when(F.col(\"genres\").like(\"%русские%\"), F.lit(1))\\\n",
    "                         .otherwise(F.lit(0)))\\\n",
    "             .withColumn(\"gen_top6\", \n",
    "                         F.when(F.col(\"genres\").like(\"%боевики%\"), F.lit(1))\\\n",
    "                         .otherwise(F.lit(0)))\\\n",
    "             .withColumn(\"gen_top7\", \n",
    "                         F.when(F.col(\"genres\").like(\"%наши%\"), F.lit(1))\\\n",
    "                         .otherwise(F.lit(0)))\\\n",
    "             .withColumn(\"gen_top8\", \n",
    "                         F.when(F.col(\"genres\").like(\"%мелодрамы%\"), F.lit(1))\\\n",
    "                         .otherwise(F.lit(0)))\\\n",
    "             .withColumn(\"gen_top9\", \n",
    "                         F.when(F.col(\"genres\").like(\"%приключения%\"), F.lit(1))\\\n",
    "                         .otherwise(F.lit(0)))\\\n",
    "             .withColumn(\"gen_top10\", \n",
    "                         F.when(F.col(\"genres\").like(\"%для детей%\"), F.lit(1))\\\n",
    "                         .otherwise(F.lit(0)))\\\n",
    "             .withColumn(\"gen_others\", \n",
    "                         F.when((F.col(\"gen_top1\")==F.lit(1))|(F.col(\"gen_top2\")==F.lit(1))\n",
    "                                |(F.col(\"gen_top3\")==F.lit(1))|(F.col(\"gen_top4\")==F.lit(1))\n",
    "                                |(F.col(\"gen_top5\")==F.lit(1))|(F.col(\"gen_top6\")==F.lit(1))\n",
    "                                |(F.col(\"gen_top7\")==F.lit(1))|(F.col(\"gen_top8\")==F.lit(1))\n",
    "                                |(F.col(\"gen_top9\")==F.lit(1))|(F.col(\"gen_top10\")==F.lit(1)), \n",
    "                                F.lit(0))\\\n",
    "                         .otherwise(F.lit(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|min(year)|max(year)|\n",
      "+---------+---------+\n",
      "|     1916|     2017|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items.select(F.min(F.col(\"year\")), F.max(F.col(\"year\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = items.withColumn(\"old_years\", \n",
    "                         F.when(F.col(\"year\") <= F.lit(1950), \n",
    "                                F.lit(1))\\\n",
    "                         .otherwise(F.lit(0)))\\\n",
    "             .withColumn(\"1951-1980\", \n",
    "                         F.when(F.col(\"year\").between(F.lit(1951), F.lit(1980)), \n",
    "                                F.lit(1))\\\n",
    "                         .otherwise(F.lit(0)))\\\n",
    "             .withColumn(\"1981-2000\", \n",
    "                         F.when(F.col(\"year\").between(F.lit(1981), F.lit(2000)), \n",
    "                                F.lit(1))\\\n",
    "                         .otherwise(F.lit(0)))\\\n",
    "             .withColumn(\"2001-2010\", \n",
    "                         F.when(F.col(\"year\").between(F.lit(2001), F.lit(2010)), \n",
    "                                F.lit(1))\\\n",
    "                         .otherwise(F.lit(0)))\\\n",
    "             .withColumn(\"new_years\", \n",
    "                         F.when(F.col(\"year\") >= F.lit(2011), \n",
    "                                F.lit(1))\\\n",
    "                         .otherwise(F.lit(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(item_id='65667', channel_id=None, datetime_availability_start=datetime.datetime(1970, 1, 1, 0, 0), datetime_availability_stop=datetime.datetime(2018, 1, 1, 0, 0), datetime_show_start=None, datetime_show_stop=None, content_type='1', title='на пробах только девушки (all girl auditions)', year=2013, genres='эротика', region_id=None, gen_top1=0, gen_top2=0, gen_top3=0, gen_top4=0, gen_top5=0, gen_top6=0, gen_top7=0, gen_top8=0, gen_top9=0, gen_top10=0, gen_others=1, old_years=0, 1951-1980=0, 1981-2000=0, 2001-2010=0, new_years=1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id_p = [item[0] for item in items.select(F.col(\"item_id\")).collect()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Просмотры телепередач."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительный файл **`laba03_views_programmes.csv`** по просмотрам передач с полями:\n",
    "\n",
    "- `ts_start` — время начала просмотра.\n",
    "- `ts_end` — время окончания просмотра.\n",
    "- `item_type` — тип просматриваемого контента:\n",
    "    - `live` — просмотр \"вживую\", в момент показа контента в эфире.\n",
    "    - `pvr` — просмотр в записи, после показа контента в эфире."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- item_id: string (nullable = true)\n",
      " |-- ts_start: long (nullable = true)\n",
      " |-- ts_end: long (nullable = true)\n",
      " |-- item_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "views_prog = spark.read.csv('/labs/slaba03/laba03_views_programmes.csv', header=True)\\\n",
    "                    .withColumn(\"ts_start\", F.col(\"ts_start\").cast(T.LongType()))\\\n",
    "                    .withColumn(\"ts_end\", F.col(\"ts_end\").cast(T.LongType()))\n",
    "views_prog.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "views_prog = views_prog.withColumn(\"ts_diff\", F.col(\"ts_end\") - F.col(\"ts_start\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_live_avg = views_prog\\\n",
    "                        .filter(F.col(\"item_type\") == F.lit(\"live\"))\\\n",
    "                        .groupBy(F.col(\"user_id\"))\\\n",
    "                        .agg(F.mean(F.col(\"ts_diff\")).alias(\"user_avg_live\"),\n",
    "                             F.count(F.col(\"item_id\")).alias(\"user_cnt_live\"))\n",
    "user_pvr_avg = views_prog\\\n",
    "                        .filter(F.col(\"item_type\") == F.lit(\"pvr\"))\\\n",
    "                        .groupBy(F.col(\"user_id\"))\\\n",
    "                        .agg(F.mean(F.col(\"ts_diff\")).alias(\"user_avg_pvr\"),\n",
    "                             F.count(F.col(\"item_id\")).alias(\"user_cnt_pvr\"))\n",
    "\n",
    "item_live_avg = views_prog\\\n",
    "                        .filter(F.col(\"item_type\") == F.lit(\"live\"))\\\n",
    "                        .groupBy(F.col(\"item_id\"))\\\n",
    "                        .agg(F.mean(F.col(\"ts_diff\")).alias(\"item_avg_live\"))\n",
    "item_pvr_avg = views_prog\\\n",
    "                        .filter(F.col(\"item_type\") == F.lit(\"pvr\"))\\\n",
    "                        .groupBy(F.col(\"item_id\"))\\\n",
    "                        .agg(F.mean(F.col(\"ts_diff\")).alias(\"item_avg_pvr\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Факты покупки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В **`laba03_train.csv`** содержатся факты покупки (колонка `purchase`) пользователями (колонка `user_id`) телепередач (колонка `item_id`). Такой формат файла вам уже знаком."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- purchase: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train = spark.read.csv('/labs/slaba03/laba03_train.csv', header=True)\\\n",
    "             .withColumn(\"user_id\", F.col(\"user_id\").cast(T.IntegerType()))\\\n",
    "             .withColumn(\"item_id\", F.col(\"item_id\").cast(T.IntegerType()))\\\n",
    "             .withColumn(\"purchase\", F.col(\"purchase\").cast(T.IntegerType()))\n",
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5032624"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|cnt_user|cnt_item|\n",
      "+--------+--------+\n",
      "|    1941|    3704|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.select(F.countDistinct(F.col(\"user_id\")).alias(\"cnt_user\"), \n",
    "             F.countDistinct(F.col(\"item_id\")).alias(\"cnt_item\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----------+----------+\n",
      "|purchase|   rows|cnt_d_user|cnt_d_item|\n",
      "+--------+-------+----------+----------+\n",
      "|       1|  10904|      1675|      3089|\n",
      "|       0|5021720|      1941|      3704|\n",
      "+--------+-------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.groupBy(F.col(\"purchase\")).agg(F.count(F.col(\"user_id\")).alias(\"rows\"), \n",
    "                                     F.countDistinct(F.col(\"user_id\")).alias(\"cnt_d_user\"), \n",
    "                                     F.countDistinct(F.col(\"item_id\")).alias(\"cnt_d_item\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21713675792357995"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10904/5021720*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В тренировочных данных 0,2% фактов покупок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user = train.groupBy(F.col(\"user_id\")).agg(F.mean(F.col(\"purchase\")).alias('user_purchase_avg'),\n",
    "                                                 F.count(F.col(\"item_id\")).alias(\"user_item_id\"),\n",
    "                                                 F.sum(F.col(\"purchase\")).alias(\"user_purchase_sum\"))\n",
    "train_item = train.groupBy(F.col(\"item_id\")).agg(F.mean(F.col(\"purchase\")).alias('item_purchase_avg'),\n",
    "                                                 F.count(F.col(\"user_id\")).alias(\"item_user_id\"),\n",
    "                                                 F.sum(F.col(\"purchase\")).alias(\"item_purchase_sum\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_merge_features(data):\n",
    "    data = data.alias(\"t\")\\\n",
    "                    .join(train_user.alias(\"tu\"), F.col(\"t.user_id\") == F.col(\"tu.user_id\") , \"left\")\\\n",
    "                    .join(train_item.alias(\"ti\"), F.col(\"t.item_id\") == F.col(\"ti.item_id\") , \"left\")\\\n",
    "                    .join(user_live_avg.alias(\"ul\"), F.col(\"t.user_id\") == F.col(\"ul.user_id\") , \"left\")\\\n",
    "                    .join(user_pvr_avg.alias(\"up\"), F.col(\"t.user_id\") == F.col(\"up.user_id\") , \"left\")\\\n",
    "                    .join(item_live_avg.alias(\"il\"), F.col(\"t.item_id\") == F.col(\"il.item_id\") , \"left\")\\\n",
    "                    .join(item_pvr_avg.alias(\"ip\"), F.col(\"t.item_id\") == F.col(\"ip.item_id\") , \"left\")\\\n",
    "                    .join(items.alias(\"i\"), F.col(\"t.item_id\") == F.col(\"i.item_id\"), \"left\")\\\n",
    "                    .select(\"t.user_id\", \n",
    "                            \"t.item_id\", \n",
    "                            \"t.purchase\", \n",
    "                            \"tu.user_purchase_avg\",\n",
    "                            \"tu.user_item_id\",\n",
    "                            \"tu.user_purchase_sum\",\n",
    "                            \"ti.item_purchase_avg\",\n",
    "                            \"ti.item_user_id\",\n",
    "                            \"ti.item_purchase_sum\",\n",
    "                            \"ul.user_avg_live\",\n",
    "                            \"ul.user_cnt_live\",\n",
    "                            \"up.user_avg_pvr\",\n",
    "                            \"up.user_cnt_pvr\",\n",
    "                            \"il.item_avg_live\",\n",
    "                            \"ip.item_avg_pvr\",\n",
    "                            \"i.gen_top1\",\n",
    "                            \"i.gen_top2\",\n",
    "                            \"i.gen_top3\",\n",
    "                            \"i.gen_top4\",\n",
    "                            \"i.gen_top5\",\n",
    "                            \"i.gen_top6\",\n",
    "                            \"i.gen_top7\",\n",
    "                            \"i.gen_top8\",\n",
    "                            \"i.gen_top9\",\n",
    "                            \"i.gen_top10\",\n",
    "                            \"i.gen_others\",\n",
    "                            \"i.old_years\",\n",
    "                            \"i.1951-1980\",\n",
    "                            \"i.1981-2000\",\n",
    "                            \"i.2001-2010\",\n",
    "                            \"i.new_years\"\n",
    "                           )\n",
    "    data = data.na.fill(0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = f_merge_features(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_assembler(data):\n",
    "    list_col_features = list(set(data.columns) - set(['user_id', 'item_id', 'purchase']))\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols = list_col_features,\n",
    "        outputCol=\"features\")\n",
    "    output = assembler.transform(data)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_features_ = f_assembler(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- purchase: integer (nullable = true)\n",
      " |-- user_purchase_avg: double (nullable = false)\n",
      " |-- user_item_id: long (nullable = true)\n",
      " |-- user_purchase_sum: long (nullable = true)\n",
      " |-- item_purchase_avg: double (nullable = false)\n",
      " |-- item_user_id: long (nullable = true)\n",
      " |-- item_purchase_sum: long (nullable = true)\n",
      " |-- user_avg_live: double (nullable = false)\n",
      " |-- user_cnt_live: long (nullable = true)\n",
      " |-- user_avg_pvr: double (nullable = false)\n",
      " |-- user_cnt_pvr: long (nullable = true)\n",
      " |-- item_avg_live: double (nullable = false)\n",
      " |-- item_avg_pvr: double (nullable = false)\n",
      " |-- gen_top1: integer (nullable = true)\n",
      " |-- gen_top2: integer (nullable = true)\n",
      " |-- gen_top3: integer (nullable = true)\n",
      " |-- gen_top4: integer (nullable = true)\n",
      " |-- gen_top5: integer (nullable = true)\n",
      " |-- gen_top6: integer (nullable = true)\n",
      " |-- gen_top7: integer (nullable = true)\n",
      " |-- gen_top8: integer (nullable = true)\n",
      " |-- gen_top9: integer (nullable = true)\n",
      " |-- gen_top10: integer (nullable = true)\n",
      " |-- gen_others: integer (nullable = true)\n",
      " |-- old_years: integer (nullable = true)\n",
      " |-- 1951-1980: integer (nullable = true)\n",
      " |-- 1981-2000: integer (nullable = true)\n",
      " |-- 2001-2010: integer (nullable = true)\n",
      " |-- new_years: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train_features_.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_features = df_train_features_.sampleBy(\"purchase\", fractions={0: 0.009, 1: 1}, seed=4242).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|purchase|count|\n",
      "+--------+-----+\n",
      "|       1|10904|\n",
      "|       0|44918|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train_features.groupby('purchase').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_id=844427, item_id=8389, purchase=1, user_purchase_avg=0.0034775888717156105, user_item_id=2588, user_purchase_sum=9, item_purchase_avg=0.005979073243647235, item_user_id=1338, item_purchase_sum=8, user_avg_live=6323.3835616438355, user_cnt_live=73, user_avg_pvr=2585.2105263157896, user_cnt_pvr=76, item_avg_live=0.0, item_avg_pvr=0.0, gen_top1=0, gen_top2=0, gen_top3=0, gen_top4=0, gen_top5=0, gen_top6=0, gen_top7=1, gen_top8=0, gen_top9=0, gen_top10=0, gen_others=0, old_years=0, 1951-1980=0, 1981-2000=1, 2001-2010=0, new_years=0, features=SparseVector(28, {6: 2585.2105, 10: 0.006, 12: 2588.0, 13: 0.0035, 14: 1338.0, 15: 76.0, 17: 8.0, 19: 9.0, 20: 1.0, 22: 73.0, 24: 6323.3836, 25: 1.0}))]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_features.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Тестовая выборка."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`laba03_test.csv`** — тестовый датасет без указанного целевого признака purchase, который вам и предстоит предсказать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- purchase: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = spark.read.csv('/labs/slaba03/laba03_test.csv', header=True)\\\n",
    "             .withColumn(\"user_id\", F.col(\"user_id\").cast(T.IntegerType()))\\\n",
    "             .withColumn(\"item_id\", F.col(\"item_id\").cast(T.IntegerType()))\\\n",
    "             .withColumn(\"purchase\", F.col(\"purchase\").cast(T.IntegerType()))\n",
    "test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+\n",
      "|user_id|item_id|purchase|\n",
      "+-------+-------+--------+\n",
      "|   1654|  94814|    null|\n",
      "+-------+-------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2156840"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|cnt_user|cnt_item|\n",
      "+--------+--------+\n",
      "|    1941|    3704|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.select(F.countDistinct(F.col(\"user_id\")).alias(\"cnt_user\"), \n",
    "             F.countDistinct(F.col(\"item_id\")).alias(\"cnt_item\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = f_merge_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_features = f_assembler(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- purchase: integer (nullable = true)\n",
      " |-- user_purchase_avg: double (nullable = false)\n",
      " |-- user_item_id: long (nullable = true)\n",
      " |-- user_purchase_sum: long (nullable = true)\n",
      " |-- item_purchase_avg: double (nullable = false)\n",
      " |-- item_user_id: long (nullable = true)\n",
      " |-- item_purchase_sum: long (nullable = true)\n",
      " |-- user_avg_live: double (nullable = false)\n",
      " |-- user_cnt_live: long (nullable = true)\n",
      " |-- user_avg_pvr: double (nullable = false)\n",
      " |-- user_cnt_pvr: long (nullable = true)\n",
      " |-- item_avg_live: double (nullable = false)\n",
      " |-- item_avg_pvr: double (nullable = false)\n",
      " |-- gen_top1: integer (nullable = true)\n",
      " |-- gen_top2: integer (nullable = true)\n",
      " |-- gen_top3: integer (nullable = true)\n",
      " |-- gen_top4: integer (nullable = true)\n",
      " |-- gen_top5: integer (nullable = true)\n",
      " |-- gen_top6: integer (nullable = true)\n",
      " |-- gen_top7: integer (nullable = true)\n",
      " |-- gen_top8: integer (nullable = true)\n",
      " |-- gen_top9: integer (nullable = true)\n",
      " |-- gen_top10: integer (nullable = true)\n",
      " |-- gen_others: integer (nullable = true)\n",
      " |-- old_years: integer (nullable = true)\n",
      " |-- 1951-1980: integer (nullable = true)\n",
      " |-- 1981-2000: integer (nullable = true)\n",
      " |-- 2001-2010: integer (nullable = true)\n",
      " |-- new_years: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test_features.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_id=822709, item_id=8389, purchase=0, user_purchase_avg=0.00037893141341417203, user_item_id=2639, user_purchase_sum=1, item_purchase_avg=0.005979073243647235, item_user_id=1338, item_purchase_sum=8, user_avg_live=4426.556541019956, user_cnt_live=451, user_avg_pvr=2052.8568019093077, user_cnt_pvr=419, item_avg_live=0.0, item_avg_pvr=0.0, gen_top1=0, gen_top2=0, gen_top3=0, gen_top4=0, gen_top5=0, gen_top6=0, gen_top7=1, gen_top8=0, gen_top9=0, gen_top10=0, gen_others=0, old_years=0, 1951-1980=0, 1981-2000=1, 2001-2010=0, new_years=0, features=SparseVector(28, {6: 2052.8568, 10: 0.006, 12: 2639.0, 13: 0.0004, 14: 1338.0, 15: 419.0, 17: 8.0, 19: 1.0, 20: 1.0, 22: 451.0, 24: 4426.5565, 25: 1.0}))]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_features.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Моделька."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    featuresCol='features', \n",
    "    labelCol='purchase',\n",
    "    numTrees=500,\n",
    "    maxDepth=20,\n",
    "    maxBins=40,\n",
    "#     featureSubsetStrategy = 'all' \n",
    ")\n",
    "\n",
    "# from pyspark.ml.regression import GBTRegressor\n",
    "# from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "# from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# model = GBTRegressor(maxIter=50,\n",
    "#                         subsamplingRate=1.0,\n",
    "#                         maxDepth=9,\n",
    "#                         featuresCol=\"features\",\n",
    "#                         labelCol=\"purchase\")\n",
    "\n",
    "model.fit(df_train_features).save('RF_lab3_1.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassificationModel.load('RF_lab3_1.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability = model.transform(df_test_features)\\\n",
    ".select(\n",
    "    F.col('user_id'),\n",
    "    F.col('item_id'),\n",
    "    F.col('rawPrediction'),\n",
    "    F.col('prediction'),\n",
    "    F.col('probability')\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Запись результата."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = probability.withColumn(\"probability\", F.col(\"probability\").cast(T.StringType()))\\\n",
    "                    .select(F.col(\"user_id\"),\n",
    "                            F.col(\"item_id\"),\n",
    "                            F.col(\"probability\"))\n",
    "result = result.sort(F.col(\"user_id\").asc(), F.col(\"item_id\").asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = result.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['user_id'] = df['user_id'].astype(int)\n",
    "df['item_id'] = df['item_id'].astype(int)\n",
    "df['purchase'] = [x[1:-1].split(',')[1] for x in df['probability']]\n",
    "df['purchase'] = df['purchase'].astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['user_id','item_id','purchase']].to_csv('lab03.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Потолок 0,785 и при повышении параметров обучение падает, либо слишком долго отрабатывает - такой вариант не подходит. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- purchase: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- purchase: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.8 ms, sys: 300 µs, total: 21.1 ms\n",
      "Wall time: 43.1 s\n"
     ]
    }
   ],
   "source": [
    "als = ALS(maxIter=20, regParam=2.2, rank=6, coldStartStrategy=\"nan\",\n",
    "          userCol='user_id', itemCol='item_id', ratingCol='purchase',\n",
    "          nonnegative=False, implicitPrefs=True, alpha=5.0, seed=87)\n",
    "%time als_model = als.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-------------+\n",
      "|user_id|item_id|purchase|   prediction|\n",
      "+-------+-------+--------+-------------+\n",
      "| 746713|   8389|       0|          0.0|\n",
      "| 883098|   8389|       0|-0.0031953193|\n",
      "| 903491|   8389|       0| 0.0056430437|\n",
      "| 903826|   8389|       0|   0.03771403|\n",
      "| 916566|   8389|       0|3.6348827E-18|\n",
      "+-------+-------+--------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 3.95 ms, sys: 461 µs, total: 4.41 ms\n",
      "Wall time: 21.4 s\n"
     ]
    }
   ],
   "source": [
    "pred_train = als_model.transform(train)\n",
    "%time pred_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_train = pred_train.withColumn(\"prediction\", F.col(\"prediction\").cast(T.DoubleType()))\n",
    "pred_train = pred_train.coalesce(5).cache()\n",
    "pred_train.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",\n",
    "                                          labelCol=\"purchase\", \n",
    "                                          metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9685476387347012"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-------------+\n",
      "|user_id|item_id|purchase|   prediction|\n",
      "+-------+-------+--------+-------------+\n",
      "| 822709|   8389|    null|3.1188553E-19|\n",
      "| 824008|   8389|    null|-0.0017191223|\n",
      "| 890476|   8389|    null|          0.0|\n",
      "| 899993|   8389|    null|  8.513293E-4|\n",
      "| 937345|   8389|    null|  0.032060243|\n",
      "+-------+-------+--------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 4.54 ms, sys: 351 µs, total: 4.9 ms\n",
      "Wall time: 19.9 s\n"
     ]
    }
   ],
   "source": [
    "pred_test = als_model.transform(test)\n",
    "%time pred_test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test = pred_test.coalesce(5).cache()\n",
    "pred_test = pred_test.withColumn(\"prediction\", F.col(\"prediction\").cast(T.DoubleType()))\n",
    "pred_test.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pred_test.select(F.col(\"user_id\"),\n",
    "                             F.col(\"item_id\"),\n",
    "                             F.col(\"prediction\").alias(\"purchase\"))\\\n",
    "                     .sort(F.col(\"user_id\").asc(),\n",
    "                           F.col(\"item_id\").asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.toPandas().to_csv(\"lab03.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
