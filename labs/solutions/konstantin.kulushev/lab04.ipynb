{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 2 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import Row\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, StandardScaler, OneHotEncoder, RegexTokenizer, VectorAssembler, IndexToString, StringIndexer, CountVectorizer, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.classification import GBTClassifier, LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "import json\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .config(conf=conf)\n",
    "         .appName(\"test\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARTITION_NUM = 4\n",
    "KAFKA_CONNECT = 'spark-master-1.newprolab.com:6667'\n",
    "INPUT_TOPIC = 'input_konstantin.kulushev'\n",
    "OUTPUT_TOPIC = 'konstantin.kulushev'\n",
    "\n",
    "read_kafka_params = {\n",
    "    \"kafka.bootstrap.servers\": KAFKA_CONNECT,\n",
    "    \"subscribe\": INPUT_TOPIC,\n",
    "    \"startingOffsets\": \"latest\",\n",
    "    \"failOnDataLoss\": False,\n",
    "}\n",
    "\n",
    "write_kafka_params = {\n",
    "   \"kafka.bootstrap.servers\": KAFKA_CONNECT,\n",
    "   \"topic\": OUTPUT_TOPIC,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = ArrayType(StringType())\n",
    "pattern = r'\\/\\/[^@\\/\\n]+(?:www\\.)?(?:[^:\\/\\n]+)'\n",
    "\n",
    "train = spark.read.csv('/labs/slaba04/gender_age_dataset.txt', header=True, sep='\\t') \\\n",
    "        .filter(F.col('gender') != '-')\n",
    "        #.withColumn('visits', F.get_json_object(F.col('user_json'), '$.visits')) \\\n",
    "        #.drop('user_json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasOutputCol, HasInputCol, HasOutputCols\n",
    "# self.getInputCol()\n",
    "\n",
    "class CountArrayLenTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(CountArrayLenTransformer, self).__init__()\n",
    "        if inputCol is not None:\n",
    "            self.setInputCol(inputCol)\n",
    "        if outputCol is not None:\n",
    "            self.setOutputCol(outputCol)\n",
    "            \n",
    "    def _transform(self, dataset):\n",
    "        return dataset.withColumn(self.getOutputCol(), F.size(F.col(self.getInputCol())))\n",
    "    \n",
    "class SearchFreqTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "    search_engines = ['yandex', 'google', 'yahoo']\n",
    "    site = ['http://www.pornhub.com', 'http://www.xvideos.com', 'http://ruxvideos.ru']\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(SearchFreqTransformer, self).__init__()\n",
    "        if inputCol is not None:\n",
    "            self.setInputCol(inputCol)\n",
    "        if outputCol is not None:\n",
    "            self.setOutputCol(outputCol)\n",
    "            \n",
    "    def _transform(self, dataset):\n",
    "        return dataset.withColumn(self.getOutputCol(), F.when(F.array_contains(self.getInputCol(), 'http://www.pornhub.com'), 1).otherwise(0))\n",
    "\n",
    "    \n",
    "class CutterArrayLenTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(CutterArrayLenTransformer, self).__init__()\n",
    "        if inputCol is not None:\n",
    "            self.setInputCol(inputCol)\n",
    "        if outputCol is not None:\n",
    "            self.setOutputCol(outputCol)\n",
    "            \n",
    "    def _transform(self, dataset):\n",
    "        return dataset \\\n",
    "            .withColumn('idx', F.monotonically_increasing_id()) \\\n",
    "            .withColumn('exploded_col', F.explode(col(self.getInputCol()))) \\\n",
    "            .withColumn('substr_col', F.substring(col('exploded_col'),2,100)) \\\n",
    "            .groupBy(col('idx')) \\\n",
    "            .agg(F.collect_list('substr_col').alias('new_column'))\n",
    "            .withColumn(self.getOutputCol(), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_tokenizer = RegexTokenizer(inputCol='user_json', outputCol='domains', gaps=False, pattern=pattern)\n",
    "tf = HashingTF(inputCol=regex_tokenizer.getOutputCol(), outputCol='tf_features', numFeatures=15000)\n",
    "idf = IDF(inputCol=tf.getOutputCol(), outputCol='features')\n",
    "\n",
    "mytr1 = CountArrayLenTransformer(inputCol=regex_tokenizer.getOutputCol(), outputCol='sites_cnt')\n",
    "mytr2 = SearchFreqTransformer(inputCol=regex_tokenizer.getOutputCol(), outputCol='porn_sites_cnt')\n",
    "\n",
    "cv = CountVectorizer(inputCol=regex_tokenizer.getOutputCol(), outputCol='cv_features', binary=False)\n",
    "#scaler = StandardScaler(inputCol=cv.getOutputCol(), outputCol='scaled_features', withStd=True, withMean=True)\n",
    "\n",
    "indexer_age = StringIndexer(inputCol=\"age\", outputCol=\"age_label\")\n",
    "indexer_gender = StringIndexer(inputCol=\"gender\", outputCol=\"gender_label\")\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['cv_features'], outputCol='features')\n",
    "\n",
    "lr_age = LogisticRegression(labelCol=indexer_age.getOutputCol(),\n",
    "                            featuresCol='features',\n",
    "                            predictionCol='pred_age',\n",
    "                            probabilityCol='prob_age',\n",
    "                            rawPredictionCol='rawPred_age',\n",
    "                            regParam=0.4,\n",
    "                            elasticNetParam=0.1)\n",
    "lr_gender = LogisticRegression(labelCol=indexer_gender.getOutputCol(),\n",
    "                               featuresCol='features',\n",
    "                               predictionCol='pred_gender',\n",
    "                               probabilityCol='prob_gender',\n",
    "                               rawPredictionCol='rawPred_gender',\n",
    "                               regParam=0.2,\n",
    "                               elasticNetParam=0.1)\n",
    "\n",
    "gbt = GBTClassifier(labelCol=indexer_gender.getOutputCol(), featuresCol=\"features\", maxDepth=7)\n",
    "\n",
    "\n",
    "rfc_age = RandomForestClassifier(labelCol=indexer_age.getOutputCol(),\n",
    "                            featuresCol='features',\n",
    "                            predictionCol='pred_age',\n",
    "                            probabilityCol='prob_age',\n",
    "                            rawPredictionCol='rawPred_age', maxDepth=7)\n",
    "\n",
    "rfc_gender = RandomForestClassifier(labelCol=indexer_gender.getOutputCol(),\n",
    "                            featuresCol='features',\n",
    "                            predictionCol='pred_gender',\n",
    "                            probabilityCol='prob_gender',\n",
    "                            rawPredictionCol='rawPred_gender', maxDepth=7)\n",
    "\n",
    "converter_age = IndexToString(inputCol=rfc_age.getPredictionCol(), outputCol='age_pred', labels=['18-24', '25-34', '35-44', '45-54', '>=55'])\n",
    "converter_gender = IndexToString(inputCol=rfc_gender.getPredictionCol(), outputCol='gender_pred', labels=['F', 'M'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_prepare = Pipeline(stages=[regex_tokenizer, tf, idf])\n",
    "pipe_index = Pipeline(stages=[indexer_age, indexer_gender])\n",
    "pipe_age = Pipeline(stages=[rfc_age, converter_age])\n",
    "pipe_gender = Pipeline(stages=[rfc_gender, converter_gender])\n",
    "pipeline = Pipeline(stages=[pipe_prepare, pipe_index, pipe_age, pipe_gender])\n",
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_age.write().overwrite().save('models/model_age')\n",
    "#model_gender.write().overwrite().save('models/model_gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreach_batch_function(df, epoch_id):\n",
    "    predictions = model.transform(df)\n",
    "    results = predictions.select(F.col('uid'), F.col('gender_pred').alias('gender'), F.col('age_pred').alias('age')) \\\n",
    "               .withColumn('value', F.to_json(F.struct(['uid', 'gender', 'age'])))\n",
    "    \n",
    "    return results.select('value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f44f8ff1908>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_sdf = spark.readStream.format(\"kafka\").options(**read_kafka_params).load()\n",
    "\n",
    "#test = spark.read.format('kafka').options(**read_kafka_params).load()\n",
    "test = kafka_sdf.select(F.col('value').cast('string').alias('user_json')) \\\n",
    "           .withColumn('uid', F.get_json_object(F.col('user_json'), '$.uid')) \\\n",
    "\n",
    "predictions = model.transform(test)\n",
    "\n",
    "results = predictions.select(F.col('uid'), F.col('gender_pred').alias('gender'), F.col('age_pred').alias('age')) \\\n",
    "               .withColumn('value', F.to_json(F.struct(['uid', 'gender', 'age'])))\n",
    "\n",
    "results.select('value') \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .options(**write_kafka_params) \\\n",
    "    .option(\"checkpointLocation\", \"kkv/chk_lab04\")\\\n",
    "    .outputMode(\"append\").start()#.awaitTermination()\n",
    "\n",
    "# test.writeStream \\\n",
    "#     .foreachBatch(foreach_batch_function) \\\n",
    "#     .format(\"kafka\") \\\n",
    "#     .options(**write_kafka_params) \\\n",
    "#     .option(\"checkpointLocation\", \"kkv/chk_lab04\") \\\n",
    "#     .outputMode(\"append\").start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SparkSession.builder.getOrCreate().streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped KafkaV2[Subscribe[input_konstantin.kulushev]]\n"
     ]
    }
   ],
   "source": [
    "def kill_all():\n",
    "    streams = SparkSession.builder.getOrCreate().streams.active\n",
    "    if streams:\n",
    "        for s in streams:\n",
    "            desc = s.lastProgress[\"sources\"][0][\"description\"]\n",
    "            s.stop()\n",
    "            print(\"Stopped {s}\".format(s=desc))\n",
    "            \n",
    "kill_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
