{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 2 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import Row\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, RegexTokenizer, StopWordsRemover\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "\n",
    "import json\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .config(conf=conf)\n",
    "         .appName(\"test\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_courses = [[23126, u'en', u'Compass - powerful SASS library that makes your life easier'],\n",
    "                [21617, u'en', u'Preparing for the AP* Computer Science A Exam \\u2014 Part 2'],\n",
    "                [16627, u'es', u'Aprende Excel: Nivel Intermedio by Alfonso Rinsche'],\n",
    "                [11556, u'es', u'Aprendizaje Colaborativo by UNID Universidad Interamericana para el Desarrollo'],\n",
    "                [16704, u'ru', u'\\u041f\\u0440\\u043e\\u0433\\u0440\\u0430\\u043c\\u043c\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u043d\\u0430 Lazarus'],\n",
    "                [13702, u'ru', u'\\u041c\\u0430\\u0442\\u0435\\u043c\\u0430\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u0430\\u044f \\u044d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u043a\\u0430']]\n",
    "\n",
    "test_ids = [item[0] for item in test_courses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "|                 cat|                desc| id|lang|                name|      provider|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "|3/business_manage...|This course intro...|  4|  en|Accounting Cycle:...|Canvas Network|\n",
      "|              11/law|This online cours...|  5|  en|American Counter ...|Canvas Network|\n",
      "|5/computer_scienc...|This course is ta...|  6|  fr|Arithmétique: en ...|Canvas Network|\n",
      "|  14/social_sciences|We live in a digi...|  7|  en|Becoming a Dynami...|Canvas Network|\n",
      "|2/biology_life_sc...|This self-paced c...|  8|  en|           Bioethics|Canvas Network|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.json('/labs/slaba02/DO_record_per_line.json')\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(cat='15/mathematics_statistics_and_data_analysis', desc='\\n\\t  \\t     \\n\\t  \\t  ', id=7527, lang='en', name='Writing in the Sciences', provider='Harvard Extension School')]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.filter(data.id == 7527).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# паттерн поиска токенов\n",
    "pattern = r\"[a-zA-Zа-яА-Яё0-9]{2,}\"\n",
    "\n",
    "# трансформеры и эстиматоры\n",
    "regexTokenizer = RegexTokenizer(inputCol='desc', outputCol='desc_tokens', gaps=False, pattern=pattern)\n",
    "remover = StopWordsRemover(inputCol=regexTokenizer.getOutputCol(), outputCol='desc_filtered')\n",
    "tf = HashingTF(inputCol=remover.getOutputCol(), outputCol='raw_features', numFeatures=10000)\n",
    "idf = IDF(inputCol=tf.getOutputCol(), outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# определяем список стоп-слов для удаления из описаний курсов\n",
    "eng_stopwords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "rus_stopwords = StopWordsRemover.loadDefaultStopWords(\"russian\")\n",
    "esp_stopwords = StopWordsRemover.loadDefaultStopWords(\"spanish\")\n",
    "\n",
    "stopwords = {\n",
    "    'en': eng_stopwords,\n",
    "    'es': esp_stopwords,\n",
    "    'ru': rus_stopwords,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "#\n",
    "# выбираем курсы на языках из задания, отфильтруем описания, содержащие только пробелы или спец.символы\n",
    "# разбиваем тексты на токены\n",
    "processed_data = data.filter(data.lang.isin(['en', 'es', 'ru']))\\\n",
    "                     .select(data.id, data.lang, data.name, data.desc)\n",
    "processed_data = processed_data.withColumn('desc', F.regexp_replace('desc', '\\\\n|\\\\r|\\\\t', ''))\\\n",
    "                               .withColumn('desc', F.regexp_replace('desc', '^\\s+$', ''))\\\n",
    "                               .filter(\"desc != ''\")\n",
    "processed_data = regexTokenizer.transform(processed_data)\n",
    "\n",
    "# удаляем стоп-слова - для каждого языка используем свой набор данных и стоп-слов\n",
    "rm_stopwords_temp = spark.createDataFrame([], processed_data.schema.add(StructField(\"desc_filtered\", ArrayType(StringType()), True)))\n",
    "for key, value in stopwords.items():\n",
    "    remover.setInputCol(regexTokenizer.getOutputCol())\n",
    "    remover.setOutputCol('desc_filtered')\n",
    "    remover.setStopWords(value)\n",
    "    temp = remover.transform(processed_data.filter(F.col('lang') == key))\n",
    "    rm_stopwords_temp = rm_stopwords_temp.union(temp)\n",
    "    \n",
    "processed_data = rm_stopwords_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# считаем tf-idf\n",
    "featurizedData = tf.transform(processed_data)\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(id=4, lang='en', name='Accounting Cycle: The Foundation of Business Measurement and Reporting', desc='This course introduces the basic financial statements used by most businesses, as well as the essential tools used to prepare them. This course will serve as a resource to help business students succeed in their upcoming university-level accounting classes, and as a refresher for upper division accounting students who are struggling to recall elementary concepts essential to more advanced accounting topics. Business owners will also benefit from this class by gaining essential skills necessary to organize and manage information pertinent to operating their business. At the conclusion of the class, students will understand the balance sheet, income statement, and cash flow statement. They will be able to differentiate between cash basis and accrual basis techniques, and know when each is appropriate. They’ll also understand the accounting equation, how to journalize and post transactions, how to adjust and close accounts, and how to prepare key financial reports. All material for this class is written and delivered by the professor, and can be previewed here. Students must have access to a spreadsheet program to participate.', desc_tokens=['this', 'course', 'introduces', 'the', 'basic', 'financial', 'statements', 'used', 'by', 'most', 'businesses', 'as', 'well', 'as', 'the', 'essential', 'tools', 'used', 'to', 'prepare', 'them', 'this', 'course', 'will', 'serve', 'as', 'resource', 'to', 'help', 'business', 'students', 'succeed', 'in', 'their', 'upcoming', 'university', 'level', 'accounting', 'classes', 'and', 'as', 'refresher', 'for', 'upper', 'division', 'accounting', 'students', 'who', 'are', 'struggling', 'to', 'recall', 'elementary', 'concepts', 'essential', 'to', 'more', 'advanced', 'accounting', 'topics', 'business', 'owners', 'will', 'also', 'benefit', 'from', 'this', 'class', 'by', 'gaining', 'essential', 'skills', 'necessary', 'to', 'organize', 'and', 'manage', 'information', 'pertinent', 'to', 'operating', 'their', 'business', 'at', 'the', 'conclusion', 'of', 'the', 'class', 'students', 'will', 'understand', 'the', 'balance', 'sheet', 'income', 'statement', 'and', 'cash', 'flow', 'statement', 'they', 'will', 'be', 'able', 'to', 'differentiate', 'between', 'cash', 'basis', 'and', 'accrual', 'basis', 'techniques', 'and', 'know', 'when', 'each', 'is', 'appropriate', 'they', 'll', 'also', 'understand', 'the', 'accounting', 'equation', 'how', 'to', 'journalize', 'and', 'post', 'transactions', 'how', 'to', 'adjust', 'and', 'close', 'accounts', 'and', 'how', 'to', 'prepare', 'key', 'financial', 'reports', 'all', 'material', 'for', 'this', 'class', 'is', 'written', 'and', 'delivered', 'by', 'the', 'professor', 'and', 'can', 'be', 'previewed', 'here', 'students', 'must', 'have', 'access', 'to', 'spreadsheet', 'program', 'to', 'participate'], desc_filtered=['course', 'introduces', 'basic', 'financial', 'statements', 'used', 'businesses', 'well', 'essential', 'tools', 'used', 'prepare', 'course', 'serve', 'resource', 'help', 'business', 'students', 'succeed', 'upcoming', 'university', 'level', 'accounting', 'classes', 'refresher', 'upper', 'division', 'accounting', 'students', 'struggling', 'recall', 'elementary', 'concepts', 'essential', 'advanced', 'accounting', 'topics', 'business', 'owners', 'also', 'benefit', 'class', 'gaining', 'essential', 'skills', 'necessary', 'organize', 'manage', 'information', 'pertinent', 'operating', 'business', 'conclusion', 'class', 'students', 'understand', 'balance', 'sheet', 'income', 'statement', 'cash', 'flow', 'statement', 'able', 'differentiate', 'cash', 'basis', 'accrual', 'basis', 'techniques', 'know', 'appropriate', 'll', 'also', 'understand', 'accounting', 'equation', 'journalize', 'post', 'transactions', 'adjust', 'close', 'accounts', 'prepare', 'key', 'financial', 'reports', 'material', 'class', 'written', 'delivered', 'professor', 'previewed', 'students', 'must', 'access', 'spreadsheet', 'program', 'participate'], raw_features=SparseVector(10000, {36: 1.0, 63: 1.0, 138: 1.0, 157: 1.0, 177: 1.0, 362: 1.0, 534: 1.0, 603: 1.0, 646: 1.0, 1023: 1.0, 1072: 1.0, 1355: 1.0, 1390: 1.0, 1446: 1.0, 1501: 1.0, 1523: 4.0, 1670: 1.0, 1697: 1.0, 2015: 3.0, 2092: 4.0, 2460: 2.0, 2523: 1.0, 2577: 1.0, 2757: 1.0, 3231: 1.0, 3251: 1.0, 3496: 1.0, 3792: 2.0, 3834: 1.0, 3849: 2.0, 3903: 1.0, 4224: 1.0, 4364: 1.0, 4436: 1.0, 4742: 1.0, 4978: 1.0, 5189: 1.0, 5374: 3.0, 5412: 1.0, 6158: 1.0, 6236: 1.0, 6395: 1.0, 6470: 1.0, 6511: 1.0, 6541: 1.0, 6642: 1.0, 6697: 1.0, 6863: 2.0, 7008: 2.0, 7270: 1.0, 7282: 2.0, 7290: 1.0, 7298: 1.0, 7735: 1.0, 7772: 1.0, 7779: 1.0, 7895: 1.0, 7936: 1.0, 7956: 2.0, 7973: 1.0, 8041: 1.0, 8140: 1.0, 8164: 2.0, 8234: 2.0, 8370: 1.0, 8534: 3.0, 8579: 1.0, 8624: 1.0, 8644: 1.0, 9328: 1.0, 9347: 1.0, 9540: 1.0, 9605: 2.0, 9695: 1.0, 9809: 1.0, 9953: 1.0, 9970: 1.0}), features=SparseVector(10000, {36: 3.8379, 63: 3.4352, 138: 3.9328, 157: 1.533, 177: 3.3209, 362: 3.441, 534: 1.74, 603: 6.1664, 646: 3.5644, 1023: 2.8174, 1072: 1.5293, 1355: 2.3118, 1390: 4.2746, 1446: 2.3366, 1501: 2.3605, 1523: 5.5415, 1670: 3.3087, 1697: 1.514, 2015: 8.7842, 2092: 15.2836, 2460: 0.8043, 2523: 4.543, 2577: 3.456, 2757: 4.9263, 3231: 2.3264, 3251: 3.734, 3496: 1.8063, 3792: 2.7986, 3834: 1.9023, 3849: 4.157, 3903: 3.0564, 4224: 2.1588, 4364: 4.5535, 4436: 3.7464, 4742: 3.8327, 4978: 2.0172, 5189: 1.3647, 5374: 5.0357, 5412: 2.9986, 6158: 3.3894, 6236: 2.0686, 6395: 2.7542, 6470: 3.7052, 6511: 1.7872, 6541: 3.1404, 6642: 4.8624, 6697: 2.8532, 6863: 6.2945, 7008: 6.1959, 7270: 4.0589, 7282: 6.0573, 7290: 4.8719, 7298: 3.7218, 7735: 3.7187, 7772: 3.8744, 7779: 1.4537, 7895: 3.7325, 7936: 3.4037, 7956: 6.8773, 7973: 3.2247, 8041: 3.7294, 8140: 4.8296, 8164: 3.2214, 8234: 6.3519, 8370: 5.3812, 8534: 6.3407, 8579: 2.1382, 8624: 3.1576, 8644: 2.3208, 9328: 3.6656, 9347: 1.4749, 9540: 3.5854, 9605: 4.0079, 9695: 3.831, 9809: 3.3829, 9953: 5.0562, 9970: 1.7787}))"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# вектор features хранит значения tf-idf для слов\n",
    "rescaledData.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция расчета косинусного сходства\n",
    "def get_cosine_similarity(v1, v2):\n",
    "    return float(v1.dot(v2) / (v1.norm(2) * v2.norm(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# словарь результатов\n",
    "recommendations = {}\n",
    "\n",
    "# схема временного датафрейма для хранения id курса и косинусного расстояния\n",
    "schema = StructType([\n",
    "  StructField('id', IntegerType(), True), # course id\n",
    "  StructField('cs', FloatType(), True) # cosine similarity\n",
    "])\n",
    "\n",
    "# для каждого курса из задания\n",
    "for course in test_courses:\n",
    "    id_, lang, desc = course\n",
    "    dists = []\n",
    "    \n",
    "    # находим tf-idf вектор, берем все курсы необходимого языка\n",
    "    course_vec = rescaledData.filter(rescaledData.id == id_).first().features\n",
    "    courses_to_search = rescaledData.filter(rescaledData.lang == lang).repartition(4)\n",
    "    \n",
    "    # считаем косинусные расстояния курса со всеми остальными курсами (плохо использовать toLocalIterator в случае очень больших партиций)\n",
    "    for row in courses_to_search.rdd.toLocalIterator():\n",
    "        dists.append((row.id, get_cosine_similarity(course_vec, row.features)))\n",
    "    \n",
    "    # делаем временный датафрейм с косинусами\n",
    "    course_dists = spark.createDataFrame(dists, schema).repartition(4)\n",
    "    # джойним по id\n",
    "    courses_to_search = courses_to_search.join(course_dists, on='id', how='inner').repartition(4)\n",
    "    # сортируем согласно заданию, берем топ 11 (т.к. топ 1 - сам курс, для которого ищем рекомендации)\n",
    "    result_for_course = courses_to_search.orderBy(['cs', 'name', 'id'], ascending=[0, 1, 1]).limit(11).collect()\n",
    "    \n",
    "    # собираем id рекомендованных курсов (выкинули первый курс)\n",
    "    recommendations[id_] = [row.id for row in result_for_course[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{23126: [13665, 14760, 13782, 20638, 24419, 15909, 2724, 25782, 17499, 19270],\n",
       " 21617: [21617, 21616, 22298, 21608, 21081, 21630, 21628, 19417, 21623, 21508],\n",
       " 16627: [11431, 17964, 12247, 5687, 16694, 12660, 5558, 17961, 11575, 9563],\n",
       " 11556: [16488, 468, 19330, 22710, 13461, 10447, 21707, 23357, 19279, 10384],\n",
       " 16704: [1236, 1247, 1365, 1164, 1273, 20288, 8186, 1233, 8203, 18331],\n",
       " 13702: [13702, 28074, 21079, 1041, 8300, 13057, 8313, 21025, 1033, 1111]}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пишем результат\n",
    "with open('lab02.json', 'w') as f:\n",
    "    json.dump(recommendations, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вопрос: как можно передать один вектор в строковом представлении в качестве аргумента, чтобы использовать UDF, а именно преобразовать строку в вектор SparseVector.parse(v1), и вычислить косинусное расстояние"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=FloatType())\n",
    "def get_cosine_similarity(v1, v2):\n",
    "    #temp_v = SparseVector.parse(v1)\n",
    "    return float(v1.dot(v2) / (v1.norm(2) * v2.norm(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.withColumn('cs', get_cosine_similarity(F.col('temp'), F.col('features'))).show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(10000, {87: 2.6541, 246: 4.0504, 258: 3.5174, 263: 16.527, 341: 5.7166, 419: 2.4797, 524: 2.1249, 721: 0.9962, 727: 2.116, 814: 4.9313, 870: 2.6432, 937: 2.4997, 1022: 2.0174, 1072: 3.0586, 1073: 5.6459, 1169: 2.5727, 1197: 3.5644, 1218: 5.6451, 1272: 7.6908, 1312: 0.7891, 1368: 1.9217, 1443: 3.077, 1463: 16.444, 1470: 48.7977, 1645: 3.6414, 1652: 3.1637, 1682: 3.4965, 1770: 3.3281, 1851: 3.009, 1882: 2.6432, 1959: 0.6588, 1990: 3.6015, 2080: 0.8154, 2412: 2.5418, 2460: 4.8259, 2587: 6.0241, 2691: 4.2987, 2801: 2.2498, 2971: 5.0504, 3102: 2.9043, 3115: 2.7285, 3145: 2.6116, 3154: 8.7203, 3162: 3.015, 3202: 2.7623, 3330: 1.649, 3434: 3.981, 3444: 3.7052, 3491: 5.3653, 3525: 2.6804, 3624: 9.9838, 3721: 3.0858, 3757: 6.0121, 3767: 3.0484, 3772: 3.1698, 3783: 2.2084, 3849: 2.0785, 3855: 0.7397, 3916: 4.733, 4061: 4.5927, 4115: 3.0143, 4260: 5.7832, 4394: 5.9564, 4422: 3.9969, 4489: 5.7115, 4520: 5.8993, 4712: 4.4165, 4762: 3.7496, 4805: 26.6333, 4858: 3.0218, 4888: 5.6769, 4904: 2.5137, 4962: 4.2772, 5141: 3.1973, 5188: 1.8571, 5242: 2.2058, 5318: 3.7052, 5429: 3.4239, 5446: 3.0666, 5548: 5.0221, 5594: 2.6076, 5612: 0.82, 5685: 3.8516, 5723: 2.6096, 5737: 2.5029, 5786: 0.7135, 6006: 1.2292, 6022: 2.8843, 6114: 3.176, 6242: 14.5728, 6262: 2.8693, 6265: 4.0469, 6311: 10.9999, 6439: 3.4037, 6537: 1.3106, 6549: 4.9624, 6595: 18.6728, 6608: 15.3288, 6668: 9.2519, 6688: 3.0548, 6714: 3.2692, 6750: 5.9468, 6835: 3.9385, 6841: 5.9205, 6842: 3.3743, 6894: 6.1151, 6969: 4.8816, 7013: 5.3071, 7038: 3.8656, 7255: 3.7543, 7318: 6.3593, 7486: 1.6515, 7510: 1.9207, 7512: 4.3373, 7515: 4.3355, 7522: 4.8458, 7712: 1.5639, 7758: 4.5124, 7779: 11.6298, 7922: 11.9846, 8002: 4.2433, 8028: 4.3206, 8057: 2.6201, 8113: 2.71, 8157: 8.1459, 8159: 7.2083, 8261: 5.3116, 8350: 53.0419, 8429: 2.4435, 8433: 3.0401, 8454: 4.7082, 8464: 3.4905, 8493: 0.7837, 8573: 5.7097, 8598: 11.8554, 8599: 8.7637, 8696: 1.6291, 8754: 6.0198, 8877: 5.6662, 8897: 2.4109, 8993: 2.6171, 9060: 111.109, 9092: 4.5465, 9146: 2.8745, 9244: 2.6288, 9263: 2.8557, 9327: 1.4131, 9347: 1.4749, 9352: 4.1933, 9533: 2.8162, 9594: 2.7234, 9605: 2.004, 9638: 3.7433, 9644: 3.748, 9688: 3.2482})"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SparseVector.parse(str(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
