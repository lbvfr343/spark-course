{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 8 --executor-memory 5g --executor-cores 4 --driver-memory 3g pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import Row\n",
    "from pyspark.sql.functions import split, col, explode, regexp_replace, round,when, collect_list, sum, count, array, row_number, avg\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import json\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .config(conf=conf)\n",
    "         .appName(\"test\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"hdfs:///labs/slaba04/gender_age_dataset.txt\"\n",
    "output_dir = \"lab04_train/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh = StructType() \\\n",
    "      .add(\"timestamp\",LongType(),True) \\\n",
    "      .add(\"url\",StringType(),True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "schema1 = StructType().add(\"visits\", ArrayType(sh),True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_Kafka = StructType().add(\"uid\",StringType(),True).add(\"visits\", ArrayType(sh),True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(uid,StringType,true),StructField(visits,ArrayType(StructType(List(StructField(timestamp,LongType,true),StructField(url,StringType,true))),true),true)))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "schema = StructType() \\\n",
    "      .add(\"gender\",StringType(),True) \\\n",
    "      .add(\"age\",StringType(),True) \\\n",
    "      .add(\"uid\",StringType(),True) \\\n",
    "      .add(\"user_json\",StringType(),True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "df = spark.read.options(delimiter='\\t').options(header = True).schema(schema).csv(input_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.withColumn(\"visits\",F.json_tuple(\"user_json\",\"visits\")).withColumn(\"visits\",F.from_json(\"visits\",ArrayType(sh)))\n",
    "#.select(\"gender\",\"age\",\"uid\",F.expr(\"TRIM(BOTH '[]' FROM visits)\").alias(\"visits\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.select(\"gender\",\"age\",\"uid\",explode(\"visits\").alias(\"visits\")) \\\n",
    "         .select(\"gender\",\"age\",\"uid\",col(\"visits\")[\"url\"].alias(\"src0\")) \\\n",
    "         .select(\"gender\",\"age\",\"uid\",regexp_replace(regexp_replace(\"src0\", \"%3A%2F%2F\",\"://\"), \"3A\",\":\").alias(\"src1\")) \\\n",
    "         .select(\"gender\",\"age\",\"uid\",regexp_replace(\"src1\", \"((http)|(https))(://)\",\"\").alias(\"src2\")) \\\n",
    "         .select(\"gender\",\"age\",\"uid\",regexp_replace(\"src2\", \"^www.\",\"\").alias(\"url_\")) \\\n",
    "         .select(\"gender\",\"age\",\"uid\",F.trim(split(\"url_\", \"%2\")[0]).alias(\"url\")) \\\n",
    "         .select(\"gender\",\"age\",\"uid\",split(\"url\", \"/\")[0].alias(\"domain\")) \\\n",
    "         .select(\"uid\", F.concat_ws(\":\",\"gender\", \"age\").alias(\"gender_age\") , \"domain\") \\\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"domains\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, IndexToString\n",
    "st = StringIndexer(inputCol=\"gender_age\", outputCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(numTrees=500, maxDepth=10, subsamplingRate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[cv,st,rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = df3.groupBy(\"uid\", \"gender_age\").agg(collect_list(\"domain\").alias(\"domains\")).filter(col(\"gender_age\") != \"-:-\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_kafka_params = {\n",
    "    \"kafka.bootstrap.servers\": 'spark-master-1.newprolab.com:6667',\n",
    "    \"subscribe\": \"input_yuriy.severyukhin\",\n",
    "    \"startingOffsets\": \"latest\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_sdf = spark.readStream.format(\"kafka\").options(**read_kafka_params1).load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_kafka_params = {\n",
    "   \"kafka.bootstrap.servers\": 'spark-master-1.newprolab.com:6667',\n",
    "   \"topic\": \"yuriy.severyukhin\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/26 11:27:49 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-master-1.newprolab.com:8020/user/yuriy.severyukhin/streaming/chk/chk_kafka/commits' to trash at: hdfs://spark-master-1.newprolab.com:8020/user/yuriy.severyukhin/.Trash/Current/user/yuriy.severyukhin/streaming/chk/chk_kafka/commits\n",
      "22/10/26 11:27:49 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-master-1.newprolab.com:8020/user/yuriy.severyukhin/streaming/chk/chk_kafka/metadata' to trash at: hdfs://spark-master-1.newprolab.com:8020/user/yuriy.severyukhin/.Trash/Current/user/yuriy.severyukhin/streaming/chk/chk_kafka/metadata\n",
      "22/10/26 11:27:49 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-master-1.newprolab.com:8020/user/yuriy.severyukhin/streaming/chk/chk_kafka/offsets' to trash at: hdfs://spark-master-1.newprolab.com:8020/user/yuriy.severyukhin/.Trash/Current/user/yuriy.severyukhin/streaming/chk/chk_kafka/offsets\n",
      "22/10/26 11:27:49 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-master-1.newprolab.com:8020/user/yuriy.severyukhin/streaming/chk/chk_kafka/sources' to trash at: hdfs://spark-master-1.newprolab.com:8020/user/yuriy.severyukhin/.Trash/Current/user/yuriy.severyukhin/streaming/chk/chk_kafka/sources\n",
      "22/10/26 11:27:49 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-master-1.newprolab.com:8020/user/yuriy.severyukhin/streaming/chk/chk_kafka/state' to trash at: hdfs://spark-master-1.newprolab.com:8020/user/yuriy.severyukhin/.Trash/Current/user/yuriy.severyukhin/streaming/chk/chk_kafka/state\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -R streaming/chk/chk_kafka/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_stream = kafka_sdf.select(F.json_tuple(col(\"value\").cast(\"string\"),\"uid\",\"visits\")).select(col(\"c0\").alias(\"uid\"),col(\"c1\").alias(\"visits\")).withColumn(\"visits\",F.from_json(\"visits\",ArrayType(sh)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInput = in_stream.select(\"uid\",explode(\"visits\").alias(\"visits\")) \\\n",
    "         .select(\"uid\",col(\"visits\")[\"url\"].alias(\"src0\")) \\\n",
    "         .select(\"uid\",regexp_replace(regexp_replace(\"src0\", \"%3A%2F%2F\",\"://\"), \"3A\",\":\").alias(\"src1\")) \\\n",
    "         .select(\"uid\",regexp_replace(\"src1\", \"((http)|(https))(://)\",\"\").alias(\"src2\")) \\\n",
    "         .select(\"uid\",regexp_replace(\"src2\", \"^www.\",\"\").alias(\"url_\")) \\\n",
    "         .select(\"uid\",F.trim(split(\"url_\", \"%2\")[0]).alias(\"url\")) \\\n",
    "         .select(\"uid\",split(\"url\", \"/\")[0].alias(\"domain\")) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = dfInput.groupBy(\"uid\").agg(collect_list(\"domain\").alias(\"domains\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <object repr() failed>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/hdp/current/spark2-client/python/pyspark/ml/wrapper.py\", line 40, in __del__\n",
      "    if SparkContext._active_spark_context and self._java_obj is not None:\n",
      "AttributeError: 'RandomForestClassifier' object has no attribute '_java_obj'\n"
     ]
    }
   ],
   "source": [
    "predicts = model.transform(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = IndexToString(inputCol=\"prediction\", outputCol=\"gender_age\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIndexerModel = st.fit(training.filter(col(\"gender_age\") != \"-:-\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IndexToString_62108b729d23"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter.setLabels(labelIndexerModel.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ = converter.transform(predicts).select(\"uid\",split(\"gender_age\",\":\").alias(\"ge\")).select(\"uid\",col(\"ge\")[0].alias(\"gender\"), col(\"ge\")[1].alias(\"age\")).distinct().coalesce(1).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "structed = out_ .select(col(\"uid\").alias(\"key\"),F.struct(col(\"uid\"), col(\"age\"),col(\"gender\")).alias(\"s\"))\n",
    "json_ds = structed.withColumn(\"value\", F.to_json(\"s\")).drop(\"s\").drop(\"key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_ds.selectExpr(\"CAST(value AS STRING)\").write.format(\"kafka\").options(**write_kafka_params).save()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sink = json_ds.writeStream.format(\"kafka\").options(**write_kafka_params)\\\n",
    "    .option(\"checkpointLocation\", \"streaming/chk/chk_kafka\")\\\n",
    "    .outputMode(\"update\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sQuery = sink.start()\n",
    "sQuery.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
