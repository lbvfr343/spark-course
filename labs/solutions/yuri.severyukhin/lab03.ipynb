{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 8 --executor-memory 5g --executor-cores 4 --driver-memory 8g pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import Row\n",
    "from pyspark.sql.functions import split, col, explode, regexp_replace, round,when, collect_list, sum, count, array, row_number, avg\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import json\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .config(conf=conf)\n",
    "         .appName(\"test\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = spark.read.parquet(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5032624"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_feature.select(\"user_id\", \"item_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5032624"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загружаем train и test наборы данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "schema1 = StructType() \\\n",
    "      .add(\"user_id\",IntegerType(),True) \\\n",
    "      .add(\"item_id\",IntegerType(),True) \\\n",
    "      .add(\"purchase\",IntegerType(),True) \n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "schema_test = StructType() \\\n",
    "      .add(\"user_id\",IntegerType(),True) \\\n",
    "      .add(\"item_id\",IntegerType(),True) \n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "df = spark.read.options(delimiter=',').options(header = True).schema(schema1).csv(\"/labs/slaba03/laba03_train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_test = spark.read.options(delimiter=',').options(header = True).schema(schema_test).csv(\"/labs/slaba03/laba03_test.csv\")\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обрабатываем таблицу с жанрами (делаем explode)\n",
    "\n",
    "### Исключаем наиболее популярный жанр - \"Зарубежные\" т.к это улучшает метрику модели.\n",
    "### Первоначально была попытка склеить жанры, но она не улучшила метрику"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.options(delimiter='\\t').options(header = True).csv(\"/labs/slaba03/laba03_items.csv\")\n",
    "\n",
    "#df2_ = df2.withColumn(\"genreArray\",explode(split(regexp_replace(regexp_replace(regexp_replace(regexp_replace(regexp_replace(regexp_replace(regexp_replace(regexp_replace(regexp_replace(regexp_replace(regexp_replace(regexp_replace(regexp_replace(regexp_replace(regexp_replace(regexp_replace(regexp_replace(col(\"genres\"),\"Фантастичские\",\"Фантастика\"),\"Триллеры\",\"Триллер\"),\"Спортивные\",\"Спорт\"),\"Семейные\",\"Семейный\"),\"Приключения\",\"Приключение\"),\"Мультфильмы\",\"Мультфильм\"),\"Музыкальные\",\"Музыкальный\"),\"Мелодрамы\",\"Мелодрама\"),\"Короткометражки\",\"Короткометражные\"),\"Комедии\",\"Комедия\"),\"Исторические\",\"Исторический\"),\"Драмы\",\"Драма\"),\"Документальные\",\"Документальный\"),\"Военные\",\"Военный\"),\" сказка\",\"Сказки\"),\"Арт-хаус\",\"Артхаус\"),\"Боевики\",\"Боевик\"),\",\"))).select(\"item_id\",\"genreArray\")\n",
    "# Первоначально была попытка склеить жанры, но она не улучшила метрику\n",
    "\n",
    "\n",
    "df2_ = df2.withColumn(\"genreArray\",explode(split(\"genres\",\",\"))) \\\n",
    "          .filter(col(\"content_type\") == 1) \\\n",
    "          .filter((col(\"genreArray\") != 'Зарубежные')) \\\n",
    "          .select(\"item_id\",\"genreArray\") \\\n",
    "          .fillna(\"жанр не указан\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Соединяем train и test массивы с полученной таблицей с жанрами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df3 = df.join(df2_,df2_.item_id == df.item_id,\"left\") \\\n",
    "        .select(\"user_id\",df.item_id,\"genreArray\",\"purchase\") \\\n",
    "        .fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.join(df2_,df2_.item_id == df.item_id,\"left\") \\\n",
    "        .select(\"user_id\",df.item_id,\"genreArray\",\"purchase\",\"decade\") \\\n",
    "        .fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_test = df_test.join(df2_,df2_.item_id == df_test.item_id,\"left\") \\\n",
    "                  .select(\"user_id\",df_test.item_id,\"genreArray\") \\\n",
    "                  .fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## По каждому жанру считаем кол-во покупок и не покупок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = df3.groupBy(\"genreArray\") \\\n",
    "         .pivot(\"purchase\") \\\n",
    "         .count() \\\n",
    "         .fillna(0) \\\n",
    "         .select(\"genreArray\",col(\"1\").alias(\"s7\"), col(\"0\").alias(\"c7\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Строим агрегат по клиентам и жанрам (кол-во покупок и общее кол-во) и джойним с общим кол-вом покупок и не покупок на жанр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df3_purchase = df3.groupBy(\"user_id\",\"genreArray\") \\\n",
    "                  .agg(sum(\"purchase\").alias(\"s1\"), count(\"purchase\").alias(\"c1\")) \\\n",
    "                  .join(df7,[\"genreArray\"],\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Для train и test cтроим первоначальную таблицу с фичами (для каждой пары user_id, item_id) считаем сколько фильмов аналогичных жанров купил и не купил клиент, а также сколько фильмов аналогичных жанров было куплено и не куплено в целом)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_feature = df3 \\\n",
    "          .join(df3_purchase,(df3_purchase.genreArray == df3.genreArray) & (df3.user_id == df3_purchase.user_id) ,\"left\") \\\n",
    "          .select(df3.user_id,df3.genreArray,\"item_id\",\"purchase\",col(\"s1\"),(col(\"c1\") - col(\"s1\")).alias(\"c\"),\"s7\",\"c7\") \\\n",
    "          .fillna(0) \\\n",
    "          .groupBy(\"user_id\",\"item_id\",\"purchase\") \\\n",
    "          .agg( \\\n",
    "               sum(\"s1\").alias(\"s\") \\\n",
    "              ,sum(\"c\").alias(\"c\") \\\n",
    "              ,sum(\"s7\").alias(\"s7\") \\\n",
    "              ,sum(\"c7\").alias(\"c7\")) \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_feature_test = df3_test \\\n",
    "    .join(df3_purchase,(df3_purchase.genreArray == df3_test.genreArray) & (df3_test.user_id == df3_purchase.user_id) ,\"left\") \\\n",
    "    .select(df3_test.user_id,df3_test.genreArray,\"item_id\",col(\"s1\"),(col(\"c1\") - col(\"s1\")).alias(\"c\"),\"s7\",\"c7\") \\\n",
    "    .fillna(0) \\\n",
    "    .groupBy(\"user_id\",\"item_id\") \\\n",
    "    .agg( \\\n",
    "        sum(\"s1\") .alias(\"s\") \\\n",
    "       ,sum(\"c\").alias(\"c\") \\\n",
    "       ,sum(\"s7\").alias(\"s7\") \\\n",
    "       ,sum(\"c7\").alias(\"c7\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Год выпуска контента делим на категории (экспертно) < 1930, < 1950, < 1970, < 1980, < 1990, < 2000, < 2010, < 2015, < 2020, > 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_cat = df2.withColumn(\"decade\",when(df2.year < 1930, 1) \\\n",
    "                                 .when(df2.year < 1950, 2) \\\n",
    "                                 .when(df2.year < 1970, 3) \\\n",
    "                                 .when(df2.year < 1980, 4) \\\n",
    "                                 .when(df2.year < 1990, 5) \\\n",
    "                                 .when(df2.year < 2000, 6) \\\n",
    "                                 .when(df2.year < 2010, 7) \\\n",
    "                                 .when(df2.year < 2015, 8) \\\n",
    "                                 .when(df2.year < 2020, 9) \\\n",
    "                                 .when(df2.year >= 2020, 10) \\\n",
    "                                 .otherwise(-10)) \\\n",
    "             .select(\"item_id\",\"decade\",\"year\",\"genres\") \\\n",
    "             .fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train и test соединяем с таблицей категорий по году выпуска контента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_cat = df.join(df2_cat,df2_cat.item_id == df.item_id,\"inner\") \\\n",
    "            .select(\"user_id\",df.item_id,\"decade\",\"purchase\",\"year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_cat_test = df_test.join(df2_cat,df2_cat.item_id == df_test.item_id,\"inner\") \\\n",
    "                      .select(\"user_id\",df_test.item_id,\"decade\",\"year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Для каждого клиента и категории считаем кол-во покупок и общее кол-во"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_cat_purchase = df3_cat.groupBy(\"user_id\",\"decade\") \\\n",
    "                          .agg(sum(\"purchase\").alias(\"s2\"), count(\"purchase\").alias(\"c2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_item_cat_purchase = df3_cat.groupBy(\"item_id\",\"decade\") \\\n",
    "                          .agg(sum(\"purchase\").alias(\"s2i\"), count(\"purchase\").alias(\"c2i\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Для train и test cтроим таблицу с фичами по категории по году выпуска контента (для каждой пары user_id, item_id) считаем сколько фильмов аналогичных категорий купил и не купил клиент)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_cat_feature = df3_cat \\\n",
    "   .join(df3_cat_purchase,(df3_cat_purchase.decade == df3_cat.decade)  \\\n",
    "               & (df3_cat.user_id == df3_cat_purchase.user_id) ,\"left\") \\\n",
    "   .select(df3_cat.user_id,df3_cat.decade,\"item_id\",\"purchase\",col(\"s2\"),(col(\"c2\") - col(\"s2\")).alias(\"c2\")) \\\n",
    "   .groupBy(\"user_id\",\"item_id\",\"purchase\") \\\n",
    "   .agg( \\\n",
    "        sum(\"s2\").alias(\"s2\") \\\n",
    "       ,sum(\"c2\").alias(\"c2\")) \\\n",
    "   .fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_cat_feature_test = df3_cat_test \\\n",
    " .join(df3_cat_purchase,(df3_cat_purchase.decade == df3_cat_test.decade) \\\n",
    "                  & (df3_cat_test.user_id == df3_cat_purchase.user_id) ,\"left\") \\\n",
    " .select(df3_cat_test.user_id,df3_cat_test.decade,\"item_id\",col(\"s2\"),(col(\"c2\") - col(\"s2\")).alias(\"c2\")) \\\n",
    " .fillna(0) \\\n",
    " .groupBy(\"user_id\",\"item_id\") \\\n",
    " .agg( \\\n",
    "    sum(\"s2\").alias(\"s2\") \\\n",
    "   ,sum(\"c2\").alias(\"c2\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Соединяем фичи по жанрам и фичи по году выпуска контента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_feature2 = df3_feature \\\n",
    "  .join(df3_cat_feature,(df3_feature.user_id == df3_cat_feature.user_id) \\\n",
    "                    & (df3_feature.item_id == df3_cat_feature.item_id) ,\"left\") \\\n",
    "  .select(df3_feature.user_id,df3_feature.purchase,df3_feature.item_id,\"s\",\"c\",(col(\"s\")/(col(\"c\") + col(\"s\"))).alias(\"z\") \\\n",
    "        ,\"s2\",\"c2\", (col(\"s2\")/(col(\"c2\") + col(\"s2\"))).alias(\"z2\") \\\n",
    "        ,\"s7\",\"c7\", (col(\"s7\")/(col(\"c7\") + col(\"s7\"))).alias(\"z7\")) \\\n",
    "  .fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_feature2_test = df3_feature_test \\\n",
    "  .join(df3_cat_feature_test,(df3_feature_test.user_id == df3_cat_feature_test.user_id) \\\n",
    "     & (df3_feature_test.item_id == df3_cat_feature_test.item_id) ,\"left\") \\\n",
    "  .select(df3_feature_test.user_id,df3_feature_test.item_id,\"s\",\"c\",(col(\"s\")/(col(\"c\") + col(\"s\"))).alias(\"z\") \\\n",
    "          ,\"s2\",\"c2\", (col(\"s2\")/(col(\"c2\") + col(\"s2\"))).alias(\"z2\") \\\n",
    "          ,\"s7\",\"c7\", (col(\"s7\")/(col(\"c7\") + col(\"s7\"))).alias(\"z7\")) \\\n",
    " .fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Для каждого клиента строим абсолютное кол-во покупок и не покупок и относительное кол-во покупок и добавляем их к фичам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f = df.groupBy(\"user_id\") \\\n",
    "         .pivot(\"purchase\") \\\n",
    "         .count() \\\n",
    "         .fillna(0) \\\n",
    "         .select(\"user_id\",col(\"1\").alias(\"su\"), col(\"0\").alias(\"cu\"), (col(\"1\")/(col(\"0\") + col(\"1\"))).alias(\"zu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_feature3 = df3_feature2.join(df_f,[\"user_id\"],\"left\").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_feature3_test = df3_feature2_test.join(df_f,[\"user_id\"],\"left\").fillna(0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Для каждого фильма строим абсолютное кол-во покупок и не покупок и относительное кол-во покупок и добавляем их к фичам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i = df.groupBy(\"item_id\") \\\n",
    "         .pivot(\"purchase\") \\\n",
    "         .count() \\\n",
    "         .fillna(0) \\\n",
    "         .select(\"item_id\",col(\"1\").alias(\"si\"), col(\"0\").alias(\"ci\"),  (col(\"1\")/(col(\"0\") + col(\"1\"))).alias(\"zi\") ) \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_feature4 = df3_feature3.join(df_i,[\"item_id\"],\"left\").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_feature4_test = df3_feature3_test.join(df_i,[\"item_id\"],\"left\").fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загружаем датасет с инофрмацией о просмотрах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = spark.read.options(delimiter=',').options(header = True).csv(\"/labs/slaba03/laba03_views_programmes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Для каждого клиента строим фичи по среднему и абсолютному времени просмотра и кол-ву просмотров и добавляем их к общей таблице с фичами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4.select(\"user_id\", (df4.ts_end - df4.ts_start).alias(\"t\")) \\\n",
    "         .groupBy(\"user_id\") \\\n",
    "         .agg(avg(\"t\").alias(\"al5\"), count(\"t\").alias(\"cl5\"), sum(\"t\").alias(\"sl5\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_feature5 = df3_feature4.join(df5,[\"user_id\"],\"left\").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_feature5_test = df3_feature4_test.join(df5,[\"user_id\"],\"left\").fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Получаем общую таблицу с фичами (строим 2 производные фичи - оносительное кол-во покупок по жанрам и году выпуска)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feature = df3_feature5.withColumn(\"sf\",col(\"s\")/(col(\"su\")+ col(\"cu\"))) \\\n",
    "                          .withColumn(\"s2f\",col(\"s2\")/(col(\"su\")+ col(\"cu\"))) \\\n",
    "                          .fillna(0) \\\n",
    "                          .cache()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feature_test = df3_feature5_test.withColumn(\"sf\",col(\"s\")/(col(\"su\")+ col(\"cu\"))) \\\n",
    "                                    .withColumn(\"s2f\",col(\"s2\")/(col(\"su\")+ col(\"cu\"))) \\\n",
    "                                    .fillna(0) \\\n",
    "                                    .cache() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Собираем фичи в вектор (берем только улучшающие метрику модели)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "\n",
    "'''assembler = VectorAssembler(\n",
    "    inputCols=[\"s\", \"c\",\"z\", \"s2\", \"c2\", \"z2\",\"su\",\"cu\",\"zu\",\"si\",\"ci\",\"zi\",\"al5\",\"cl5\",\"sl5\", \"c7\",\"sf\",\"s2f\"],\n",
    "    outputCol=\"features\") '''\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"s\", \"c\", \"z\",\"s2\",\"su\",\"zu\",\"si\",\"ci\",\"zi\",\"cl5\",\"sl5\",\"c7\",\"sf\",\"s2f\",\"sd\",\"zd\",\"sdf\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"s\", \"c\", \"z\",\"s2\",\"su\",\"zu\",\"si\",\"ci\",\"zi\",\"cl5\",\"sl5\",\"c7\",\"sf\",\"s2f\"],\n",
    "    outputCol=\"features\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Логистическая регрессия "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(labelCol= \"purchase\", maxIter=6, regParam=0.001)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler,lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(all_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обработка результата модели и выгрузка в файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "firstelement=udf(lambda v:float(v[1]),FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform(all_feature_test).cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "w = Window.orderBy(\"user_id\",\"item_id\")\n",
    "final = prediction.select((row_number().over(w)).alias(\"r\"),\"user_id\",\"item_id\",firstelement(\"probability\").alias(\"purchase\")).coalesce(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/29 22:47:15 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-master-1.newprolab.com:8020/user/yuriy.severyukhin/lab3.csv' to trash at: hdfs://spark-master-1.newprolab.com:8020/user/yuriy.severyukhin/.Trash/Current/user/yuriy.severyukhin/lab3.csv1667072835434\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -R lab3.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.write.options(header='True', delimiter=',',mode = 'overwrite').csv(\"lab3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get: `_SUCCESS': File exists\r\n"
     ]
    }
   ],
   "source": [
    " !hdfs dfs -get lab3.csv/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm lab03.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv part*.csv lab03.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
