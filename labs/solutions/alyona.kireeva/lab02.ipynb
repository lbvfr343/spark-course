{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 2 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "    \n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, Normalizer, StopWordsRemover\n",
    "from pyspark import Row\n",
    "\n",
    "import re\n",
    "import json\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "spark = (SparkSession\n",
    "        .builder\n",
    "        .config(conf=conf)\n",
    "        .appName('Lab02')\n",
    "        .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master-4.newprolab.com:4044\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe138187630>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_string(series):\n",
    "    regex = re.compile(u'[\\w\\d]{2,}', re.U)\n",
    "    words = series.str.findall(regex)\n",
    "    return words\n",
    "\n",
    "tokenizer_udf = F.pandas_udf(clear_string, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('/labs/slaba02/DO_record_per_line.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cat: string (nullable = true)\n",
      " |-- desc: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- provider: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "|                 cat|                desc| id|lang|                name|      provider|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "|3/business_manage...|This course intro...|  4|  en|Accounting Cycle:...|Canvas Network|\n",
      "|              11/law|This online cours...|  5|  en|American Counter ...|Canvas Network|\n",
      "|5/computer_scienc...|This course is ta...|  6|  fr|Arithmétique: en ...|Canvas Network|\n",
      "|  14/social_sciences|We live in a digi...|  7|  en|Becoming a Dynami...|Canvas Network|\n",
      "|2/biology_life_sc...|This self-paced c...|  8|  en|           Bioethics|Canvas Network|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# токенизируем\n",
    "df = df.withColumn(\"token\", tokenizer_udf(F.col(\"desc\")))\n",
    "\n",
    "# убираем стоп-слова\n",
    "remover = StopWordsRemover(inputCol=\"token\", outputCol=\"remove\")\n",
    "df = remover.transform(df)\n",
    "\n",
    "# TF\n",
    "hashingTF = HashingTF(\n",
    "    inputCol=\"remove\", \n",
    "    outputCol=\"tf\", \n",
    "    numFeatures=10000\n",
    ")\n",
    "tf = hashingTF.transform(df)\n",
    "\n",
    "# TF-IDF\n",
    "idf = IDF(inputCol=\"tf\", outputCol=\"tfidf\").fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "tfidf = tfidf.drop('token','remove','tf')\n",
    "\n",
    "# нормализуем\n",
    "normalizer = Normalizer(inputCol=\"tfidf\", outputCol=\"norm\")\n",
    "data = normalizer.transform(tfidf)\n",
    "data = data.drop('cat','desc','provider', 'tfidf')\n",
    "\n",
    "# косинусня мера близости\n",
    "dot_udf = F.udf(lambda x,y: float(x.dot(y)), DoubleType())\n",
    "result_data = data.alias(\"i\").join(data.alias(\"j\"), F.col(\"i.id\") < F.col(\"j.id\"))\\\n",
    "    .select(\n",
    "        F.col(\"i.id\").alias(\"i\"), \n",
    "        F.col(\"j.id\").alias(\"j\"),\n",
    "        F.col(\"i.lang\").alias(\"i_lang\"), \n",
    "        F.col(\"j.lang\").alias(\"j_lang\"),\n",
    "        F.col(\"i.name\").alias(\"i_name\"), \n",
    "        F.col(\"j.name\").alias(\"j_name\"), \n",
    "        dot_udf(\"i.norm\", \"j.norm\").alias(\"cos\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+------+--------------------+--------------------+--------------------+\n",
      "|  i|  j|i_lang|j_lang|              i_name|              j_name|                 cos|\n",
      "+---+---+------+------+--------------------+--------------------+--------------------+\n",
      "|  4|  5|    en|    en|Accounting Cycle:...|American Counter ...|0.004165624020100...|\n",
      "|  4|  6|    en|    fr|Accounting Cycle:...|Arithmétique: en ...|0.011528231112801742|\n",
      "|  4|  7|    en|    en|Accounting Cycle:...|Becoming a Dynami...|0.027471109713127377|\n",
      "|  4|  8|    en|    en|Accounting Cycle:...|           Bioethics|0.027650154233236635|\n",
      "|  4|  9|    en|    en|Accounting Cycle:...|College Foundatio...| 0.03125132417217193|\n",
      "+---+---+------+------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df\n",
    "del tf\n",
    "del tfidf\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# курсы, для которых нужны рекомендации\n",
    "courses_to_rec = [\n",
    "    [23126, u'en', u'Compass - powerful SASS library that makes your life easier'],\n",
    "    [21617, u'en', u'Preparing for the AP* Computer Science A Exam \\u2014 Part 2'],\n",
    "    [16627, u'es', u'Aprende Excel: Nivel Intermedio by Alfonso Rinsche'],\n",
    "    [11556, u'es', u'Aprendizaje Colaborativo by UNID Universidad Interamericana para el Desarrollo'], \n",
    "    [16704, u'ru', u'\\u041f\\u0440\\u043e\\u0433\\u0440\\u0430\\u043c\\u043c\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u043d\\u0430 Lazarus'], \n",
    "    [13702, u'ru', u'\\u041c\\u0430\\u0442\\u0435\\u043c\\u0430\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u0430\\u044f \\u044d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u043a\\u0430']\n",
    "]\n",
    "\n",
    "# список их id\n",
    "id_list = [x[0] for x in courses_to_rec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# оставляем только строки, в которых есть наши курсы\n",
    "result_df = result_data.filter(\n",
    "    (F.col('i').isin(id_list)|F.col('j').isin(id_list))&(F.col('i_lang')==F.col('j_lang'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "del result_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------+--------------------+--------------------+--------------------+\n",
      "|  i|    j|i_lang|j_lang|              i_name|              j_name|                 cos|\n",
      "+---+-----+------+------+--------------------+--------------------+--------------------+\n",
      "|  4|21617|    en|    en|Accounting Cycle:...|Preparing for the...| 0.04708163917463968|\n",
      "|  4|23126|    en|    en|Accounting Cycle:...|Compass - powerfu...|0.011433001452314673|\n",
      "|  5|21617|    en|    en|American Counter ...|Preparing for the...|0.024248589506925412|\n",
      "|  5|23126|    en|    en|American Counter ...|Compass - powerfu...|0.006758205222059656|\n",
      "|  7|21617|    en|    en|Becoming a Dynami...|Preparing for the...|0.015767482428724364|\n",
      "+---+-----+------+------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23126\n",
      "21617\n",
      "16627\n",
      "11556\n",
      "16704\n",
      "13702\n"
     ]
    }
   ],
   "source": [
    "# собираем рекомендации\n",
    "\n",
    "result = {}\n",
    "\n",
    "for course in id_list:\n",
    "    \n",
    "    print(course)\n",
    "\n",
    "\n",
    "    temp = result_df.filter((F.col('i')==course)|(F.col('j')==course)).sort(F.col('cos').desc()).limit(10)\n",
    "\n",
    "    i = [int(row.i) for row in temp.collect()]\n",
    "    j = [int(row.j) for row in temp.collect()]\n",
    "\n",
    "    result_list = []\n",
    "    for count in range(10):\n",
    "        if i[count]!=course:\n",
    "            result_list.append(i[count])\n",
    "        else:\n",
    "            result_list.append(j[count])\n",
    "            \n",
    "    result[str(course)] = result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'23126': [14760, 13665, 13782, 11978, 25782, 3819, 26864, 14380, 3919, 6206],\n",
       " '21617': [21609,\n",
       "  21616,\n",
       "  22298,\n",
       "  21608,\n",
       "  21081,\n",
       "  19417,\n",
       "  21673,\n",
       "  21628,\n",
       "  21630,\n",
       "  21623],\n",
       " '16627': [11431, 17964, 11575, 12247, 10738, 13021, 17961, 5558, 12660, 5687],\n",
       " '11556': [16488, 5750, 16929, 11554, 18005, 7833, 7121, 23357, 8098, 3660],\n",
       " '16704': [1247, 1236, 1365, 1164, 1273, 20288, 1233, 18331, 8186, 8203],\n",
       " '13702': [864, 21079, 8313, 28074, 1111, 13057, 1033, 21025, 8123, 1217]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lab02.json\", \"w\") as fp:\n",
    "    json.dump(result , fp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
