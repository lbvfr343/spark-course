{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 2 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import Row\n",
    "import json\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .config(conf=conf)\n",
    "         .appName(\"test\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master-4.newprolab.com:4044\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f651c04f470>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master-4.newprolab.com:4044\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=pyspark-shell>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "need_to_predict_for = [[23126, u'en', u'Compass - powerful SASS library that makes your life easier'], [21617, u'en', u'Preparing for the AP* Computer Science A Exam \\u2014 Part 2'], [16627, u'es', u'Aprende Excel: Nivel Intermedio by Alfonso Rinsche'], [11556, u'es', u'Aprendizaje Colaborativo by UNID Universidad Interamericana para el Desarrollo'], [16704, u'ru', u'\\u041f\\u0440\\u043e\\u0433\\u0440\\u0430\\u043c\\u043c\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u043d\\u0430 Lazarus'], [13702, u'ru', u'\\u041c\\u0430\\u0442\\u0435\\u043c\\u0430\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u0430\\u044f \\u044d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u043a\\u0430']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[11556,\n",
       "  'es',\n",
       "  'Aprendizaje Colaborativo by UNID Universidad Interamericana para el Desarrollo'],\n",
       " [13702, 'ru', 'Математическая экономика'],\n",
       " [16627, 'es', 'Aprende Excel: Nivel Intermedio by Alfonso Rinsche'],\n",
       " [16704, 'ru', 'Программирование на Lazarus'],\n",
       " [21617, 'en', 'Preparing for the AP* Computer Science A Exam — Part 2'],\n",
       " [23126, 'en', 'Compass - powerful SASS library that makes your life easier']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "need_to_predict_for = sorted(need_to_predict_for, key=lambda row: row[0])\n",
    "need_to_predict_for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"lang\": \"en\", \"name\": \"Accounting Cycle: The Foundation of Business Measurement and Reporting\", \"cat\": \"3/business_management|6/economics_finance\", \"provider\": \"Canvas Network\", \"id\": 4, \"desc\": \"This course introduces the basic financial statements used by most businesses, as well as the essential tools used to prepare them. This course will serve as a resource to help business students succeed in their upcoming university-level accounting classes, and as a refresher for upper division accounting students who are struggling to recall elementary concepts essential to more advanced accounting topics. Business owners will also benefit from this class by gaining essential skills necessary to organize and manage information pertinent to operating their business. At the conclusion of the class, students will understand the balance sheet, income statement, and cash flow statement. They will be able to differentiate between cash basis and accrual basis techniques, and know when each is appropriate. They\\u2019ll als"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -head '/labs/slaba02/DO_record_per_line.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(fields=[\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"cat\", StringType()),\n",
    "    StructField(\"lang\", StringType()),\n",
    "    StructField(\"provider\", StringType()),\n",
    "    StructField(\"desc\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses = spark.read\\\n",
    "          .schema(schema)\\\n",
    "          .format(\"json\")\\\n",
    "          .load(\"/labs/slaba02/DO_record_per_line.json\")\\\n",
    "          .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, name: string, cat: string, lang: string, provider: string, desc: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses = courses.repartition(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'name', 'cat', 'lang', 'provider', 'desc']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses.schema.fieldNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses = courses[[courses.id, courses.desc]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, RegexTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|   id|                desc|               words|\n",
      "+-----+--------------------+--------------------+\n",
      "|  469|Tinkering activit...|[tinkering, activ...|\n",
      "| 2465|This course is th...|[this, course, is...|\n",
      "|  202|Environmental law...|[environmental, l...|\n",
      "|12505| Accounting skill...|[accounting, skil...|\n",
      "|12440|\n",
      "Aprendemos, sobr...|[aprendemos, sobr...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexTokenizer(inputCol=\"desc\", outputCol=\"words\", pattern=\"[\\\\p{L}\\\\w\\\\d]{2,}\", gaps=False)\n",
    "wordsData = tokenizer.transform(courses)\n",
    "wordsData = wordsData[f.size(wordsData.words) > 0]\n",
    "wordsData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+\n",
      "|   id|                desc|               words|         rawFeatures|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "|  469|Tinkering activit...|[tinkering, activ...|(10000,[32,70,128...|\n",
      "| 2465|This course is th...|[this, course, is...|(10000,[157,201,3...|\n",
      "|  202|Environmental law...|[environmental, l...|(10000,[32,70,91,...|\n",
      "|12505| Accounting skill...|[accounting, skil...|(10000,[1,8,11,18...|\n",
      "|12440|\n",
      "Aprendemos, sobr...|[aprendemos, sobr...|(10000,[21,147,24...|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "featurizedData = hashingTF.transform(wordsData).cache()\n",
    "featurizedData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|   id|         rawFeatures|            features|\n",
      "+-----+--------------------+--------------------+\n",
      "|  469|(10000,[32,70,128...|(10000,[32,70,128...|\n",
      "| 2465|(10000,[157,201,3...|(10000,[157,201,3...|\n",
      "|  202|(10000,[32,70,91,...|(10000,[32,70,91,...|\n",
      "|12505|(10000,[1,8,11,18...|(10000,[1,8,11,18...|\n",
      "|12440|(10000,[21,147,24...|(10000,[21,147,24...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=1)\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData).cache()\n",
    "\n",
    "rescaledData[[rescaledData.id, rescaledData.rawFeatures, rescaledData.features]].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'desc', 'words', 'rawFeatures', 'features']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rescaledData.schema.fieldNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_for_specific_vector(a):\n",
    "    return f.udf(lambda b: float(a.dot(b) / (a.norm(2) * b.norm(2))), FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_map = {}\n",
    "for i in [i[0] for i in need_to_predict_for]:\n",
    "    vectors_map[i] = rescaledData[rescaledData.id == i][[rescaledData.features]].take(1)[0].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}\n",
    "for i in [i[0] for i in need_to_predict_for]:\n",
    "    rows = rescaledData.withColumn(str(i), cos_for_specific_vector(vectors_map[i])(\"features\"))\\\n",
    "        .filter(rescaledData.id != i)\\\n",
    "        .orderBy(str(i), ascending=False)[[rescaledData.id, f.col(str(i))]]\\\n",
    "        .take(10)\n",
    "    result[str(i)] = [row.id for row in rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'11556': [16488, 468, 13461, 23357, 7833, 19330, 9289, 10447, 22710, 387],\n",
       " '13702': [864, 6516, 25502, 8313, 8979, 15649, 17834, 16357, 15678, 13890],\n",
       " '16627': [11431, 11575, 25010, 12247, 5687, 13021, 17964, 18661, 9408, 12863],\n",
       " '16704': [3864, 25724, 23407, 25726, 23864, 1247, 1236, 18023, 25627, 25991],\n",
       " '21617': [21609,\n",
       "  21608,\n",
       "  21616,\n",
       "  21492,\n",
       "  21624,\n",
       "  21623,\n",
       "  21506,\n",
       "  21703,\n",
       "  21675,\n",
       "  21700],\n",
       " '23126': [14760,\n",
       "  13665,\n",
       "  13782,\n",
       "  20638,\n",
       "  24419,\n",
       "  15909,\n",
       "  2724,\n",
       "  25782,\n",
       "  23756,\n",
       "  17499]}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"lab02.json\", \"w\") as outfile:\n",
    "    json.dump(result, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"11556\": [16488, 468, 13461, 23357, 7833, 19330, 9289, 10447, 22710, 387], \"13702\": [864, 6516, 25502, 8313, 8979, 15649, 17834, 16357, 15678, 13890], \"16627\": [11431, 11575, 25010, 12247, 5687, 13021, 17964, 18661, 9408, 12863], \"16704\": [3864, 25724, 23407, 25726, 23864, 1247, 1236, 18023, 25627, 25991], \"21617\": [21609, 21608, 21616, 21492, 21624, 21623, 21506, 21703, 21675, 21700], \"23126\": [14760, 13665, 13782, 20638, 24419, 15909, 2724, 25782, 23756, 17499]}"
     ]
    }
   ],
   "source": [
    "! cat lab02.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
