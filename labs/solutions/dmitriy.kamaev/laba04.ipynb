{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 2 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import Row\n",
    "import json\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .config(conf=conf)\n",
    "         .appName(\"test\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /labs/slaba04/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -head /labs/slaba04/gender_age_dataset.txt | sed -n '1,2p'\n",
    "!hdfs dfs -head /labs/slaba04/gender_age_dataset.txt | xxd | sed -n '1,5p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- uid: string (nullable = true)\n",
      " |-- user_json: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"delimiter\", \"\\t\") \\\n",
    "        .load('/labs/slaba04/gender_age_dataset.txt')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count(), df.filter(df.gender != '-').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df.gender != '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Шаг 1: Объединяем пол и возрастную группу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- uid: string (nullable = true)\n",
      " |-- user_json: string (nullable = true)\n",
      " |-- clazz: string (nullable = true)\n",
      "\n",
      "-RECORD 0-------------------------\n",
      " gender    | F                    \n",
      " age       | 18-24                \n",
      " uid       | d50192e5-c44e-4ae... \n",
      " user_json | {\"visits\": [{\"url... \n",
      " clazz     | F18-24               \n",
      "-RECORD 1-------------------------\n",
      " gender    | M                    \n",
      " age       | 25-34                \n",
      " uid       | d502331d-621e-472... \n",
      " user_json | {\"visits\": [{\"url... \n",
      " clazz     | M25-34               \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1 = df.withColumn('clazz', F.concat(df.gender, df.age))\n",
    "df_1.printSchema()\n",
    "df_1.show(2, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Баланс классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_sum = df_1.select(df_1.clazz).groupBy(df_1.clazz).count()\n",
    "clazz_sum = cl_sum.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['M25-34',\n",
       " 'F25-34',\n",
       " 'M35-44',\n",
       " 'F35-44',\n",
       " 'F18-24',\n",
       " 'F45-54',\n",
       " 'M45-54',\n",
       " 'M18-24',\n",
       " 'F>=55',\n",
       " 'M>=55']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_sum = {}\n",
    "for row in clazz_sum:\n",
    "    class_sum[row.clazz] = row[1]\n",
    "sorted_classes = [i[0] for i in sorted(class_sum.items(), key=lambda item: item[1])[::-1]]\n",
    "sorted_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Шаг 2: Парсим json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, TimestampType, ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_url = StructType([ \n",
    "    StructField(\"url\",StringType())\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_visits = StructType([\n",
    "    StructField(\"visits\", ArrayType(schema_url))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df_1.withColumn('visits', F.from_json(df_1.user_json, schema_visits).visits)\n",
    "df_2.printSchema()\n",
    "df_2.show(2, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Шаг 3: Разбиваем массив сайтов на строки урлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = df_2.withColumn('site', F.explode(df_2.visits)).withColumn('url', F.col('site').url)\n",
    "df_3.printSchema()\n",
    "df_3.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Шаг 4: Вытаскиваем название сайта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4 = df_3.withColumn('site_name', F.regexp_extract(df_3.url, r'\\w+:\\/\\/(www\\.)?(([\\w-]+)(\\.[\\w-]+)*)\\/?', 2))\n",
    "df_4.printSchema()\n",
    "df_4.explain()\n",
    "df_4.select('site', 'site_name').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Шаг 5: Плющим посещения одних и тех же сайтов в одной истории посещения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5 = df_4.select(df_4.uid, df_4.clazz, df_4.site_name).distinct().select(df_4.clazz, df_4.site_name)\n",
    "df_5.printSchema()\n",
    "df_5.explain()\n",
    "df_5.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Шаг 6: Группируем по сайтам и транспонируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_6 = df_5.withColumn('amount', F.lit(1)).groupBy('site_name').pivot('clazz').sum('amount').na.fill(0).cache()\n",
    "df_6.printSchema()\n",
    "df_6.explain()\n",
    "df_6.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_6.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_6.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = df_6.columns[1:]\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = sorted_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вычислим дефолтный массив весов по классам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_nn = np.array([class_sum[clazz] for clazz in classes])\n",
    "def_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_arr = def_nn / np.sum(def_nn)\n",
    "def_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_class = classes[def_arr.argmax()]\n",
    "def_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Шаг 7: Соединяем все классы в один вектор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import DenseVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут собираем по `sorted_classes`, чтобы когда `argmax` находил одинаковые веса по классам, отдавал наиболее встречаемый класс в \"обучающей\" выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecAssembler = VectorAssembler(inputCols=sorted_classes, outputCol=\"classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_7 = vecAssembler.transform(df_6)\n",
    "df_7.printSchema()\n",
    "df_7.explain()\n",
    "df_7.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Шаг 8: Усредняем влияние между сайтами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_8 = df_7.withColumn('wclasses', F.udf(lambda classes: DenseVector(classes.toArray()/np.sum(classes.toArray().astype(int))), VectorUDT())(df_7.classes)).cache()\n",
    "df_8.printSchema()\n",
    "df_8.explain()\n",
    "df_8.show(2, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Результат подготовки данных для предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_weight = df_8.select(df_8.site_name, df_8.wclasses)\\\n",
    "    .rdd\\\n",
    "    .map(lambda row: (1, {row.site_name: row.wclasses.toArray().tolist()}))\\\n",
    "    .reduceByKey(lambda x, y: {**x, **y})\\\n",
    "    .collect()[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm site_weight_data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"site_weight_data.json\", \"w\") as outfile:\n",
    "    json.dump(site_weight, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head site_weight_data.json | cut -c 1-150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"site_weight_data.json\", \"r\") as infile:\n",
    "    site_weight = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_w_br = spark.sparkContext.broadcast(site_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_kafka_params = {\n",
    "    \"kafka.bootstrap.servers\": 'spark-master-1.newprolab.com:6667',\n",
    "    \"subscribe\": \"input_dmitriy.kamaev\",\n",
    "    \"startingOffsets\": \"latest\",\n",
    "    \"failOnDataLoss\": 'False'\n",
    "}\n",
    "kafka_sdf = spark.readStream.format(\"kafka\").options(**read_kafka_params).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int],\n",
       " DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_sdf, rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_sdf = rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кафка Шаг 1: value -> json: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafkaSchema = StructType([\n",
    "    StructField(\"uid\", StringType()),\n",
    "    StructField(\"visits\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_1 = kafka_sdf.withColumn('json', F.from_json(kafka_sdf.value.cast('string'), kafkaSchema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кафка Шаг 2: Достаем uid и посещения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_2 = sdf_1.select(sdf_1.json.uid.alias('uid'), F.from_json(sdf_1.json.visits, ArrayType(schema_url)).url.alias('urls'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uid: string (nullable = true)\n",
      " |-- urls: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отсюда и ниже начнутся deprecated шаги, потому что первый вариант был написан на aggregated механизмах, как выяснилось позже, нельзя работать в режиме append после группировки и нельзя использовать F.PandasUDFType.GROUPED_AGG в стримовых дата фреймах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Deprecated) Кафка Шаг 3: Explode, чтобы обработать все сайты регуляркой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uid: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_3 = sdf_2.select(sdf_2.uid, F.explode(sdf_2.urls).alias('url'))\n",
    "sdf_3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кафка Шаг 3: Вытаскиваем имена сайтов из урлов и делаем distinct (set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "@F.pandas_udf(ArrayType(StringType()))\n",
    "def extract_site_name(urls_sr):\n",
    "    return urls_sr.apply(lambda urls: list(set([re.search(r'\\w+:\\/\\/(www\\.)?(([\\w-]+)(\\.[\\w-]+)*)\\/?', url).group(2) for url in urls])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uid: string (nullable = true)\n",
      " |-- urls: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- site_names: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_3 = sdf_2.withColumn('site_names', extract_site_name(sdf_2.urls))\n",
    "sdf_3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Deprecated) Кафка Шаг 4: Выдираем имя сайта с помощью регулярки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uid: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- site_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_4 = sdf_3.withColumn('site_name', F.regexp_extract(sdf_3.url, r'\\w+:\\/\\/(www\\.)?(([\\w-]+)(\\.[\\w-]+)*)\\/?', 2))\n",
    "sdf_4.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кафка Шаг 4: Считаем веса по посещенным сайтам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.pandas_udf(StringType())\n",
    "def get_class_from_visited_sites(site_names_sr):\n",
    "    def _get_class_from_visited_sites(site_names):\n",
    "        # [0]*len(classes) - если сайта нет в списке, то вставляем нулевой массив\n",
    "        weights = np.array([site_w_br.value[site_name] if site_name in site_w_br.value else [0]*len(classes) for site_name in site_names])\n",
    "        return sorted_classes[weights.sum(axis=0).argmax()]\n",
    "    return site_names_sr.apply(_get_class_from_visited_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uid: string (nullable = true)\n",
      " |-- urls: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- site_names: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- clazz: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_4 = sdf_3.withColumn('clazz', get_class_from_visited_sites(sdf_3.site_names))\n",
    "sdf_4.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Deprecated) Кафка Шаг 5: Ставим в соответствие сайту массив весов по классам\n",
    "Если в обучающей выборке не было сайта, который пришел на прогноз, заменяем неизвестный сайт на нулевой массив"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uid: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- site_name: string (nullable = true)\n",
      " |-- weights: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_5 = sdf_4.withColumn('weights', F.pandas_udf(lambda sr: \n",
    "                                                 sr.apply(lambda site_name: \n",
    "                                                          site_w_br.value[site_name] if site_name in site_w_br.value else [0]*len(classes)),\n",
    "                                                 ArrayType(FloatType())\n",
    "                                                )(sdf_4.site_name))\n",
    "sdf_5.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_5.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кафка Шаг 5: Вытаскиваем из класса пол и возраст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uid: string (nullable = true)\n",
      " |-- urls: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- site_names: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- clazz: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_5 = sdf_4.withColumn('gender', sdf_4.clazz.substr(1, 1)).withColumn('age', sdf_4.clazz.substr(2, 5))\n",
    "sdf_5.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Deprecated) Кафка Шаг 6: Складываем вектора по uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uid: string (nullable = true)\n",
      " |-- clazz: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_6 = sdf_5.groupBy(sdf_5.uid).agg(F.pandas_udf(lambda sr:\n",
    "                                                  sorted_classes[sr.map(lambda arr: np.array(arr)).sum().argmax()],\n",
    "                                                  StringType(),\n",
    "                                                  F.PandasUDFType.GROUPED_AGG\n",
    "                                                 )(sdf_5.weights).alias('clazz'))\n",
    "sdf_6.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_6.select('clazz').groupBy('clazz').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Deprecated) Кафка Шаг 7: Вытаскиваем из класс пол и возраст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uid: string (nullable = true)\n",
      " |-- clazz: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_7 = sdf_6.withColumn('gender', sdf_6.clazz.substr(1, 1)).withColumn('age', sdf_6.clazz.substr(2, 5))\n",
    "sdf_7.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кафка запись результата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_kafka_params = {\n",
    "   \"kafka.bootstrap.servers\": 'spark-master-1.newprolab.com:6667',\n",
    "   \"topic\": \"dmitriy.kamaev\"\n",
    "}\n",
    "write_sinc = sdf_5.select(sdf_5.uid, sdf_5.gender, sdf_5.age)\\\n",
    "    .select(F.to_json(F.struct(col(\"*\"))).alias(\"value\"))\\\n",
    "    .writeStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .options(**write_kafka_params)\\\n",
    "    .option(\"checkpointLocation\", \"streaming/chk/chk_kafka\")\\\n",
    "    .outputMode(\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq = write_sinc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " {'message': 'Waiting for data to arrive',\n",
       "  'isDataAvailable': False,\n",
       "  'isTriggerActive': False})"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq.isActive, sq.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False,\n",
       " {'message': 'Stopped', 'isDataAvailable': False, 'isTriggerActive': False})"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq.isActive, sq.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq.exception()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Отладка через паркет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parquet_sink(df, file_name):\n",
    "    return df \\\n",
    "            .repartition(1) \\\n",
    "            .writeStream \\\n",
    "            .format(\"parquet\") \\\n",
    "            .option(\"path\", \"{f}\".format(f=file_name)) \\\n",
    "            .option(\"checkpointLocation\", \"tmp/chk_sg/{f}\".format(f=file_name)) \\\n",
    "            .trigger(processingTime=\"10 seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "drwxr-xr-x   - dmitriy.kamaev dmitriy.kamaev          0 2022-10-27 18:51 tmp/chk_sg/ss_01.parquet/commits\r\n",
      "-rw-r--r--   3 dmitriy.kamaev dmitriy.kamaev         45 2022-10-27 18:50 tmp/chk_sg/ss_01.parquet/metadata\r\n",
      "drwxr-xr-x   - dmitriy.kamaev dmitriy.kamaev          0 2022-10-27 18:51 tmp/chk_sg/ss_01.parquet/offsets\r\n",
      "drwxr-xr-x   - dmitriy.kamaev dmitriy.kamaev          0 2022-10-27 18:50 tmp/chk_sg/ss_01.parquet/sources\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls tmp/chk_sg/ss_01.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/27 18:25:38 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-master-1.newprolab.com:8020/user/dmitriy.kamaev/tmp/chk_sg/ss_01.parquet' to trash at: hdfs://spark-master-1.newprolab.com:8020/user/dmitriy.kamaev/.Trash/Current/user/dmitriy.kamaev/tmp/chk_sg/ss_01.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r tmp/chk_sg/ss_01.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.DataStreamWriter at 0x7fa974e31390>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sink = create_parquet_sink(kafka_sdf, \"ss_01.parquet\")\n",
    "sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fa974e318d0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq = sink.start()\n",
    "sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq.isActive, sq.exception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Stopped', 'isDataAvailable': False, 'isTriggerActive': False}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kill_all():\n",
    "    streams = SparkSession.builder.getOrCreate().streams.active\n",
    "    if streams:\n",
    "        for s in streams:\n",
    "            desc = s.lastProgress[\"sources\"][0][\"description\"]\n",
    "            s.stop()\n",
    "            print(\"Stopped {s}\".format(s=desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kill_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -ls ss_01.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "-RECORD 0-----------------------------\n",
      " key           | null                 \n",
      " value         | [7B 22 75 69 64 2... \n",
      " topic         | input_dmitriy.kamaev \n",
      " partition     | 0                    \n",
      " offset        | 5354                 \n",
      " timestamp     | 2022-10-26 20:48:... \n",
      " timestampType | 0                    \n",
      "-RECORD 1-----------------------------\n",
      " key           | null                 \n",
      " value         | [7B 22 75 69 64 2... \n",
      " topic         | input_dmitriy.kamaev \n",
      " partition     | 0                    \n",
      " offset        | 5355                 \n",
      " timestamp     | 2022-10-26 20:48:... \n",
      " timestampType | 0                    \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max, col\n",
    "\n",
    "rates = spark.read.parquet(\"ss_01.parquet\")\n",
    "print(rates.count())\n",
    "rates.printSchema()\n",
    "rates.show(2, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
