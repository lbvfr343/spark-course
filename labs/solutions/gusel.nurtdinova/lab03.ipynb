{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 4 --driver-memory 8g pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark import Row\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer, OneHotEncoderEstimator, VectorAssembler, StringIndexer\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier, DecisionTreeClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Imputer, OneHotEncoderEstimator, VectorAssembler, StringIndexer\n",
    "from pyspark.ml.linalg import VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .config(conf=conf)\n",
    "         .appName(\"KMP_lab03_2v\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master-5.newprolab.com:4052\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8d7812c518>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5c6b4d5d9db0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Читаем и обрабатываем данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "-rw-r--r--   3 hdfs hdfs   91066524 2022-01-06 18:46 /labs/slaba03/laba03_items.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs   29965581 2022-01-06 18:46 /labs/slaba03/laba03_test.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs   74949368 2022-01-06 18:46 /labs/slaba03/laba03_train.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs  871302535 2022-01-06 18:46 /labs/slaba03/laba03_views_programmes.csv\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /labs/slaba03/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Телепередачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`laba03_items.csv`** — дополнительные данные по items. В данном файле много лишней или ненужной информации, так что задача её фильтрации и отбора ложится на вас. Поля в файле, на которых хотелось бы остановиться:\n",
    "\n",
    "- `item_id` — primary key. Соответствует item_id в предыдущем файле.\n",
    "- `content_type` — тип телепередачи (1 — платная, 0 — бесплатная). Вас интересуют платные передачи.\n",
    "- `title` — название передачи, текстовое поле.\n",
    "- `year` — год выпуска передачи, число.\n",
    "- `genres` — поле с жанрами передачи, разделёнными через запятую."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = spark.read.csv('/labs/slaba03/laba03_items.csv', sep=\"\\t\", header=True)\\\n",
    "             .filter(F.col(\"content_type\") == F.lit(1))\n",
    "items = items.withColumn(\"year\", F.col(\"year\").cast(T.IntegerType()))\\\n",
    "    .withColumn(\"datetime_availability_start\", F.to_timestamp(F.col(\"datetime_availability_start\"), \n",
    "                                                        \"yyyy-MM-dd'T'HH:mm:ss'Z'\"))\\\n",
    "    .withColumn(\"datetime_availability_stop\", F.to_timestamp(F.col(\"datetime_availability_stop\"), \n",
    "                                                        \"yyyy-MM-dd'T'HH:mm:ss'Z'\"))\\\n",
    "    .withColumn(\"datetime_show_start\", F.to_timestamp(F.col(\"datetime_show_start\"), \n",
    "                                                        \"yyyy-MM-dd'T'HH:mm:ss'Z'\"))\\\n",
    "    .withColumn(\"datetime_show_stop\", F.to_timestamp(F.col(\"datetime_show_stop\"), \n",
    "                                                        \"yyyy-MM-dd'T'HH:mm:ss'Z'\"))\\\n",
    "    .withColumn(\"title\", F.lower(F.col(\"title\")))\\\n",
    "    .withColumn(\"genres\", F.lower(F.col(\"genres\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = items.filter(F.col(\"year\").isNotNull())\n",
    "items = items.filter(F.col(\"genres\").isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разметим жанры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3668, 3668)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items.select(F.col(\"item_id\")).count(), items.select(F.col(\"item_id\")).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              genres|count|\n",
      "+--------------------+-----+\n",
      "|ужасы,триллеры,за...|   79|\n",
      "|мультфильмы,детск...|   72|\n",
      "|  комедии,зарубежные|   66|\n",
      "|  эротика,зарубежные|   58|\n",
      "|        комедии,наши|   53|\n",
      "|             эротика|   51|\n",
      "|комедии,драмы,зар...|   50|\n",
      "|    драмы,зарубежные|   48|\n",
      "|триллеры,драмы,за...|   46|\n",
      "|    ужасы,зарубежные|   45|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items.groupBy(\"genres\").count().orderBy(F.col(\"count\").desc()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Т.к. жанр может встретиться в 1 фильме несколько раз, отметим ТОП 10 жанров. Остальные в Прочее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_top = Counter(\",\".join([gen[0].replace(\" \", \"\") for gen in items.select(F.col(\"genres\")).collect()]).split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('зарубежные', 1739),\n",
       " ('драмы', 957),\n",
       " ('комедии', 857),\n",
       " ('триллеры', 655),\n",
       " ('русские', 582),\n",
       " ('боевики', 543),\n",
       " ('наши', 517),\n",
       " ('мелодрамы', 473),\n",
       " ('приключения', 437),\n",
       " ('длядетей', 427)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(genres_top.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = items.withColumn(\"gen_top1\", \n",
    "                         F.when(F.col(\"genres\").like(\"%зарубежные%\"), F.lit(1))\\\n",
    "                         .otherwise(F.lit(0)))\\\n",
    "             .withColumn(\"gen_top2\", \n",
    "                         F.when(F.col(\"genres\").like(\"%драмы%\"), F.lit(1))\\\n",
    "                         .otherwise(F.lit(0)))\\\n",
    "             .withColumn(\"gen_top3\", \n",
    "                         F.when(F.col(\"genres\").like(\"%комедии%\"), F.lit(1))\\\n",
    "                         .otherwise(F.lit(0)))\\\n",
    "             .withColumn(\"gen_top4\", \n",
    "                         F.when(F.col(\"genres\").like(\"%триллеры%\"), F.lit(1))\\\n",
    "                         .otherwise(F.lit(0)))\\\n",
    "             .withColumn(\"gen_top5\", \n",
    "                         F.when(F.col(\"genres\").like(\"%русские%\"), F.lit(1))\\\n",
    "                         .otherwise(F.lit(0)))\\\n",
    "             .withColumn(\"gen_top6\", \n",
    "                         F.when(F.col(\"genres\").like(\"%боевики%\"), F.lit(1))\\\n",
    "                         .otherwise(F.lit(0)))\\\n",
    "             .withColumn(\"gen_top7\", \n",
    "                         F.when(F.col(\"genres\").like(\"%наши%\"), F.lit(1))\\\n",
    "                         .otherwise(F.lit(0)))\\\n",
    "             .withColumn(\"gen_top8\", \n",
    "                         F.when(F.col(\"genres\").like(\"%мелодрамы%\"), F.lit(1))\\\n",
    "                         .otherwise(F.lit(0)))\\\n",
    "             .withColumn(\"gen_top9\", \n",
    "                         F.when(F.col(\"genres\").like(\"%приключения%\"), F.lit(1))\\\n",
    "                         .otherwise(F.lit(0)))\\\n",
    "             .withColumn(\"gen_top10\", \n",
    "                         F.when(F.col(\"genres\").like(\"%для детей%\"), F.lit(1))\\\n",
    "                         .otherwise(F.lit(0)))\\\n",
    "             .withColumn(\"gen_others\", \n",
    "                         F.when((F.col(\"gen_top1\")==F.lit(1))|(F.col(\"gen_top2\")==F.lit(1))\n",
    "                                |(F.col(\"gen_top3\")==F.lit(1))|(F.col(\"gen_top4\")==F.lit(1))\n",
    "                                |(F.col(\"gen_top5\")==F.lit(1))|(F.col(\"gen_top6\")==F.lit(1))\n",
    "                                |(F.col(\"gen_top7\")==F.lit(1))|(F.col(\"gen_top8\")==F.lit(1))\n",
    "                                |(F.col(\"gen_top9\")==F.lit(1))|(F.col(\"gen_top10\")==F.lit(1)), \n",
    "                                F.lit(0))\\\n",
    "                         .otherwise(F.lit(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|min(year)|max(year)|\n",
      "+---------+---------+\n",
      "|     1916|     2017|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items.select(F.min(F.col(\"year\")), F.max(F.col(\"year\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = items.withColumn(\"old_years\", \n",
    "                         F.when(F.col(\"year\") <= F.lit(1950), \n",
    "                                F.lit(1))\\\n",
    "                         .otherwise(F.lit(0)))\\\n",
    "             .withColumn(\"1951-1980\", \n",
    "                         F.when(F.col(\"year\").between(F.lit(1951), F.lit(1980)), \n",
    "                                F.lit(1))\\\n",
    "                         .otherwise(F.lit(0)))\\\n",
    "             .withColumn(\"1981-2000\", \n",
    "                         F.when(F.col(\"year\").between(F.lit(1981), F.lit(2000)), \n",
    "                                F.lit(1))\\\n",
    "                         .otherwise(F.lit(0)))\\\n",
    "             .withColumn(\"2001-2010\", \n",
    "                         F.when(F.col(\"year\").between(F.lit(2001), F.lit(2010)), \n",
    "                                F.lit(1))\\\n",
    "                         .otherwise(F.lit(0)))\\\n",
    "             .withColumn(\"new_years\", \n",
    "                         F.when(F.col(\"year\") >= F.lit(2011), \n",
    "                                F.lit(1))\\\n",
    "                         .otherwise(F.lit(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(item_id='65667', channel_id=None, datetime_availability_start=datetime.datetime(1970, 1, 1, 0, 0), datetime_availability_stop=datetime.datetime(2018, 1, 1, 0, 0), datetime_show_start=None, datetime_show_stop=None, content_type='1', title='на пробах только девушки (all girl auditions)', year=2013, genres='эротика', region_id=None, gen_top1=0, gen_top2=0, gen_top3=0, gen_top4=0, gen_top5=0, gen_top6=0, gen_top7=0, gen_top8=0, gen_top9=0, gen_top10=0, gen_others=1, old_years=0, 1951-1980=0, 1981-2000=0, 2001-2010=0, new_years=1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id_p = [item[0] for item in items.select(F.col(\"item_id\")).collect()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Просмотры телепередач."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительный файл **`laba03_views_programmes.csv`** по просмотрам передач с полями:\n",
    "\n",
    "- `ts_start` — время начала просмотра.\n",
    "- `ts_end` — время окончания просмотра.\n",
    "- `item_type` — тип просматриваемого контента:\n",
    "    - `live` — просмотр \"вживую\", в момент показа контента в эфире.\n",
    "    - `pvr` — просмотр в записи, после показа контента в эфире."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- item_id: string (nullable = true)\n",
      " |-- ts_start: long (nullable = true)\n",
      " |-- ts_end: long (nullable = true)\n",
      " |-- item_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "views_prog = spark.read.csv('/labs/slaba03/laba03_views_programmes.csv', header=True)\\\n",
    "                    .withColumn(\"ts_start\", F.col(\"ts_start\").cast(T.LongType()))\\\n",
    "                    .withColumn(\"ts_end\", F.col(\"ts_end\").cast(T.LongType()))\n",
    "views_prog.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "views_prog = views_prog.withColumn(\"ts_diff\", F.col(\"ts_end\") - F.col(\"ts_start\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_live_avg = views_prog\\\n",
    "                        .filter(F.col(\"item_type\") == F.lit(\"live\"))\\\n",
    "                        .groupBy(F.col(\"user_id\"))\\\n",
    "                        .agg(F.mean(F.col(\"ts_diff\")).alias(\"user_avg_live\"),\n",
    "                             F.count(F.col(\"item_id\")).alias(\"user_cnt_live\"))\n",
    "user_pvr_avg = views_prog\\\n",
    "                        .filter(F.col(\"item_type\") == F.lit(\"pvr\"))\\\n",
    "                        .groupBy(F.col(\"user_id\"))\\\n",
    "                        .agg(F.mean(F.col(\"ts_diff\")).alias(\"user_avg_pvr\"),\n",
    "                             F.count(F.col(\"item_id\")).alias(\"user_cnt_pvr\"))\n",
    "\n",
    "item_live_avg = views_prog\\\n",
    "                        .filter(F.col(\"item_type\") == F.lit(\"live\"))\\\n",
    "                        .groupBy(F.col(\"item_id\"))\\\n",
    "                        .agg(F.mean(F.col(\"ts_diff\")).alias(\"item_avg_live\"))\n",
    "item_pvr_avg = views_prog\\\n",
    "                        .filter(F.col(\"item_type\") == F.lit(\"pvr\"))\\\n",
    "                        .groupBy(F.col(\"item_id\"))\\\n",
    "                        .agg(F.mean(F.col(\"ts_diff\")).alias(\"item_avg_pvr\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Факты покупки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В **`laba03_train.csv`** содержатся факты покупки (колонка `purchase`) пользователями (колонка `user_id`) телепередач (колонка `item_id`). Такой формат файла вам уже знаком."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- purchase: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train = spark.read.csv('/labs/slaba03/laba03_train.csv', header=True)\\\n",
    "             .withColumn(\"user_id\", F.col(\"user_id\").cast(T.IntegerType()))\\\n",
    "             .withColumn(\"item_id\", F.col(\"item_id\").cast(T.IntegerType()))\\\n",
    "             .withColumn(\"purchase\", F.col(\"purchase\").cast(T.IntegerType()))\n",
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5032624"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|cnt_user|cnt_item|\n",
      "+--------+--------+\n",
      "|    1941|    3704|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.select(F.countDistinct(F.col(\"user_id\")).alias(\"cnt_user\"), \n",
    "             F.countDistinct(F.col(\"item_id\")).alias(\"cnt_item\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----------+----------+\n",
      "|purchase|   rows|cnt_d_user|cnt_d_item|\n",
      "+--------+-------+----------+----------+\n",
      "|       1|  10904|      1675|      3089|\n",
      "|       0|5021720|      1941|      3704|\n",
      "+--------+-------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.groupBy(F.col(\"purchase\")).agg(F.count(F.col(\"user_id\")).alias(\"rows\"), \n",
    "                                     F.countDistinct(F.col(\"user_id\")).alias(\"cnt_d_user\"), \n",
    "                                     F.countDistinct(F.col(\"item_id\")).alias(\"cnt_d_item\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21713675792357995"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10904/5021720*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В тренировочных данных 0,2% фактов покупок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user = train.groupBy(F.col(\"user_id\")).agg(F.mean(F.col(\"purchase\")).alias('user_purchase_avg'),\n",
    "                                                 F.count(F.col(\"item_id\")).alias(\"user_item_id\"),\n",
    "                                                 F.sum(F.col(\"purchase\")).alias(\"user_purchase_sum\"))\n",
    "train_item = train.groupBy(F.col(\"item_id\")).agg(F.mean(F.col(\"purchase\")).alias('item_purchase_avg'),\n",
    "                                                 F.count(F.col(\"user_id\")).alias(\"item_user_id\"),\n",
    "                                                 F.sum(F.col(\"purchase\")).alias(\"item_purchase_sum\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_merge_features(data):\n",
    "    data = data.alias(\"t\")\\\n",
    "                    .join(train_user.alias(\"tu\"), F.col(\"t.user_id\") == F.col(\"tu.user_id\") , \"left\")\\\n",
    "                    .join(train_item.alias(\"ti\"), F.col(\"t.item_id\") == F.col(\"ti.item_id\") , \"left\")\\\n",
    "                    .join(user_live_avg.alias(\"ul\"), F.col(\"t.user_id\") == F.col(\"ul.user_id\") , \"left\")\\\n",
    "                    .join(user_pvr_avg.alias(\"up\"), F.col(\"t.user_id\") == F.col(\"up.user_id\") , \"left\")\\\n",
    "                    .join(item_live_avg.alias(\"il\"), F.col(\"t.item_id\") == F.col(\"il.item_id\") , \"left\")\\\n",
    "                    .join(item_pvr_avg.alias(\"ip\"), F.col(\"t.item_id\") == F.col(\"ip.item_id\") , \"left\")\\\n",
    "                    .join(items.alias(\"i\"), F.col(\"t.item_id\") == F.col(\"i.item_id\"), \"left\")\\\n",
    "                    .select(\"t.user_id\", \n",
    "                            \"t.item_id\", \n",
    "                            \"t.purchase\", \n",
    "                            \"tu.user_purchase_avg\",\n",
    "                            \"tu.user_item_id\",\n",
    "                            \"tu.user_purchase_sum\",\n",
    "                            \"ti.item_purchase_avg\",\n",
    "                            \"ti.item_user_id\",\n",
    "                            \"ti.item_purchase_sum\",\n",
    "                            \"ul.user_avg_live\",\n",
    "                            \"ul.user_cnt_live\",\n",
    "                            \"up.user_avg_pvr\",\n",
    "                            \"up.user_cnt_pvr\",\n",
    "                            \"il.item_avg_live\",\n",
    "                            \"ip.item_avg_pvr\",\n",
    "                            \"i.gen_top1\",\n",
    "                            \"i.gen_top2\",\n",
    "                            \"i.gen_top3\",\n",
    "                            \"i.gen_top4\",\n",
    "                            \"i.gen_top5\",\n",
    "                            \"i.gen_top6\",\n",
    "                            \"i.gen_top7\",\n",
    "                            \"i.gen_top8\",\n",
    "                            \"i.gen_top9\",\n",
    "                            \"i.gen_top10\",\n",
    "                            \"i.gen_others\",\n",
    "                            \"i.old_years\",\n",
    "                            \"i.1951-1980\",\n",
    "                            \"i.1981-2000\",\n",
    "                            \"i.2001-2010\",\n",
    "                            \"i.new_years\"\n",
    "                           )\n",
    "    data = data.na.fill(0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = f_merge_features(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_assembler(data):\n",
    "    list_col_features = list(set(data.columns) - set(['user_id', 'item_id', 'purchase']))\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols = list_col_features,\n",
    "        outputCol=\"features\")\n",
    "    output = assembler.transform(data)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_features_ = f_assembler(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- purchase: integer (nullable = true)\n",
      " |-- user_purchase_avg: double (nullable = false)\n",
      " |-- user_item_id: long (nullable = true)\n",
      " |-- user_purchase_sum: long (nullable = true)\n",
      " |-- item_purchase_avg: double (nullable = false)\n",
      " |-- item_user_id: long (nullable = true)\n",
      " |-- item_purchase_sum: long (nullable = true)\n",
      " |-- user_avg_live: double (nullable = false)\n",
      " |-- user_cnt_live: long (nullable = true)\n",
      " |-- user_avg_pvr: double (nullable = false)\n",
      " |-- user_cnt_pvr: long (nullable = true)\n",
      " |-- item_avg_live: double (nullable = false)\n",
      " |-- item_avg_pvr: double (nullable = false)\n",
      " |-- gen_top1: integer (nullable = true)\n",
      " |-- gen_top2: integer (nullable = true)\n",
      " |-- gen_top3: integer (nullable = true)\n",
      " |-- gen_top4: integer (nullable = true)\n",
      " |-- gen_top5: integer (nullable = true)\n",
      " |-- gen_top6: integer (nullable = true)\n",
      " |-- gen_top7: integer (nullable = true)\n",
      " |-- gen_top8: integer (nullable = true)\n",
      " |-- gen_top9: integer (nullable = true)\n",
      " |-- gen_top10: integer (nullable = true)\n",
      " |-- gen_others: integer (nullable = true)\n",
      " |-- old_years: integer (nullable = true)\n",
      " |-- 1951-1980: integer (nullable = true)\n",
      " |-- 1981-2000: integer (nullable = true)\n",
      " |-- 2001-2010: integer (nullable = true)\n",
      " |-- new_years: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train_features_.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_features = df_train_features_.sampleBy(\"purchase\", fractions={0: 0.009, 1: 1}, seed=4242).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|purchase|count|\n",
      "+--------+-----+\n",
      "|       1|10904|\n",
      "|       0|44918|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train_features.groupby('purchase').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_id=844427, item_id=8389, purchase=1, user_purchase_avg=0.0034775888717156105, user_item_id=2588, user_purchase_sum=9, item_purchase_avg=0.005979073243647235, item_user_id=1338, item_purchase_sum=8, user_avg_live=6323.3835616438355, user_cnt_live=73, user_avg_pvr=2585.2105263157896, user_cnt_pvr=76, item_avg_live=0.0, item_avg_pvr=0.0, gen_top1=0, gen_top2=0, gen_top3=0, gen_top4=0, gen_top5=0, gen_top6=0, gen_top7=1, gen_top8=0, gen_top9=0, gen_top10=0, gen_others=0, old_years=0, 1951-1980=0, 1981-2000=1, 2001-2010=0, new_years=0, features=SparseVector(28, {2: 6323.3836, 8: 0.0035, 9: 73.0, 12: 1.0, 13: 76.0, 14: 9.0, 16: 2588.0, 18: 8.0, 21: 2585.2105, 22: 0.006, 23: 1338.0, 26: 1.0}))]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_features.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Тестовая выборка."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`laba03_test.csv`** — тестовый датасет без указанного целевого признака purchase, который вам и предстоит предсказать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- purchase: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = spark.read.csv('/labs/slaba03/laba03_test.csv', header=True)\\\n",
    "             .withColumn(\"user_id\", F.col(\"user_id\").cast(T.IntegerType()))\\\n",
    "             .withColumn(\"item_id\", F.col(\"item_id\").cast(T.IntegerType()))\\\n",
    "             .withColumn(\"purchase\", F.col(\"purchase\").cast(T.IntegerType()))\n",
    "test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+\n",
      "|user_id|item_id|purchase|\n",
      "+-------+-------+--------+\n",
      "|   1654|  94814|    null|\n",
      "+-------+-------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2156840"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|cnt_user|cnt_item|\n",
      "+--------+--------+\n",
      "|    1941|    3704|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.select(F.countDistinct(F.col(\"user_id\")).alias(\"cnt_user\"), \n",
    "             F.countDistinct(F.col(\"item_id\")).alias(\"cnt_item\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = f_merge_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_features = f_assembler(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- purchase: integer (nullable = true)\n",
      " |-- user_purchase_avg: double (nullable = false)\n",
      " |-- user_item_id: long (nullable = true)\n",
      " |-- user_purchase_sum: long (nullable = true)\n",
      " |-- item_purchase_avg: double (nullable = false)\n",
      " |-- item_user_id: long (nullable = true)\n",
      " |-- item_purchase_sum: long (nullable = true)\n",
      " |-- user_avg_live: double (nullable = false)\n",
      " |-- user_cnt_live: long (nullable = true)\n",
      " |-- user_avg_pvr: double (nullable = false)\n",
      " |-- user_cnt_pvr: long (nullable = true)\n",
      " |-- item_avg_live: double (nullable = false)\n",
      " |-- item_avg_pvr: double (nullable = false)\n",
      " |-- gen_top1: integer (nullable = true)\n",
      " |-- gen_top2: integer (nullable = true)\n",
      " |-- gen_top3: integer (nullable = true)\n",
      " |-- gen_top4: integer (nullable = true)\n",
      " |-- gen_top5: integer (nullable = true)\n",
      " |-- gen_top6: integer (nullable = true)\n",
      " |-- gen_top7: integer (nullable = true)\n",
      " |-- gen_top8: integer (nullable = true)\n",
      " |-- gen_top9: integer (nullable = true)\n",
      " |-- gen_top10: integer (nullable = true)\n",
      " |-- gen_others: integer (nullable = true)\n",
      " |-- old_years: integer (nullable = true)\n",
      " |-- 1951-1980: integer (nullable = true)\n",
      " |-- 1981-2000: integer (nullable = true)\n",
      " |-- 2001-2010: integer (nullable = true)\n",
      " |-- new_years: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test_features.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_id=761341, item_id=8389, purchase=0, user_purchase_avg=0.0003875968992248062, user_item_id=2580, user_purchase_sum=1, item_purchase_avg=0.005979073243647235, item_user_id=1338, item_purchase_sum=8, user_avg_live=1358.0625, user_cnt_live=16, user_avg_pvr=3240.4285714285716, user_cnt_pvr=28, item_avg_live=0.0, item_avg_pvr=0.0, gen_top1=0, gen_top2=0, gen_top3=0, gen_top4=0, gen_top5=0, gen_top6=0, gen_top7=1, gen_top8=0, gen_top9=0, gen_top10=0, gen_others=0, old_years=0, 1951-1980=0, 1981-2000=1, 2001-2010=0, new_years=0, features=SparseVector(28, {2: 1358.0625, 8: 0.0004, 9: 16.0, 12: 1.0, 13: 28.0, 14: 1.0, 16: 2580.0, 18: 8.0, 21: 3240.4286, 22: 0.006, 23: 1338.0, 26: 1.0}))]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_features.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Моделька."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-439f82f936dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#                         labelCol=\"purchase\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RF_lab3_1.sav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/bd9/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    featuresCol='features', \n",
    "    labelCol='purchase',\n",
    "    numTrees=500,\n",
    "    maxDepth=20,\n",
    "    maxBins=40,\n",
    "#     featureSubsetStrategy = 'all' \n",
    ")\n",
    "\n",
    "# from pyspark.ml.regression import GBTRegressor\n",
    "# from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "# from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# model = GBTRegressor(maxIter=50,\n",
    "#                         subsamplingRate=1.0,\n",
    "#                         maxDepth=9,\n",
    "#                         featuresCol=\"features\",\n",
    "#                         labelCol=\"purchase\")\n",
    "\n",
    "model.fit(df_train_features).save('RF_lab3_1.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassificationModel.load('RF_lab3_1.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability = model.transform(df_test_features)\\\n",
    ".select(\n",
    "    F.col('user_id'),\n",
    "    F.col('item_id'),\n",
    "    F.col('rawPrediction'),\n",
    "    F.col('prediction'),\n",
    "    F.col('probability')\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Запись результата."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = probability.withColumn(\"probability\", F.col(\"probability\").cast(T.StringType()))\\\n",
    "                    .select(F.col(\"user_id\"),\n",
    "                            F.col(\"item_id\"),\n",
    "                            F.col(\"probability\"))\n",
    "result = result.sort(F.col(\"user_id\").asc(), F.col(\"item_id\").asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = result.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['user_id'] = df['user_id'].astype(int)\n",
    "df['item_id'] = df['item_id'].astype(int)\n",
    "df['purchase'] = [x[1:-1].split(',')[1] for x in df['probability']]\n",
    "df['purchase'] = df['purchase'].astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['user_id','item_id','purchase']].to_csv('lab03.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Потолок 0,785 и при повышении параметров обучение падает, либо слишком долго отрабатывает - такой вариант не подходит. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- purchase: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- purchase: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.5 ms, sys: 1.24 ms, total: 13.7 ms\n",
      "Wall time: 40.4 s\n"
     ]
    }
   ],
   "source": [
    "als = ALS(maxIter=24, regParam=2.3, rank=6, coldStartStrategy=\"nan\",\n",
    "          userCol='user_id', itemCol='item_id', ratingCol='purchase',\n",
    "          nonnegative=False, implicitPrefs=True, alpha=4.0, seed=89)\n",
    "%time als_model = als.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = als_model.transform(train)\n",
    "#%time pred_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_train = pred_train.withColumn(\"prediction\", F.col(\"prediction\").cast(T.DoubleType()))\n",
    "pred_train = pred_train.coalesce(5).cache()\n",
    "pred_train.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",\n",
    "                                          labelCol=\"purchase\", \n",
    "                                          metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3894.evaluate.\n: java.lang.IllegalStateException: SparkContext has been shutdown\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2053)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:309)\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:151)\n\tat org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:62)\n\tat org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:61)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.OrderedRDDFunctions.sortByKey(OrderedRDDFunctions.scala:61)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$4$lzycompute(BinaryClassificationMetrics.scala:155)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$4(BinaryClassificationMetrics.scala:146)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions$lzycompute(BinaryClassificationMetrics.scala:148)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions(BinaryClassificationMetrics.scala:148)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.createCurve(BinaryClassificationMetrics.scala:226)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.roc(BinaryClassificationMetrics.scala:86)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.areaUnderROC(BinaryClassificationMetrics.scala:97)\n\tat org.apache.spark.ml.evaluation.BinaryClassificationEvaluator.evaluate(BinaryClassificationEvaluator.scala:87)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-c44b9f3153cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \"\"\"\n\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0misLargerBetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o3894.evaluate.\n: java.lang.IllegalStateException: SparkContext has been shutdown\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2053)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:309)\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:151)\n\tat org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:62)\n\tat org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:61)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.OrderedRDDFunctions.sortByKey(OrderedRDDFunctions.scala:61)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$4$lzycompute(BinaryClassificationMetrics.scala:155)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$4(BinaryClassificationMetrics.scala:146)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions$lzycompute(BinaryClassificationMetrics.scala:148)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions(BinaryClassificationMetrics.scala:148)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.createCurve(BinaryClassificationMetrics.scala:226)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.roc(BinaryClassificationMetrics.scala:86)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.areaUnderROC(BinaryClassificationMetrics.scala:97)\n\tat org.apache.spark.ml.evaluation.BinaryClassificationEvaluator.evaluate(BinaryClassificationEvaluator.scala:87)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "evaluator.evaluate(pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = als_model.transform(test)\n",
    "#%time pred_test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = pred_test.coalesce(5).cache()\n",
    "pred_test = pred_test.withColumn(\"prediction\", F.col(\"prediction\").cast(T.DoubleType()))\n",
    "pred_test.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pred_test.select(F.col(\"user_id\"),\n",
    "                             F.col(\"item_id\"),\n",
    "                             F.col(\"prediction\").alias(\"purchase\"))\\\n",
    "                     .sort(F.col(\"user_id\").asc(),\n",
    "                           F.col(\"item_id\").asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.toPandas().to_csv(\"lab03.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
