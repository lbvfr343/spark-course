import os
import sys
os.environ["PYSPARK_PYTHON"]='/opt/anaconda/envs/bd9/bin/python'
os.environ["SPARK_HOME"]='/usr/hdp/current/spark2-client'
os.environ["PYSPARK_SUBMIT_ARGS"]='--num-executors 2 pyspark-shell'

spark_home = os.environ.get('SPARK_HOME', None)
if not spark_home:
    raise ValueError('SPARK_HOME environment variable is not set')

sys.path.insert(0, os.path.join(spark_home, 'python'))
sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))
exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())

from pyspark import SparkConf
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import *
from pyspark import Row
import json

conf = SparkConf()

spark = (SparkSession
         .builder
         .config(conf=conf)
         .appName("test")
         .getOrCreate())
         
rdd = sc.textFile("/labs/laba01/ml-100k/u.data")

task1 = rdd \
    .filter(lambda x: "\t318\t" in x) \
    .map(lambda x: (x.split("\t")[2], 1)) \
    .reduceByKey(lambda x, y: x + y) \
    .sortByKey().collect()

task2 = rdd \
    .map(lambda x: (x.split("\t")[2], 1)) \
    .reduceByKey(lambda x, y: x + y) \
    .sortByKey().collect()
    
import json

d = {'hist_film': [x[1] for x in task1], 'hist_all': [x[1] for x in task2]}

with open('lab01.json', 'w') as f:
    f.write(json.dumps(d))

spark.stop()


