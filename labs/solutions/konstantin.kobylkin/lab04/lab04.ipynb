{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа №4. Константин Кобылкин. Вариант 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 5 --executor-memory 4g --executor-cores 1 --driver-memory 2g pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"Konstantin Kobylkin lab 4 app\") \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as t\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, , IndexToString\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.param.shared import HasOutputCol, HasInputCol\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark import keyword_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка данных обучающей выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/labs/slaba04/gender_age_dataset.txt'\n",
    "\n",
    "schema = t.StructType(fields=[\n",
    "    t.StructField('gender', t.StringType()),\n",
    "    t.StructField('age', t.StringType()),\n",
    "    t.StructField('uid', t.StringType()),\n",
    "    t.StructField('user_json', t.StringType()),\n",
    "])\n",
    "\n",
    "train_data = spark.read.csv(path, \n",
    "                            header=True, \n",
    "                            schema=schema, \n",
    "                            sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Схема для парсинга данных по визитам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits_schema = t.StructType([\n",
    "    t.StructField('visits', t.ArrayType(\n",
    "        t.StructType([\n",
    "            t.StructField('url', t.StringType(), True),\n",
    "            t.StructField('timestamp', t.LongType(), True)\n",
    "        ])\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Собственно, чтение данных обучающей выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- uid: string (nullable = true)\n",
      " |-- url: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.filter(train_data.age != '-') \\\n",
    "                       .filter(train_data.gender != '-') \\\n",
    "                       .withColumn('visits', f.from_json(f.col('user_json'), \n",
    "                                                         visits_schema )) \\\n",
    "                       .withColumn('url', f.col('visits.visits.url')) \\\n",
    "                       .drop('visits', 'user_json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LabelEncoding для пола и возраста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexerG = StringIndexer(inputCol='gender', \n",
    "                         outputCol='gender_i')\n",
    "indexerA = StringIndexer(inputCol='age', \n",
    "                         outputCol='age_i')\n",
    "indexModelG = indexerG.fit(train_data)\n",
    "indexModelA = indexerA.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParseURLTransformer(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(ParseURLTransformer, self).__init__()\n",
    "        if inputCol is not None:\n",
    "            self.setInputCol(inputCol)\n",
    "        if outputCol is not None:\n",
    "            self.setOutputCol(outputCol)\n",
    "        \n",
    "    def _transform(self, dataset):\n",
    "        res = dataset.withColumn('url_inter', \n",
    "                                 f.explode(self.getInputCol())) \\\n",
    "                     .withColumn('url_inter', \n",
    "                                 f.expr('parse_url(url_inter, \"HOST\")')) \\\n",
    "                     .withColumn('url_inter', \n",
    "                                 f.lower(f.col('url_inter'))) \\\n",
    "                     .drop(self.getInputCol())\n",
    "        \n",
    "        res_col_list = res.columns\n",
    "        res_col_list.remove('url_inter')\n",
    "        res = res.groupBy(res_col_list) \\\n",
    "                 .agg(f.collect_list('url_inter').alias(self.getOutputCol()))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlpt = ParseURLTransformer(inputCol='url', outputCol='url_parsed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выстраивание pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasher = HashingTF(numFeatures=130000, \n",
    "                   binary=False, \n",
    "                   inputCol=\"url_parsed\", \n",
    "                   outputCol=\"url_freq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "forestG = RandomForestClassifier(featuresCol='url_freq', \n",
    "                                 labelCol='gender_i',\n",
    "                                 predictionCol='predictionG', \n",
    "                                 probabilityCol='probabilityG', \n",
    "                                 rawPredictionCol='rawPredictionG')\n",
    "forestA = RandomForestClassifier(featuresCol='url_freq', \n",
    "                                 labelCol='age_i',  \\\n",
    "                                 predictionCol='predictionA', \n",
    "                                 probabilityCol='probabilityA', \n",
    "                                 rawPredictionCol='rawPredictionA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обратное преобразование индексов в метки возраста и пола"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringerG = IndexToString(inputCol='predictionG', \n",
    "                          outputCol='gender_s', \n",
    "                          labels=indexModelG.labels)\n",
    "stringerA = IndexToString(inputCol='predictionA', \n",
    "                          outputCol='age_s', \n",
    "                          labels=indexModelA.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "    urlpt,\n",
    "    hasher,\n",
    "    forestG,\n",
    "    forestA,\n",
    "    stringerG,\n",
    "    stringerA\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- uid: string (nullable = true)\n",
      " |-- url: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- gender_i: double (nullable = false)\n",
      " |-- age_i: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_visits = indexModelG.transform(train_data)\n",
    "df_visits = indexModelA.transform(df_visits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = pipeline.fit(df_visits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Описание параметров входного потока"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_BOOTSTRAP_SERVER = 'spark-master-1.newprolab.com:6667'\n",
    "KAFKA_INPUT_TOPIC = 'input_konstantin.kobylkin'\n",
    "KAFKA_OUTPUT_TOPIC = 'konstantin.kobylkin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_stream = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format('kafka')\n",
    "    .option('kafka.bootstrap.servers', KAFKA_BOOTSTRAP_SERVER)\n",
    "    .option('subscribe', KAFKA_INPUT_TOPIC)\n",
    "    .option('startingOffsets', 'earliest')\n",
    "    .option('failOnDataLoss', 'False')\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_schema = t.StructType([\n",
    "    t.StructField('uid', t.StringType(), True),\n",
    "    t.StructField('visits', t.StringType(), True),\n",
    "])\n",
    "\n",
    "visit_schema2 = t.ArrayType(\n",
    "    t.StructType([\n",
    "        t.StructField('url', t.StringType(), True),\n",
    "        t.StructField('timestamp', t.LongType(), True)\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uid: string (nullable = true)\n",
      " |-- url: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed_sdf = kafka_stream.select(f.col('value').cast('string').alias('value')) \\\n",
    "                         .select(f.from_json(f.col('value'), event_schema).alias('event')) \\\n",
    "                         .select( 'event.uid', f.from_json(f.col('event.visits'), visit_schema2).alias('visits')) \\\n",
    "                         .withColumn( 'url', f.col('visits.url')) \\\n",
    "                         .drop('visits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предсказание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uid: string (nullable = true)\n",
      " |-- url_parsed: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- url_freq: vector (nullable = true)\n",
      " |-- rawPredictionG: vector (nullable = true)\n",
      " |-- probabilityG: vector (nullable = true)\n",
      " |-- predictionG: double (nullable = false)\n",
      " |-- rawPredictionA: vector (nullable = true)\n",
      " |-- probabilityA: vector (nullable = true)\n",
      " |-- predictionA: double (nullable = false)\n",
      " |-- gender_s: string (nullable = true)\n",
      " |-- age_s: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_df_test = pipeline_model.transform(parsed_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uid: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_df = predictions_df_test.select('uid', f.col('gender_s').alias('gender'), f.col('age_s').alias('age'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Описание параметров выходного потока с предсказаниями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7ffa9c069a90>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_write_stream = (\n",
    "    predictions_df\n",
    "    .select(f.to_json(f.struct(*predictions_df.columns)).alias('value'))\n",
    "    .writeStream\n",
    "    .format(\"kafka\")\n",
    "    .outputMode(\"complete\")\n",
    "    .option(\"checkpointLocation\", \"checkpoints/checkpoints_lab04\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVER)\n",
    "    .option(\"topic\", KAFKA_OUTPUT_TOPIC)\n",
    ")\n",
    "kafka_write_stream.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kill_all():\n",
    "    streams = SparkSession.builder.getOrCreate().streams.active\n",
    "    if streams:\n",
    "        for s in streams:\n",
    "            desc = s.lastProgress[\"sources\"][0][\"description\"]\n",
    "            s.stop()\n",
    "            print(\"Stopped {s}\".format(s=desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped KafkaV2[Subscribe[input_konstantin.kobylkin]]\n"
     ]
    }
   ],
   "source": [
    "kill_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
