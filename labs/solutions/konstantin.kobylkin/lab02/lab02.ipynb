{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная 2. Кобылкин Константин. Вариант 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 --executor-memory 4g --executor-cores 1 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"Recommendation system\") \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).appName(\"RecSys\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(fields=[\n",
    "    StructField(\"lang\", StringType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"cat\", StringType()),\n",
    "    StructField(\"provider\", StringType()),\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"desc\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read\\\n",
    "          .format(\"json\")\\\n",
    "          .schema(schema)\\\n",
    "          .option(\"multiline\",\"false\") \\\n",
    "          .load(\"/labs/slaba02/DO_record_per_line.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28153\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /labs/slaba02/DO_record_per_line.json | wc -l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28153"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выделение подмножества фильмов на тех, же языках, что и запросы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select([\"id\", \"lang\", \"desc\", \"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_ids = set([23126, 21617, 16627, 11556, 16704, 13702])\n",
    "query_course_df = df.filter(df.id.isin(course_ids))\n",
    "query_course_languages = set(query_course_df.select(['lang']).distinct().toPandas().lang.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, desc, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df.lang.isin(query_course_languages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вычисление TF-IDF-весов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=ArrayType(StringType()))\n",
    "def customTokenizer(description):\n",
    "    regex = re.compile(u'[\\w\\d]{2,}', re.U)\n",
    "    return regex.findall(description.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer(inputCol=\"desc\",\n",
    "#                       outputCol=\"desc_words\")\n",
    "\n",
    "df = df.withColumn(\"desc_words\", \n",
    "                   customTokenizer(\"desc\"))\n",
    "stop_words_remover = StopWordsRemover(inputCol=\"desc_words\", #tokenizer.getOutputCol(),\n",
    "                                      outputCol=\"filtered_desc_words\")\n",
    "hashing_TF_transformer = HashingTF(inputCol=stop_words_remover.getOutputCol(), \n",
    "                                   outputCol=\"TF\",\n",
    "                                   numFeatures=10000)\n",
    "IDF_transformer = IDF(inputCol=hashing_TF_transformer.getOutputCol(), \n",
    "                      outputCol=\"TF-IDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_transform_pipeline = [#tokenizer,\n",
    "                         stop_words_remover,\n",
    "                         hashing_TF_transformer]\n",
    "\n",
    "for transformer in tf_transform_pipeline:\n",
    "    df = transformer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_dfs = {}\n",
    "\n",
    "\n",
    "for lang in query_course_languages:\n",
    "    lang_df = df.filter(df.lang == lang)\n",
    "    lang_dfs[lang] = IDF_transformer.fit(lang_df).transform(lang_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вычисление мер сходства с запросами и формирование результата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_course_id_langs_map = query_course_df.select(['id', 'lang']).toPandas().set_index('id').lang.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosineSimilarityEstimatorGenerator(given_course_tf_idf):\n",
    "    def cosineSimilarityEstimator(course_tf_idf):\n",
    "        if course_tf_idf.norm(2) == 0.0:\n",
    "            return 0.0\n",
    "        return float(course_tf_idf.dot(given_course_tf_idf) / (course_tf_idf.norm(2) * given_course_tf_idf.norm(2)))\n",
    "    return cosineSimilarityEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}\n",
    "\n",
    "for query_course_id in query_course_id_langs_map:\n",
    "    query_course_lang_df = lang_dfs[query_course_id_langs_map[query_course_id]]\n",
    "    query_course_tf_idf = query_course_lang_df.filter(query_course_lang_df.id == \n",
    "                                                      query_course_id).select('TF-IDF').toPandas().loc[0, 'TF-IDF']\n",
    "    query_course_cosineSimilarityEstimator = udf(cosineSimilarityEstimatorGenerator(query_course_tf_idf), \n",
    "                                                 DoubleType())\n",
    "    query_course_lang_df = query_course_lang_df.withColumn(\"course_\" + str(query_course_id), \n",
    "                                                           query_course_cosineSimilarityEstimator(\"TF-IDF\"))\n",
    "    best_matches = query_course_lang_df.orderBy(col(\"course_\" + str(query_course_id)).desc(),\n",
    "                                                col(\"name\").asc(),\n",
    "                                                col(\"id\").asc()).limit(11).toPandas()\n",
    "    result[str(query_course_id)] = best_matches.loc[best_matches.id != query_course_id, 'id'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Запись в файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('lab02.json', 'w') as fp:\n",
    "    json.dump(result, fp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
