{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1QefkD-2ne3"
   },
   "source": [
    "## Инициализация контекста и библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EylLRbT92ne4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GaLdydKx2ne5"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = SparkConf()\n",
    "spark = SparkSession.builder.config(conf=conf).appName(\"dmitriy.sokolov.laba02\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "mTvaLfH52ne6",
    "outputId": "72312f08-57ab-4391-9345-97943c1d8b03"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master-4.newprolab.com:4052\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>dmitriy.sokolov.laba02</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f301c845ac8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ReRdyYn52ne6",
    "outputId": "a45df51c-0326-4d68-c458-e80c5ed1eee0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/bd9/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['f']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lB9coKEr2ne7"
   },
   "source": [
    "## Задания для нахождения похожих курсов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "vGjfXhbg2ne7"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "w9ZB0-Ql2ne7",
    "outputId": "3159da4e-5d29-480f-c547-1adc0a2c9664"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "|id   |lang|\n",
      "+-----+----+\n",
      "|23126|en  |\n",
      "|21617|en  |\n",
      "|16627|es  |\n",
      "|11556|es  |\n",
      "|16704|ru  |\n",
      "|13702|ru  |\n",
      "+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tasks = spark.createDataFrame([\n",
    "    [23126, u'en'], \n",
    "    [21617, u'en'], \n",
    "    [16627, u'es'], \n",
    "    [11556, u'es'], \n",
    "    [16704, u'ru'], \n",
    "    [13702, u'ru']\n",
    "],\n",
    "    ['id', 'lang']\n",
    ")\n",
    "\n",
    "tasks.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRLnch_c2ne7"
   },
   "source": [
    "## Считывание датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "iZv5sPCq2ne8",
    "outputId": "b1fd80ec-4fcc-4a37-fe9c-8ec65d3f866b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cat: string (nullable = true)\n",
      " |-- desc: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- provider: string (nullable = true)\n",
      "\n",
      "CPU times: user 3.19 ms, sys: 0 ns, total: 3.19 ms\n",
      "Wall time: 2.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = spark.read\\\n",
    "          .format(\"json\")\\\n",
    "          .option(\"sep\", \"|\")\\\n",
    "          .load(\"/labs/slaba02/DO_record_per_line.json\")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "u0WOBM8B2ne8",
    "outputId": "47b76798-3a76-454b-c1df-e2be8a28c4c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "|                 cat|                desc| id|lang|                name|      provider|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "|3/business_manage...|This course intro...|  4|  en|Accounting Cycle:...|Canvas Network|\n",
      "|              11/law|This online cours...|  5|  en|American Counter ...|Canvas Network|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUGjsbEe2ne8"
   },
   "source": [
    "## Делаем TF-IDF над полем DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "sXlW-NYL2ne9"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, StopWordsRemover, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "bwApbBOY2ne9",
    "outputId": "926d4371-b3f5-4779-f8b6-881b5ff16df7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------------------+\n",
      "| id|lang|      filtered_words|\n",
      "+---+----+--------------------+\n",
      "|  4|  en|[course, introduc...|\n",
      "|  5|  en|[online, course, ...|\n",
      "|  6|  fr|[course, taught, ...|\n",
      "+---+----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "CPU times: user 30.2 ms, sys: 10.8 ms, total: 41.1 ms\n",
      "Wall time: 23.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = Tokenizer(inputCol=\"desc\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(df)\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "wordsData = remover.transform(wordsData)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "    \n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "df = rescaledData\n",
    "df.cache()\n",
    "\n",
    "df.select(\"id\", \"lang\", \"filtered_words\").show(3, truncate = True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "pW-euEbd2ne9"
   },
   "outputs": [],
   "source": [
    "def cos_sim_udf(v1):   \n",
    "    def cos_sim(v2):  \n",
    "        dev = float(v1.norm(2) * v2.norm(2))\n",
    "        res = 0\n",
    "        if dev != 0:\n",
    "            res = float(v1.dot(v2)) / dev\n",
    "        return res \n",
    "    return f.udf(cos_sim, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "6aFX-tXQ2ne-"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "dct_output = {}\n",
    "\n",
    "for row in tasks.sort('id').collect():\n",
    "    id = row[0]\n",
    "    lang = row[1]\n",
    "    vector = rescaledData[rescaledData['id'] == id][['features']].collect()[0][0]\n",
    "    data = (df\n",
    "            .filter(rescaledData.lang == lang)\n",
    "            .filter(rescaledData.id != id)\n",
    "            .withColumn('dist', cos_sim_udf(vector)('features'))\n",
    "            .sort(desc('dist'), 'name', 'id')\n",
    "            .select('id')\n",
    "            .limit(10)\n",
    "            .collect()\n",
    "           )\n",
    "    dct_output.update({str(id): [int(row.id) for row in data]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ApjfHX242ne-",
    "outputId": "5f182361-16f6-401e-ea2f-af5a89443b11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"11556\": [16488, 13461, 468, 23357, 19330, 16929, 387, 10447, 11554, 9289], \"13702\": [864, 1216, 7173, 1052, 8313, 17017, 19613, 21017, 17015, 8082], \"16627\": [11431, 12247, 13021, 25010, 11575, 5687, 5372, 12863, 9598, 22680], \"16704\": [1365, 20645, 1426, 20105, 8217, 1236, 1164, 1219, 8123, 875], \"21617\": [21609, 21608, 21616, 21492, 21624, 21623, 21630, 21628, 21508, 21703], \"23126\": [13782, 13665, 24419, 20638, 2724, 25782, 2633, 2723, 13348, 15909]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "json_output = json.dumps(dct_output)\n",
    "print(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "SZDmZFyL2ne_"
   },
   "outputs": [],
   "source": [
    "with open('lab02.json', 'w') as file:\n",
    "    file.write(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ennypma-2ne_",
    "outputId": "9e3bce79-7196-40eb-8fee-8fbf2eb97ead"
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
