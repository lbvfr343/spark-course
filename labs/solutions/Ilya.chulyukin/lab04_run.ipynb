{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 5 --executor-memory 4g --executor-cores 1 --driver-memory 3g pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"lab04\") \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import SparseVector, DenseVector\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover, CountVectorizer,StringIndexer,OneHotEncoder\n",
    "import pyspark.sql.functions as f_\n",
    "from pyspark.sql.types import FloatType, StructType, StructField, IntegerType, StringType, ArrayType,TimestampType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml import Pipeline\n",
    "import json\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import unquote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@f_.udf(StringType())\n",
    "def url_cyr(url):\n",
    "    return str(unquote(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Time is : 19:17:07\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time is :\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_kafka_params = {\n",
    "    \"kafka.bootstrap.servers\": 'spark-master-1.newprolab.com:6667',\n",
    "    \"subscribe\": \"input_ilya.chulyukin\",\n",
    "    \"checkpointLocation\":\"/tmp/chk_/{n}\".format(n='ilya.chulyukin'),\n",
    "    \"startingOffsets\": \"earliest\"\n",
    "}\n",
    "kafka_sdf = spark.read.format(\"kafka\").options(**read_kafka_params).load()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"kafka.bootstrap.servers\": 'spark-master-1.newprolab.com:6667',\n",
    "    \"subscribe\": \"input_ivan.ivanov\",\n",
    "    \"startingOffsets\": \"latest\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "read_kafka_params = {\n",
    "    \"kafka.bootstrap.servers\": 'spark-master-1.newprolab.com:6667',\n",
    "    \"subscribe\": \"input_ilya.chulyukin\",\n",
    "    \"startingOffsets\": \"latest\"\n",
    "}\n",
    "kafka_sdf = spark.readStream.format(\"kafka\").options(**read_kafka_params).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = \"map<string, array<struct<url:string,timestamp:string>>>\"\n",
    "kafka_in_tabular_data=kafka_sdf.select(f_.get_json_object(f_.col(\"value\").cast(\"string\"), \"$.uid\").alias(\"uid\"),\n",
    "                f_.concat(f_.lit('''{\"visits\": '''),f_.get_json_object(f_.col(\"value\").cast(\"string\"), \"$.visits\"),f_.lit('''}''')).alias(\"user_json\")\n",
    "                ).withColumn('json', f_.from_json(f_.col('user_json'), json_schema))\\\n",
    "    .select('uid', f_.explode('json'))\\\n",
    "    .select('uid',f_.explode('value'))\\\n",
    "    .select('uid',f_.col('col').url.alias('url'), f_.col('col').timestamp.alias('timestamp'))\\\n",
    "    .select( 'uid',f_.col('timestamp').alias('ts_original'),f_.to_timestamp(f_.from_unixtime(f_.col('timestamp')/1000)).alias('timestamp'), url_cyr(f_.col('url')).alias('url'))\\\n",
    "    .select('*'\n",
    "                ,f_.regexp_replace(f_.lower(f_.split(f_.col('url'),'/').getItem(2)),'www.','').alias('domain')\n",
    "                ,f_.when((f_.length(f_.split(f_.col('url'),'\\\\?').getItem(0))  >  f_.coalesce(f_.length(f_.split(f_.col('url'),'\\\\?').getItem(1)),f_.lit(0))), f_.split(f_.col('url'),'\\\\?').getItem(0) ).otherwise(f_.col('url')).alias('url2'))\\\n",
    "    .select('*',f_.trim(f_.lower(f_.regexp_replace(f_.regexp_replace('url2',r'[^\\pL\\p{Space}]',' ' ),'[ ]+',' '))).alias('url_words'))\\\n",
    "            .withColumn(\"lag_ts\",f_.lag('timestamp').over(Window.partitionBy(\"uid\").orderBy(f_.col('timestamp'))))\\\n",
    "            .withColumn(\"lag_domain\",f_.lag('domain').over(Window.partitionBy(\"uid\").orderBy(f_.col('timestamp'))))\\\n",
    "    .select('*'\n",
    "                ,(f_.col('timestamp').cast('long') - f_.col('lag_ts').cast('long')).alias('timediff')\n",
    "            ,f_.when(((f_.col('lag_domain').isNotNull()) & (f_.col('lag_domain') != f_.col('domain'))),f_.lit(1)).otherwise(f_.lit(0)).alias('change_site_flg')\n",
    "           )\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_in_grouped_tabular_data=kafka_in_tabular_data.groupBy('uid').agg(\n",
    "     f_.concat_ws( ' ',f_.collect_list(f_.col('url_words'))).alias('urls_w')\n",
    "    ,f_.concat_ws( ' ',f_.collect_list(f_.col('domain'))).alias('domains')\n",
    "    ,f_.max('timestamp').cast('long').alias('max_ts')\n",
    "    ,f_.min('timestamp').cast('long').alias('min_ts')\n",
    "    ,f_.avg('timestamp').cast('long').alias('avg_ts')\n",
    "    ,f_.max('timediff').alias('max_tdiff')\n",
    "    ,f_.min('timediff').alias('min_tdiff')\n",
    "    ,f_.avg('timediff').alias('avg_tdiff')\n",
    "    ,f_.countDistinct('domain').alias('domain_cnt')\n",
    "    ,f_.sum('change_site_flg').alias('change_site_cnt')\n",
    "    ,f_.count('*').alias('visit_cnt')\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_in_grouped_tabular_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.8 ms, sys: 10 ms, total: 46.9 ms\n",
      "Wall time: 27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kafka_tabular_tfidf_data=calc.transform(kafka_in_grouped_tabular_data)\n",
    "test_data=assembler.transform(kafka_tabular_tfidf_data).select('uid','features')\n",
    "model_load=RandomForestClassificationModel.load('lr_01_dup.sav')\n",
    "test_predictions=model_load.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_decode=spark.createDataFrame([[float(i),v.split('_')[0],v.split('_')[1]] for i,v in enumerate(si_label.labels)],schema='prediction:float, gender:string,age:string')\n",
    "result=test_predictions.join(label_decode,['prediction'],'left').select('uid','gender','age')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_kafka(topic, data):\n",
    "    kafka_params = {\"kafka.bootstrap.servers\": \"spark-master-1.newprolab.com:6667\"}\n",
    "    kafka_doc = f_.to_json(f_.struct(f_.col(\"*\")))\n",
    "    raw = data \\\n",
    "        .select(kafka_doc.alias(\"value\")) \\\n",
    "        .withColumn(\"topic\", f_.lit(topic))\n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.8 ms, sys: 14.8 ms, total: 36.6 ms\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "write_kafka_params = {\n",
    "   \"kafka.bootstrap.servers\": 'spark-master-1.newprolab.com:6667',\n",
    "   \"topic\": \"ilya.chulyukin\",\n",
    "   \"checkpointLocation\":\"/tmp/chk_/{n}\".format(n='ilya.chulyukin')\n",
    "}\n",
    "write_kafka('ilya.chulyukin',result).write.format(\"kafka\").options(**write_kafka_params)\\\n",
    "    .option(\"checkpointLocation\", \"/tmp/chk_/{n}\".format(n='ilya.chulyukin')).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Time is : 19:19:13\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time is :\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! hdfs dfs -get -f hdfs://spark-master-1.newprolab.com:8020/user/grigoriy.propirnyy/lr_22_dup.sav"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
