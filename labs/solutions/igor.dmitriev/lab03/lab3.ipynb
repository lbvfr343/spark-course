{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 16 --executor-memory 4g --executor-cores 8 --driver-memory 4g pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARTITIONS = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, StructType, StructField, DataType, StringType, LongType, TimestampType, BooleanType, IntegerType\n",
    "schema = StructType(fields=[\n",
    "                         StructField(\"item_id\",IntegerType(), True),\\\n",
    "                         StructField(\"channel_id\",IntegerType(), True),\\\n",
    "                         StructField(\"datetime_availability_start\",TimestampType(), True),\\\n",
    "                         StructField(\"datetime_availability_stop\",TimestampType(), True),\\\n",
    "                         StructField(\"datetime_show_start\",TimestampType(), True),\\\n",
    "                         StructField(\"datetime_show_stop\",TimestampType(), True),\\\n",
    "                         StructField(\"content_type\",IntegerType(), True),\\\n",
    "                         StructField(\"title\",StringType(), True),\\\n",
    "                         StructField(\"year\", StringType(), True),\\\n",
    "                         StructField('genres',StringType(), True),\\\n",
    "                         StructField(\"region_id\",IntegerType(), True),\\\n",
    "                         StructField(\"broken\",StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_items = spark.read.csv(\"/labs/slaba03/laba03_items.csv\",\n",
    "                       sep='\\t',\n",
    "                       header=True,\n",
    "                       nullValue = '',\n",
    "                       multiLine=True,\n",
    "                       enforceSchema=True,\n",
    "                       escape='\"',\n",
    "                       dateFormat=\"yyyy.dd\",\n",
    "                       columnNameOfCorruptRecord='broken',\n",
    "                       schema=schema)\n",
    "\n",
    "raw_items = raw_items.drop(\"datetime_availability_start\",\n",
    "                     \"datetime_availability_stop\", \"datetime_show_start\",\n",
    "                     \"datetime_show_stop\",\n",
    "                     \"channel_id\", \"region_id\", \"broken\")\n",
    "raw_items = raw_items.na.fill({'title': ' ', 'genres': '', 'year': 'empty'})\n",
    "items = raw_items.filter(\"content_type == 1\").withColumn(\"category_words\", f.split(\"genres\", \",\")).drop(\"genres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- content_type: integer (nullable = true)\n",
      " |-- title: string (nullable = false)\n",
      " |-- year: string (nullable = false)\n",
      " |-- category_words: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, VectorAssembler,\\\n",
    "    RegexTokenizer, StopWordsRemover, MinMaxScaler,\\\n",
    "    HashingTF, StringIndexer, CountVectorizer, IDF, OneHotEncoder, Normalizer, PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|item_id|            features|\n",
      "+-------+--------------------+\n",
      "|    578|(10180,[7684,1003...|\n",
      "| 100452|(10180,[665,3873,...|\n",
      "|  92471|(10180,[2911,4493...|\n",
      "|  75516|(10180,[2073,4549...|\n",
      "| 100500|(10180,[9543,1003...|\n",
      "|  11153|(10180,[5715,1001...|\n",
      "|   7679|(10180,[1188,2386...|\n",
      "|  69922|(10180,[5490,9632...|\n",
      "| 100218|(10180,[1752,6654...|\n",
      "|   7632|(10180,[1249,4068...|\n",
      "|   4711|(10180,[4061,1001...|\n",
      "|  86748|(10180,[7243,1000...|\n",
      "|  87546|(10180,[3420,7636...|\n",
      "|  91995|(10180,[665,715,1...|\n",
      "|  93505|(10180,[6127,9160...|\n",
      "|  92528|(10180,[8274,1000...|\n",
      "|  72961|(10180,[3857,7325...|\n",
      "|  99900|(10180,[665,1646,...|\n",
      "| 100188|(10180,[4083,4225...|\n",
      "|  88648|(10180,[1466,9269...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#tokenizer = Tokenizer(inputCol=\"title\")\n",
    "tokenizer = RegexTokenizer(inputCol=\"title\", pattern=u'[\\p{L}+]{2,}', gaps=False)\n",
    "\n",
    "stop_words = list(set.union(set(StopWordsRemover.loadDefaultStopWords(\"russian\")),\n",
    "set(StopWordsRemover.loadDefaultStopWords(\"english\"))))\n",
    "\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol = \"words\")\n",
    "\n",
    "tf = HashingTF(inputCol=remover.getOutputCol(), outputCol=\"title_tf_idf\", numFeatures=10000, binary=True)\n",
    "\n",
    "#tf = CountVectorizer(inputCol=remover.getOutputCol())\n",
    "#idf = IDF(inputCol=tf.getOutputCol())\n",
    "#scaler = MinMaxScaler(inputCol=idf.getOutputCol(), outputCol=\"title_tf_idf\")\n",
    "\n",
    "tf_c = HashingTF(inputCol=\"category_words\", outputCol=\"cat\", numFeatures=100, binary=True)\n",
    "\n",
    "si = StringIndexer(inputCol=\"year\", outputCol=\"si_years\")\n",
    "ohe_y = OneHotEncoder(inputCol=si.getOutputCol(), outputCol=\"years\")\n",
    "\n",
    "va = VectorAssembler(inputCols = [\"title_tf_idf\", \"cat\", \"years\"], handleInvalid = \"keep\")\n",
    "nrm = Normalizer(inputCol = va.getOutputCol(), outputCol=\"features\",)\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[\n",
    "    tokenizer,\n",
    "    remover,\n",
    "    tf,\n",
    "    #idf,\n",
    "    #scaler,\n",
    "    tf_c,\n",
    "    si,\n",
    "    ohe_y,\n",
    "    va,\n",
    "    nrm\n",
    "])\n",
    "\n",
    "pipeline_model = pipeline.fit(items)\n",
    "df = pipeline_model.transform(items).select(\"item_id\",\"features\").repartition(PARTITIONS).distinct().cache()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(fields=[\n",
    "                         StructField(\"user_id\",LongType(), True),\\\n",
    "                         StructField(\"item_id\",LongType(), True),\\\n",
    "                         StructField(\"ts_start\",LongType(), True),\\\n",
    "                         StructField(\"ts_end\",LongType(), True),\\\n",
    "                         StructField(\"item_type\",StringType(), True)])\n",
    "raw_views = spark.read.csv(\"/labs/slaba03/laba03_views_programmes.csv\", sep=',', header=True, schema=schema)\n",
    "raw_views = raw_views.withColumn(\"ts_start\", f.hour(f.from_unixtime(\"ts_start\"))).\\\n",
    "                      withColumn(\"ts_end\", f.hour(f.from_unixtime(\"ts_end\"))).\\\n",
    "                      withColumn(\"item_type\", f.regexp_replace('item_type', 'live', '1')).\\\n",
    "                      withColumn(\"item_type\", f.regexp_replace('item_type', 'pvr', '0')).\\\n",
    "                      withColumn(\"is_live\", f.col('item_type').cast(IntegerType())).\\\n",
    "                      drop(\"item_type\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "arr_sum = udf(lambda x: sum(x), IntegerType())   \n",
    "\n",
    "views = raw_views.groupBy(\"user_id\").agg(f.collect_list(\"item_id\").alias(\"item_ids\"),\\\n",
    "                                 f.collect_list(\"ts_start\").alias(\"h_starts\"),\\\n",
    "                                 f.collect_list(\"ts_end\").alias(\"h_ends\"),\\\n",
    "                                 f.sum(\"is_live\").alias(\"sum_is_live\"),\\\n",
    "                                 f.count(\"is_live\").alias(\"count_is_live\"))\n",
    "views = views.withColumn(\"live_per\", f.col(\"sum_is_live\") / f.col(\"count_is_live\") ).\\\n",
    "              drop(\"count_is_live\", \"sum_is_live\").repartition(PARTITIONS).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_item = HashingTF(inputCol=\"item_ids\", numFeatures=10000, binary=True)\n",
    "tf_h_start = HashingTF(inputCol=\"h_starts\", numFeatures=24)\n",
    "tf_h_end = HashingTF(inputCol=\"h_ends\", numFeatures=24)\n",
    "\n",
    "scaler_h_start = MinMaxScaler(inputCol=tf_h_start.getOutputCol())\n",
    "scaler_h_end = MinMaxScaler(inputCol=tf_h_end.getOutputCol())\n",
    "\n",
    "\n",
    "va = VectorAssembler(inputCols = [tf_item.getOutputCol(),\n",
    "                                  scaler_h_start.getOutputCol(),\n",
    "                                  scaler_h_end.getOutputCol(),\n",
    "                                  \"live_per\"])\n",
    "\n",
    "nrm = Normalizer(inputCol=va.getOutputCol(), outputCol=\"u_features\")\n",
    "\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[tf_item, tf_h_start, tf_h_end, scaler_h_start, scaler_h_end, va, nrm])\n",
    "\n",
    "pipeline_model = pipeline.fit(views)\n",
    "df_views = pipeline_model.transform(views).select(\"user_id\", \"u_features\").repartition(PARTITIONS).distinct().cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|user_id|          u_features|\n",
      "+-------+--------------------+\n",
      "| 778092|(10049,[1753,2233...|\n",
      "| 878654|(10049,[14,69,84,...|\n",
      "| 932242|(10049,[37,1263,3...|\n",
      "| 897562|(10049,[8,18,25,4...|\n",
      "| 872344|(10049,[6635,7958...|\n",
      "| 927038|(10049,[146,149,1...|\n",
      "| 741217|(10049,[1,2,19,28...|\n",
      "| 920307|(10049,[389,896,1...|\n",
      "| 857007|(10049,[18,74,152...|\n",
      "| 844660|(10049,[0,12,17,2...|\n",
      "| 822408|(10049,[44,48,69,...|\n",
      "| 858175|(10049,[17,23,26,...|\n",
      "| 852348|(10049,[130,242,2...|\n",
      "| 825660|(10049,[19,31,53,...|\n",
      "| 927510|(10049,[198,229,2...|\n",
      "| 906692|(10049,[6,7,19,22...|\n",
      "| 864958|(10049,[6,10,31,4...|\n",
      "| 849009|(10049,[71,110,12...|\n",
      "| 866205|(10049,[613,3286,...|\n",
      "| 920380|(10049,[525,671,7...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_views.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "schema = StructType(fields=[\n",
    "                         StructField(\"user_id\",LongType(), True),\\\n",
    "                         StructField(\"item_id\",LongType(), True),\\\n",
    "                         StructField(\"purchase\",IntegerType(), True)])\n",
    "raw_train = spark.read.csv(\"/labs/slaba03/laba03_train.csv\", sep=',', header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = raw_train.sampleBy(\"purchase\", fractions={0: 0.005, 1: 1}, seed=5757).repartition(PARTITIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "va = VectorAssembler(inputCols = [\"u_features\", \"features\"], outputCol=\"all_features\", handleInvalid=\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+--------------------+\n",
      "|item_id|user_id|purchase|        all_features|\n",
      "+-------+-------+--------+--------------------+\n",
      "|  11214| 625638|       0|(20229,[370,734,8...|\n",
      "|  88768| 875711|       1|(20229,[35,58,230...|\n",
      "|  94689| 747028|       1|(20229,[0,1,2,3,5...|\n",
      "|  10065| 747028|       1|(20229,[0,1,2,3,5...|\n",
      "|  96398| 831574|       0|(20229,[5,6,20,37...|\n",
      "|  95416| 865208|       0|(20229,[22,24,47,...|\n",
      "|  74569| 921908|       1|(20229,[114,333,3...|\n",
      "|   7573| 852632|       0|(20229,[24,63,113...|\n",
      "| 102441| 894940|       0|(20229,[154,184,2...|\n",
      "|  74570| 871411|       1|(20229,[74,158,17...|\n",
      "| 100410| 902914|       0|(20229,[17,57,89,...|\n",
      "| 101992| 937376|       1|(20229,[64,160,17...|\n",
      "|  80416| 776138|       1|(20229,[2,46,47,4...|\n",
      "|   8588| 742324|       1|(20229,[19,71,104...|\n",
      "| 100100| 875739|       0|(20229,[192,274,2...|\n",
      "|  73659| 811663|       0|(20229,[7,18,23,2...|\n",
      "|  89636| 851412|       0|(20229,[51,63,78,...|\n",
      "|   4302| 899297|       0|(20229,[84,100,13...|\n",
      "|   9272| 907552|       0|(20229,[13,633,19...|\n",
      "|  10743| 932049|       0|(20229,[1548,2158...|\n",
      "+-------+-------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = va.transform(data.join(df_views, \"user_id\", \"left\").join(df, \"item_id\", \"left\")).drop(\"u_features\", \"features\").repartition(PARTITIONS).cache()\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "lr = LogisticRegression(featuresCol='all_features', labelCol=\"purchase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.sampleBy(\"purchase\", fractions={0: 0.9, 1: 0.9}, seed=5757).repartition(PARTITIONS)\n",
    "test = data.join(train, (data.user_id == train.user_id) & (data.item_id == train.item_id), how=\"leftanti\").repartition(PARTITIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = lr.fit(train)\n",
    "predictions = lr_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8234304875099727"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"purchase\", metricName='areaUnderROC')\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "paramGrid = ParamGridBuilder().addGrid(lr.maxIter, [15, 30, 50, 100])\\\n",
    "                              .addGrid(lr.regParam, [0.01, 0.05, 0.1])\\\n",
    "                              .addGrid(lr.threshold, [0.05, 0.1, 0.5])\\\n",
    "                              .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid,\n",
    "                              evaluator=evaluator, numFolds=5, parallelism=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model = crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LogisticRegression_7930e0157b5e', name='maxIter', doc='max number of iterations (>= 0).'): 100,\n",
       " Param(parent='LogisticRegression_7930e0157b5e', name='regParam', doc='regularization parameter (>= 0).'): 0.1,\n",
       " Param(parent='LogisticRegression_7930e0157b5e', name='threshold', doc='Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p].'): 0.05}"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_model.getEstimatorParamMaps()[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = lr.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Отправка результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(fields=[\n",
    "                         StructField(\"user_id\", LongType(), True),\\\n",
    "                         StructField(\"item_id\", LongType(), True)])\n",
    "raw_test = spark.read.csv(\"/labs/slaba03/laba03_test.csv\", sep=',', header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "va = VectorAssembler(inputCols = [\"u_features\", \"features\"], outputCol=\"all_features\", handleInvalid=\"keep\")\n",
    "t = va.transform(raw_test.join(df_views, \"user_id\", \"left\").join(df, \"item_id\", \"left\")).drop(\"u_features\", \"features\").repartition(PARTITIONS).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = best_model.transform(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as T\n",
    "\n",
    "et_1 = f.udf(lambda v: v.toArray().tolist()[1], T.FloatType())\n",
    "\n",
    "p = p.drop(\"all_features\").withColumn(\"purchase\", et_1(\"probability\"))\n",
    "p = p.drop('rawPrediction', 'probability', 'prediction').repartition(PARTITIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = p.select('user_id', 'item_id', 'purchase').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['item_id'].count() == 2156840"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.sort_values(['user_id', 'item_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(\"lab03.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
