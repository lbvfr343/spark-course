{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 6 --executor-memory 4g --executor-cores 2 --driver-memory 2g pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "\n",
    "raw = spark.read.json('/labs/slaba02/DO_record_per_line.json').drop(\"provider\")\n",
    "raw = raw.repartition(12)\n",
    "\n",
    "#Ниже код для разбора категории текста, он в нашем случае не поможет, так как тестовые данные ее не содержат\n",
    "#def filter_empty(l):\n",
    "#    return [int(i) for i in (filter(lambda x: x is not None and len(x) > 0, l))]\n",
    "\n",
    "#filter_empty_udf = F.udf(filter_empty, ArrayType(StringType()))\n",
    "\n",
    "#raw = raw.withColumn(\"cat_code\", filter_empty_udf(F.split(raw.cat, \"\\/\\w+\\|*\")))\\\n",
    "#        .drop(\"provider\", \"cat\")\\\n",
    "#        .withColumnRenamed(\"cat_code\", \"cat\")\\\n",
    "#        .drop(\"cat_code\")\n",
    "\n",
    "\n",
    "#df = df.filter((df.lang == 'en') & (df.cat.isin(['3', '6', '14'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover\n",
    "\n",
    "#tokenizer = Tokenizer(inputCol=\"desc\", outputCol=\"words\")\n",
    "tokenizer = RegexTokenizer(inputCol=\"desc\", outputCol=\"raw_words\", pattern=u'[\\p{L}+]{2,}', gaps=False)\n",
    "\n",
    "stop_words = list(set.union(set(StopWordsRemover.loadDefaultStopWords(\"russian\")),\n",
    "set(StopWordsRemover.loadDefaultStopWords(\"spanish\")),\n",
    "set(StopWordsRemover.loadDefaultStopWords(\"english\"))))\n",
    "\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol = \"words\")\n",
    "\n",
    "from pyspark.ml.feature import HashingTF\n",
    "tf = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "\n",
    "#Альтернативный вариант посчитать разряженного вектора bag-of-words CountVectorizer\n",
    "#from pyspark.ml.feature import CountVectorizer\n",
    "#tf = CountVectorizer(inputCol=remover.getOutputCol(), outputCol=\"rawFeatures\")\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import IDF\n",
    "idf = IDF(inputCol=tf.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[\n",
    "    tokenizer,\n",
    "    remover,\n",
    "    tf,\n",
    "    idf\n",
    "])\n",
    "\n",
    "pipeline_model = pipeline.fit(raw)\n",
    "df = pipeline_model.transform(raw).select(\"id\", \"lang\", \"name\", \"features\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_: long (nullable = true)\n",
      " |-- lang_: string (nullable = true)\n",
      " |-- name_: string (nullable = true)\n",
      " |-- features_: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept = [[23126, u'en', u'Compass - powerful SASS library that makes your life easier'], [21617, u'en', u'Preparing for the AP* Computer Science A Exam \\u2014 Part 2'], [16627, u'es', u'Aprende Excel: Nivel Intermedio by Alfonso Rinsche'], [11556, u'es', u'Aprendizaje Colaborativo by UNID Universidad Interamericana para el Desarrollo'], [16704, u'ru', u'\\u041f\\u0440\\u043e\\u0433\\u0440\\u0430\\u043c\\u043c\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u043d\\u0430 Lazarus'], [13702, u'ru', u'\\u041c\\u0430\\u0442\\u0435\\u043c\\u0430\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u0430\\u044f \\u044d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u043a\\u0430']]\n",
    "test_data = spark.createDataFrame(data=dept, schema=[\"id\", \"lang\", \"name\"]).drop(\"name\", \"lang\")\n",
    "s_df = test_data.join(df, \"id\").\\\n",
    "                            withColumnRenamed(\"id\", \"id_\").\\\n",
    "                            withColumnRenamed(\"lang\", \"lang_\").\\\n",
    "                            withColumnRenamed(\"name\", \"name_\").\\\n",
    "                            withColumnRenamed(\"features\", \"features_\")\n",
    "s_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "distance_udf = F.udf(lambda u, v: float((v.dot(u) / (v.norm(2) * u.norm(2)))), FloatType())\n",
    "tup = s_df.crossJoin(df).filter(\"id != id_\").filter(\"lang == lang_\").\\\n",
    "                withColumn(\"rating\", distance_udf(F.col(\"features\"), F.col(\"features_\"))).\\\n",
    "                select(\"id\", \"id_\", \"rating\", \"name\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "def top10(x, y, z):\n",
    "    res = sorted(list(zip(x, [-i for i in y], z)), key=itemgetter(1, 2, 0),)\n",
    "    return [x[0] for x in res[:10]]\n",
    "\n",
    "top10_udf = F.udf(top10, ArrayType(IntegerType()))\n",
    "\n",
    "res = tup.groupBy(\"id_\").\\\n",
    "                    agg(F.collect_list(\"id\").alias(\"ids\"), F.collect_list(\"rating\").alias(\"ratings\"), F.collect_list(\"name\").alias(\"names\")).\\\n",
    "                    withColumn(\"top10\", top10_udf(F.col(\"ids\"), F.col(\"ratings\"), F.col(\"names\"))).\\\n",
    "                    select(\"id_\", \"top10\").cache()\n",
    "output = dict([(x[0], x[1]) for x in res.collect()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"lab02.json\", 'w') as f:\n",
    "    json.dump(output, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
