{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 16 --executor-memory 4g --executor-cores 8 --driver-memory 4g pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARTITIONS = 256\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, StructType, StructField, DataType, StringType, LongType, TimestampType, BooleanType, IntegerType\n",
    "schema = StructType(fields=[\n",
    "                         StructField(\"gender\",StringType(), True),\\\n",
    "                         StructField(\"age\",StringType(), True),\\\n",
    "                         StructField(\"uid\",StringType(), True),\\\n",
    "                         StructField(\"user_json\",StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_dict = {\"F\":\"1\", \"M\":\"0\", \">=55\":\"0\", \"45-54\": \"1\", \"35-44\": \"2\", \"25-34\": \"3\", \"18-24\": \"4\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Transformer\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml.param.shared import  HasInputCol, HasOutputCol\n",
    "from pyspark.sql.functions import udf\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "class ExtractURLs(Transformer, HasInputCol, HasOutputCol):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(ExtractURLs, self).__init__()\n",
    "        if inputCol is not None:\n",
    "            self.setInputCol(inputCol)\n",
    "        if outputCol is not None:\n",
    "            self.setOutputCol(outputCol)\n",
    "            \n",
    "    def _transform(self, dataset):\n",
    "        arr_extract = udf(lambda arr: [urlparse(x.url).netloc for x in arr] , ArrayType(StringType()))\n",
    "        \n",
    "        schema_json = StructType(fields=[\n",
    "                         StructField(\"visits\", ArrayType(\n",
    "                                                 StructType(fields=[\n",
    "                                                     StructField(\"url\", StringType(), True),\n",
    "                                                     StructField(\"timestamp\", StringType(), True)\n",
    "                                                 ])), False)])\n",
    "        \n",
    "        return dataset.withColumn(\"user_json_\", f.from_json(self.getInputCol(), schema_json))\\\n",
    "                      .withColumn(\"visits\", f.col(\"user_json_\").visits)\\\n",
    "                      .withColumn(self.getOutputCol(), arr_extract(f.col(\"visits\")))\\\n",
    "                      .drop(\"visits\", \"user_json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_items = spark.read.csv(\"/labs/slaba04\", sep=\"\\t\", header=True, schema=schema)\\\n",
    "             .filter(\"gender != '-'\")\\\n",
    "             .na.replace(replace_dict)\\\n",
    "             .withColumn(\"age\", f.col(\"age\").cast(\"int\"))\\\n",
    "             .withColumn(\"gender\", f.col(\"gender\").cast(\"int\"))\\\n",
    "             .repartition(PARTITIONS)\\\n",
    "             .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, CountVectorizer, IDF, Normalizer\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "eurls = ExtractURLs(inputCol=\"user_json\")\n",
    "tf = CountVectorizer(inputCol=eurls.getOutputCol(), minTF=2, maxDF=0.5)\n",
    "idf = IDF(inputCol=tf.getOutputCol())\n",
    "nrml = Normalizer(inputCol=idf.getOutputCol())\n",
    "gender_model = NaiveBayes(featuresCol=nrml.getOutputCol(),\n",
    "                          labelCol=\"gender\",\n",
    "                          predictionCol=\"gender_p\",\n",
    "                          rawPredictionCol=\"skip_1\", \n",
    "                          probabilityCol=\"skip_2\"\n",
    "                         )\n",
    "age_model = NaiveBayes(featuresCol=nrml.getOutputCol(),\n",
    "                       labelCol=\"age\", predictionCol=\"age_p\",\n",
    "                       rawPredictionCol=\"skip_3\", probabilityCol=\"skip_4\")\n",
    "pipeline = Pipeline(stages=[\n",
    "    eurls,\n",
    "    tf,\n",
    "    idf,\n",
    "    nrml,\n",
    "    age_model,\n",
    "    gender_model\n",
    "])\n",
    "\n",
    "model = pipeline.fit(raw_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = model.transform(raw_items).select(\"uid\", \"gender\",  \"age\", \"gender_p\", \"age_p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проверка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = raw_items.sampleBy(\"gender\", fractions={0: 0.9, 1: 0.9}, seed=5757).repartition(PARTITIONS)\n",
    "test_model = pipeline.fit(train)\n",
    "\n",
    "test = raw_items.join(train, \"uid\", \"leftanti\")\n",
    "predictions = test_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"gender_p\",\n",
    "                                          labelCol=\"gender\", metricName='accuracy')\n",
    "gender_acc = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"age_p\",\n",
    "                                          labelCol=\"age\", metricName='accuracy')\n",
    "age_acc = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6358333333333334\n",
      "0.4288888888888889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(gender_acc)\n",
    "print(age_acc)\n",
    "gender_acc * age_acc > 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание стрима"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_gender_dict = {1.0: \"F\", 0.0: \"M\"}\n",
    "inv_age_dict = {0.0:\">=55\", 1.0:\"45-54\", 2.0:\"35-44\", 3.0:\"25-34\", 4.0:\"18-24\"}\n",
    "\n",
    "pack2json = udf(lambda x,y,z : '{{\"uid\":\"{:s}\", \"age\":\"{:s}\", \"gender\":\"{:s}\"}}'.format(x,\\\n",
    "                                                                                      inv_age_dict[y],\\\n",
    "                                                                                      inv_gender_dict[z]),\\\n",
    "              StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_kafka_params = {\n",
    "    \"kafka.bootstrap.servers\": 'spark-master-1.newprolab.com:6667',\n",
    "    \"subscribe\": \"input_igor.dmitriev\",\n",
    "    \"startingOffsets\": \"latest\"\n",
    "}\n",
    "\n",
    "kafka_sdf = spark.readStream.format(\"kafka\").options(**read_kafka_params).load()\n",
    "\n",
    "schema_json = StructType(fields=[\n",
    "                 StructField(\"uid\", StringType(), False),\\\n",
    "                 StructField(\"visits\", StringType(), False)])\n",
    "\n",
    "kafka_sdf = kafka_sdf.select(\"value\")\\\n",
    "        .withColumn(\"json_raw\", f.from_json(f.col(\"value\").cast(\"string\"), schema_json))\\\n",
    "        .withColumn(\"uid\", f.col(\"json_raw\").uid)\\\n",
    "        .withColumn(\"visits\", f.col(\"json_raw\").visits )\\\n",
    "        .withColumn(\"user_json\", f.format_string('{\"visits\": %s}', f.col(\"visits\")))\\\n",
    "        .select(\"uid\", \"user_json\")\n",
    "\n",
    "batch_df = model.transform(kafka_sdf)\\\n",
    "    .select(\"uid\", \"age_p\", \"gender_p\")\\\n",
    "    .withColumn(\"value\", pack2json(f.col(\"uid\"), f.col(\"age_p\"), f.col(\"gender_p\")))\\\n",
    "    .drop(\"gender_p\", \"age_p\", \"uid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_kafka_params = {\n",
    "   \"kafka.bootstrap.servers\": 'spark-master-1.newprolab.com:6667',\n",
    "   \"topic\": \"igor.dmitriev\"\n",
    "}\n",
    "\n",
    "sink = batch_df.writeStream.format(\"kafka\").options(**write_kafka_params)\\\n",
    "    .option(\"checkpointLocation\", \"streaming/chk/chk_kafka\")\\\n",
    "    .outputMode(\"append\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sink.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped KafkaV2[Subscribe[input_igor.dmitriev]]\n"
     ]
    }
   ],
   "source": [
    "kill_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kill_all():\n",
    "    streams = SparkSession.builder.getOrCreate().streams.active\n",
    "    if streams:\n",
    "        for s in streams:\n",
    "            desc = s.lastProgress[\"sources\"][0][\"description\"]\n",
    "            s.stop()\n",
    "            print(\"Stopped {s}\".format(s=desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
